{"cells":[{"cell_type":"markdown","metadata":{},"source":["```python\n","[\n","    {\"title\": \"RLHF on transformer language models\", \"icon\": \"1-circle-fill\", \"subtitle\" : \"(100%)\"},\n","    {\"title\": \"Bonus\", \"icon\": \"star\", \"subtitle\" : \"\"}\n","]\n","```"]},{"cell_type":"markdown","metadata":{"id":"whCcGn3OQOaP"},"source":["# [2.4] - RLHF\n"]},{"cell_type":"markdown","metadata":{"id":"NzpGGRLH1tc8"},"source":["### Colab: [exercises](https://colab.research.google.com/drive/13TDGeRdUcZ30nlfkN_PAxQxe8oX2x49u?usp=sharing) | [solutions](https://colab.research.google.com/drive/1KEXcflwuTGxf6JkAWdDCveXldDs6qgfw?usp=sharing)\n","\n","[Streamlit page](https://arena3-chapter2-rl.streamlit.app/[2.4]_RLHF)\n","\n","Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), and ask any questions on the dedicated channels for this chapter of material."]},{"cell_type":"markdown","metadata":{"id":"ST7GZ0xkxW6j"},"source":["<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/shoggoth.png\" width=\"350\">\n"]},{"cell_type":"markdown","metadata":{"id":"QinA9Nq7o3Gl"},"source":["# Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"norB1TaCo5D3"},"source":["This section is designed to take you through a full implementation of RLHF (Reinforcement Learning from Human Feedback). Much of this follows on directly from the PPO implementation from yesterday, with only a few minor adjustments and new concepts. You'll (hopefully) be pleased to learn that we're disposing of OpenAI's gym environment for this final day of exercises, and instead going back to our week 1 roots with TransformerLens!\n","\n","We'll start by discussing how the RL setting we've used for tasks like CartPole and Atari fits into the world of autoregressive transformer language models. We'll then go through standard parts of the PPO setup (e.g. objective function, memory buffer, rollout and learning phases) and show how to adapt them for our transformer. Finally, we'll put everything together into a `RLHFTrainer` class, and perform RLHF on our transformer!\n","\n","> **Note - these exercises assume you're running on an A100 (either a virtual machine or Colab Pro+).** If you're running on a less powerful machine e.g. A10, we recommend setting `LOW_GPU_MEM = True` below. This will switch the model to RLHF from `\"gpt2-medium\"` to `\"gpt2-small\"`,\n","as well as adjust some other parameters like the batch size, the number of tokens generated, and some hyperparamters. "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["LOW_GPU_MEM = True\n","BASE_MODEL = \"gpt2-small\" if LOW_GPU_MEM else \"gpt2-medium\"\n"]},{"cell_type":"markdown","metadata":{"id":"Dm0IF6N3o5-T"},"source":["## Content & Learning Objectives\n"]},{"cell_type":"markdown","metadata":{"id":"G6Oaifgzs_3U"},"source":["### 1️⃣ RLHF on transformer language models\n","\n","Most of the exercises today build towards the implementation of the `RLHFTrainer` class, similar to how DQN and PPO have worked these last few days.\n","\n","> ##### Learning objectives\n",">\n","> - Understand how the RL agent / action / environment paradigm works in the context of autoregressive transformer models\n","> - Understand how the RLHF algorithm works, and how it fits on top of PPO\n","> - Learn about value heads, and how they can be used to turn transformers into actor & critic networks with shared architectures\n","> - Write a full RLHF training loop, and use it to train your transformer with the \"maximize output of periods\" reward function\n","> - Observe and understand the instances of mode collapse that occur when training with this reward function\n","> - Experiment with different reward functions & training hyperparameters\n","\n","### ☆ Bonus\n","\n","This section offers some suggested ways to extend the core RLHF exercises.\n","\n","> #### Learning objectives\n",">  \n","> - Improve your RLHF implementation via techniques like differential learning rates, frozen layers, or adaptive KL penalties\n","> - Perform some exploratory mechanistic interpretability on RLHF'd models\n","> - Learn about the trlX library, which is designed to train transformers via RLHF in a way which abstracts away many of the low-level details"]},{"cell_type":"markdown","metadata":{"id":"PMAos5FlpD_7"},"source":["## Reading\n","\n","- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf) (~10 minutes)\n","    - An accessible and mostly non-technical introduction to RLHF, which discusses it in context of the full pipeline for training autoregressive transformer language models (starting with pretraining, which is what we did in the first day of last week).\n","- [RLHF+ChatGPT: What you must know](https://www.youtube.com/watch?v=PBH2nImUM5c) (~5 minutes)\n","    - The first half of this video provides a high-level overview of RLHF, discussing things like mode collapse, and relates this to the [shoggoth meme](https://i.kym-cdn.com/photos/images/original/002/546/572/bd3.png) that many of you have likely seen!"]},{"cell_type":"markdown","metadata":{"id":"eoTwssxTo6-M"},"source":["## Setup code\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# FILTERS: ~\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","ipython.run_line_magic(\"autoreload\", \"2\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# FILTERS: colab\n","# TAGS: master-comment\n","\n","import os\n","import sys\n","from importlib.metadata import distributions\n","from pathlib import Path\n","\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","chapter = \"chapter2_rl\"\n","repo = \"ARENA_3.0\"\n","branch = \"master_file\"\n","\n","# Install dependencies\n","if \"jaxtyping\" not in [dist.metadata[\"Name\"] for dist in distributions()]:\n","    %pip install transformer_lens jaxtyping eindex-callum wandb\n","\n","# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n","root = (\n","    \"/content\"\n","    if IN_COLAB\n","    else \"/root\"\n","    if repo not in os.getcwd()\n","    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",")\n","\n","if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n","    if not IN_COLAB:\n","        !sudo apt-get install unzip\n","        %pip install jupyter ipython --upgrade\n","\n","    if not os.path.exists(f\"{root}/{chapter}\"):\n","        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n","        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n","        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n","        !rm {root}/{branch}.zip\n","        !rmdir {root}/{repo}-{branch}\n","\n","\n","if f\"{root}/{chapter}/exercises\" not in sys.path:\n","    sys.path.append(f\"{root}/{chapter}/exercises\")\n","\n","os.chdir(f\"{root}/{chapter}/exercises\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import time\n","from dataclasses import dataclass\n","from functools import partial\n","from pathlib import Path\n","from typing import Callable\n","\n","import einops\n","import numpy as np\n","import torch as t\n","import torch.nn as nn\n","import wandb\n","from eindex import eindex\n","from jaxtyping import Float, Int\n","from rich import print as rprint\n","from rich.table import Table\n","from torch import Tensor\n","from transformer_lens import HookedTransformer, HookedTransformerKeyValueCache, utils\n","from transformer_lens.hook_points import HookPoint\n","\n","# Make sure exercises are in the path\n","chapter = \"chapter2_rl\"\n","section = \"part4_rlhf\"\n","root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n","exercises_dir = root_dir / chapter / \"exercises\"\n","section_dir = exercises_dir / section\n","if str(exercises_dir) not in sys.path:\n","    sys.path.append(str(exercises_dir))\n","\n","# FILTERS: ~colab\n","MAIN = __name__ == \"__main__\"\n","# END FILTERS\n","\n","import part4_rlhf.tests as tests\n","\n","device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"markdown","metadata":{"id":"Zz_P1RIGp7oD"},"source":["# 1️⃣ RLHF on transformer language models"]},{"cell_type":"markdown","metadata":{"id":"f8Sy-0JYp9cM"},"source":["## The \"transformer environment\"\n","\n","We'll start by discussing how we apply the reinforcement learning framework of states/actions/rewards to the setting of autoregressive language modelling. Lots of our intuitions should carry over from yesterday, it's just some of the details that have changed!\n"]},{"cell_type":"markdown","metadata":{"id":"_7dN9AbWp_AW"},"source":["### States, actions and episodes\n","\n","Our actor is an autoregressive language model. The actions $a_t$ are the tokens generated by the model (i.e. the action space is the model's vocabulary). The states $s_t$ are **the entire sequence up to that point** (not just the most recent token). In other words, given a state $s_t$ (sequence) and action $a_t$ (token generation), our new state is the concatenation which we'll denote as $s_{t+1} = [s_t \\; a_t]$.\n","\n","Each episode is a fixed length (i.e. all our sampled outputs will have the same number of tokens generated from them). Each episode starts with an initial \"prefix prompt\", which is chosen before the start of training.\n"]},{"cell_type":"markdown","metadata":{"id":"lP1Pss7Ip_5B"},"source":["### Rewards and value functions\n","\n","The reward $r_t$ is a function of the sequence $s_t$. Sometimes it will be a very simple function like the sum of periods `.` in the sequence, other times it'll get a bit more complicated (e.g. using a text classification model to estimate the sentiment of a sequence - we'll do this later!).\n","\n","In our case, we'll only evaluate the reward at the end of the episode. This means we don't really have a concept of discount factors here - the reward only comes once, and as soon as it comes our episode terminates.\n","\n","The value function $V(s_t)$ is an estimate of the expected sum of future rewards (up to the end of the episode), which in this case means it's an estimate of what the reward will be once we get to the end of the sequence. We'll be adding a value head to our transformer model to estimate this value function (more on this later).\n","\n","> Note - a key part of RLHF is the actual gathering of and learning from human feedback, in order to train the reward function. We're not going to be doing that here, instead we'll be working with a fixed reward function. This means our implementation today is a lot more like classical reinforcement learning, and we'll be able to structure it in a way which is very similar to yesterday's PPO implementation."]},{"cell_type":"markdown","metadata":{"id":"MBg0Goa1qAtr"},"source":["### ~~Generalized~~ Advantage Estimation\n","\n","We won't be using the GAE formula today for computing advantages, we'll just be directly computing it via $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$, where $a_t$ is the value which was actually taken and $Q(s_t, a_t)$ is the critic's estimate of the value function at this new state $s_{t+1} = [s_t \\; a_t]$.\n","\n","We can get away with this because our setup has pretty low variance when it comes to the advantage of particular actions. GAE is most helpful when it reduces variance in the advantage estimation (it does this at the cost of introducing more bias from including future value function estimates), and so it's especially useful when our environment is one with high variability when the advantage (and optimal policy) changes significantly between steps. But this doesn't really apply to us, since every action just adds a single token onto our sequence.\n","\n","That said, you're welcome to experiment with the setup and try to use GAE instead! This is suggested as a bonus exercise at the end."]},{"cell_type":"markdown","metadata":{"id":"tGkGdIt9ta2c"},"source":["<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-rl-state.png\" width=\"700\">"]},{"cell_type":"markdown","metadata":{"id":"dcDX2Jz7qCB8"},"source":["## RLHF Setup\n","\n","With this context in mind, we're now ready to look at the full RLHF setup we'll be using:\n","\n","<img src=\"https://pbs.twimg.com/media/FkLOrrPWYAAiFLF.jpg:large\" width=\"700\">\n","\n","Our autoregressive transformer model (we'll be using GPT2-Small) is the actor, and its value head will play the role of the critic. We follow the standard PPO setup:\n","\n","- In **rollout phase**, the actor generates a bunch of sequences all starting from the prefix prompt. We compute advantage estimates using the critic network (value head) and store the experiences in memory.\n","- In **learning phase**, we sample from these generated experiences (i.e. from a bunch of generated sequences of different lengths, some of which might be prefixes of each other). We compute our objective function (which is the sum of the same 3 terms as yesterday) and perform a gradient step wrt it.\n","\n","The only new element is the **KL prediction shift penalty**. This is a penalty we add to our overall loss function to stop the transformer from diverging too much from its initial distribution. We want to make our transformer maximize reward, but not in a way which causes it to become completely incoherent!\n","\n","Note that we compute $D_{KL}(\\pi_{PPO} || \\pi_{base})$, not the other way around. This is because we want to penalize our new model for generating outputs which would be **extremely unlikely under the old model**, i.e. when $\\pi_{PPO}$ is high and $\\pi_{base}$ is low. We generally want to focus our model's output into a more concentrated version of the distribution it already has. For example in RLHF, we want to keep a low probability on completely incoherent behaviour which the original model would never have generated. But on the other hand, it's clearly fine for there to be some behaviours (e.g. offensive hate speech) which have a nontrivial probability in our base model but near-zero probability in our new model - in fact this is often desireable! For more on the intuition behind this orientation of the distributions in KL divergence, see [this post](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence).\n","\n","<!-- An alternative perspective can be found from [this post](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence) - the KL divergence $D_{KL}(P || Q)$ is large when the observations $P$ give you a lot of evidence that your hypothesis $Q$ is false. We want to make sure that the original (probably coherent and sensible) model $Q$ is still a good approximation for how $P$ behaves, i.e. it shouldn't be too obvious when we observe the outputs of $P$ that they've been generated by a different model. -->"]},{"cell_type":"markdown","metadata":{},"source":["### Summary\n"," \n","Since we're using a fixed reward function rather than training it from human feedback, our RLHF implementation looks very similar to yesterday's PPO implementation. The differences are summarized in the table below:\n","\n","| |  PPO (general) | RLHF |   \n","|---|---|---|\n","| **States** | Contains partial knowledge of our environment | Sequence of tokens up to this point (and the model's internal state representation of that sequence) |\n","| **Actions** | Something our agent can do to change its state | Generating a new token, taking us to state $s_{t+1} = [s_t \\; a_t]$ |\n","| **Rewards** | A function of the state, which is computed after each new state is reached | A function of the sequence, can be computed after each new token but we'll just compute it once at the end of the sequence |\n","| **Multiple steps in parallel?** | Yes, we used `SyncVectorEnv` to parallelize the rollout phase | Yes, we'll pass batches of sequences into the transformer model, generating multiple new tokens at once |\n","| **Actor & critic networks** | Architectures can be shared (e.g. for Atari) or disjoint (e.g. for CartPole) | Actor is a transformer model, critic is a value head (so most architecture is shared) |\n","| **Advantage estimation** | Use GAE with discount factor $\\lambda$ | Often uses GAE, but we'll just use simple next-step difference $V(s_{t+1}) - V(s_t)$ |\n","| **Anything extra?** |  | KL penalty on the new policy wrt the baseline policy |\n","\n","TODO - is multiple env really equiv to the RLHF thing above?"]},{"cell_type":"markdown","metadata":{},"source":["## RLHF training args\n","\n","\n","\n","Now that you have a rough idea of what our\n","\n","We've given you a bunch of training args to use for RLHF. Most of these are similar to the PPO args, although we'll go over the ones we've added / removed.\n","\n","- We're now using `total_phases` to control how long our training lasts for, rather than using `total_timesteps`. This makes more sense for us, because the total number of timesteps (= number of actions we take = number of tokens we generate) will vary depending on the length of the sequences we generate.\n","- We've removed the arguments `gamma` and `gae_lambda` for computing the advantage function, since as discussed we'll be computing the advantage in a simpler and more direct way (you'll do this in the next exercise).\n","- We've added the following arguments related to the base model & text sampling:\n","    - `base_model`, for specifying different base models (default is `\"gpt2-small\"`)\n","    - `gen_len`, the length of the sequences we generate.\n","    - `temperature`, for controlling the sampling temperature of our sequences.\n","    - `prefix`, the string we use to generate all samples.\n","- As well as the following extra RLHF-specific arguments:\n","    - `kl_coef`, for controlling the strength of the KL prediction shift penalty.\n","    - `reward_fn`, for the reward function we use.\n","    - `normalize_reward`, for whether we normalize the reward (this won't always be necessary).\n","- We've also added two learning rates, since it makes sense to have a different learning rate for our value head and the rest of the model (more on this later!)."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["@dataclass\n","class RLHFTrainingArgs:\n","    # Basic / global\n","    seed: int = 1\n","\n","    # Wandb / logging\n","    use_wandb: bool = False\n","    wandb_project_name: str = \"RLHF\"\n","    wandb_entity: str | None = None\n","\n","    # Duration of different phases\n","    total_phases: int = 200\n","    batch_size: int = 256\n","    num_minibatches: int = 4\n","    batches_per_learning_phase: int = 2\n","\n","    # Optimization hyperparameters\n","    base_lr: float = 2e-5\n","    head_lr: float = 5e-4\n","    max_grad_norm: float = 1.0\n","    warmup_steps: int = 20\n","    final_scale: float = 0.1\n","\n","    # Computing other PPO loss functions\n","    clip_coef: float = 0.2\n","    vf_coef: float = 0.15\n","    ent_coef: float = 0.001\n","\n","    # Base model & sampling arguments\n","    base_model: str = BASE_MODEL\n","    gen_len: int = 30\n","    temperature: float = 1.0\n","    k: int = 10\n","    prefix: str = \"This is\"\n","    prepend_bos: bool = True\n","\n","    # RLHF-specific arguments\n","    kl_coef: float = 1.0\n","    reward_fn: Callable[[list[str]], Float[Tensor, \"batch\"]] = lambda x: 0.0\n","    normalize_reward: bool = True\n","\n","    def __post_init__(self):\n","        assert self.batch_size % self.num_minibatches == 0, \"batch_size should be divisible by num_minibatches\"\n","        self.minibatch_size = self.batch_size // self.num_minibatches\n"]},{"cell_type":"markdown","metadata":{"id":"MjcZTRY0qC60"},"source":["## Value head\n","\n","If you worked on the Atari exercises yesterday, then you'l be used to the idea of having shared architecture between our policy and value networks. Intuitively, this is because both networks need to learn some kind of high-level encoding of the important variables in the environment - they just do different things with this encoding.\n","\n","This leads to the idea of a **value head**. A value head is basically just a simple classifier model which we stick to one of the policy network's internal activations. You can think of this as a kind of feature extraction. When it comes to transformer models, we usually attach our value head to **the value of the residual stream at the very last layer, after layernorm but before unembedding**. Recall the key idea of **residual stream as output accumulation** - by the very last layer, it contains the most context about the overall sequence.\\*\n","\n","\\*Technically this might not always be true, since there is some evidence that components of a transformer erase information in order to write different information to the residual stream. However, in practice we usually find that the residual stream at the last layer is the most useful for downstream tasks.\n","\n","How do we implement this? Before you read further down, try to think about how you might implement this yourself, i.e. how you could extend the functionality of your `HookedTransformer` model by adding a value head, without completely rewriting the `HookedTransformer` architecture.\n","\n","<details>\n","<summary>Hint</summary>\n","\n","Think about using hook functions.\n","\n","</details>\n","\n","<details>\n","<summary>Answer - one possible method</summary>\n","\n","One method would be to directly edit the model by replacing its modules with different ones. But this is a bit awkward, because we have to also change modules which are downstream of the value head to make sure that they're only taking the residual stream as input (not the value head's output), etc.\n","\n","A different method, which is what we'll be using in these exercises, is to use **hook functions**. We can attach a hook function to the residual stream at the final layer, and have it apply our value head to the residual stream values & store the output externally. Then we can use `model.run_with_hooks` to get our logits like normal, and fetch our value estimate from the external storage object.\n","\n","We're used to using hook functions during inference mode to perform causal interventions or compute statistical functions of our activations, but they can also be used during training mode to perform computations which are part of the autograd's computational graph.\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{"id":"0DGl6d01qDvd"},"source":["### Exercise - implement `TransformerWithValueHead`\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 15-25 minutes on this exercise.\n","```\n","\n","Here is a diagram of your implementation.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/value-head-3.png\" width=\"600\">\n","\n","First define `self.base_model` and `self.value_head` in your init step (reminder that you should use `HookedTransformer.from_pretrained` to load in a pretrained model). Then rewrite the `forward` method so that it outputs both the logits from a forward pass *and* the output of the value head.\n","\n","The easiest and most direct way to get the output of the value head would be to **add a hook to the residual stream before the unembedding matrix, which computes the output of the value head and stores it externally (or as a class attribute).** You can review the material from section 1.2 if you don't remember how to use hooks, and you can refer to the diagram on the [reference page](https://arena3-chapter1-transformer-interp.streamlit.app/Reference_Page) for how to get the correct hook name.\n","\n","Why do we need to add the hook after the layernorm? The answer is that the residual stream can often [grow in magnitude over time](https://www.lesswrong.com/posts/8mizBCm3dyc432nK8/residual-stream-norms-grow-exponentially-over-the-forward). Our rewards will be normalized (see later exercise), and so we want to make sure the outputs of our value head (which are estimates of the reward) also start off normalized."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7883,"status":"ok","timestamp":1706293224815,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"MWifkerdqEqE","outputId":"f6ac3f74-f08e-4bc7-f2c0-1199def13256"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained model gpt2-small into HookedTransformer\n","All tests for `TransformerWithValueHead` passed!\n"]}],"source":["class TransformerWithValueHead(nn.Module):\n","    \"\"\"\n","    Defines a GPT model with a value head (the latter taking the last hidden state as input, post-layernorm).\n","\n","    The value head is a simple MLP with one hidden layer, and scalar output:\n","\n","        Linear(d_model -> 4*d_model)\n","        ReLU\n","        Linear(4*d_model -> 1)\n","\n","    All linear layers have biases.\n","    \"\"\"\n","\n","    base_model: HookedTransformer\n","    value_head: nn.Sequential\n","\n","    def __init__(self, base_model: HookedTransformer):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.cfg = base_model.cfg\n","\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        d_model = self.base_model.cfg.d_model\n","        self.value_head = nn.Sequential(nn.Linear(d_model, 4 * d_model), nn.ReLU(), nn.Linear(4 * d_model, 1))\n","        # SOLUTION END\n","\n","    def forward(\n","        self,\n","        input_ids: Int[Tensor, \"batch seq\"],\n","        **kwargs,\n","    ) -> tuple[Float[Tensor, \"batch seq d_vocab\"], Int[Tensor, \"batch seq\"]]:\n","        \"\"\"\n","        Args:\n","            input_ids: the input tokens to the transformer\n","            kwargs: additional keyword arguments to `base_model.forward` (or equivalently to `base_model.run_with_hooks`)\n","\n","        Returns:\n","            logits: the logits of the transformer\n","            values: the value estimates for the input sequences\n","        \"\"\"\n","        values = None\n","\n","        def calc_and_store_value_head_output(resid_post: Float[Tensor, \"batch seq d_model\"], hook: HookPoint):\n","            nonlocal values\n","            values = self.value_head(resid_post).squeeze(-1)\n","\n","        logits = self.base_model.run_with_hooks(\n","            input_ids,\n","            return_type=\"logits\",\n","            fwd_hooks=[(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)],\n","            **kwargs,\n","        )\n","        return logits, values\n","\n","\n","if MAIN:\n","    base_model = HookedTransformer.from_pretrained(BASE_MODEL)\n","\n","    # Define a reference model (we'll use this during RLHF)\n","    model = TransformerWithValueHead(base_model).to(device)\n","\n","    # Test your value head's architecture\n","    assert isinstance(model.base_model, HookedTransformer)\n","    assert isinstance(model.value_head, nn.Module)\n","    d_model = model.base_model.cfg.d_model\n","    n_params_expected = (d_model + 1) * 4 * d_model + (4 * d_model + 1)\n","    assert len(model.value_head) == 3, \"Your value head should be a `nn.Sequential` with 3 layers.\"\n","    assert sum(p.numel() for p in model.value_head.parameters()) == n_params_expected, \"Unexpected param count\"\n","\n","    # Test your class's forward pass\n","    batch_size, seq_len = 2, 10\n","    input_ids = t.randint(0, 1000, (batch_size, seq_len)).to(device)\n","    logits, values = model(input_ids)\n","    assert logits.shape == (batch_size, seq_len, model.base_model.cfg.d_vocab), \"logits should be (batch, seq, d_vocab)\"\n","    assert values.shape == (batch_size, seq_len), \"value head output should be (batch, seq)\"\n","\n","    print(\"All tests for `TransformerWithValueHead` passed!\")\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary>Solution</summary>\n","\n","Note that this solution uses the `nonlocal` keyword to return the value head output. There are many other ways to do this, e.g. using the `.ctx` hook attribute, or storing the value head output as a property before returning it.\n","\n","```python\n","SOLUTION\n","```\n","\n","</details>"]},{"cell_type":"markdown","metadata":{},"source":["## Sampling from a transformer\n","\n","Now that we essentially have our agent & critic network, let's look at how we can use them to take actions in our environment - in this case, generate new tokens.\n","\n","Before we implement this, we'll first discuss **KV caching**, which is a topic you might have come across at the end of the first day of transformer material if you got that far, but if not we'll provide a refresher here."]},{"cell_type":"markdown","metadata":{},"source":["\n","### KV caching\n","\n","KV caching is the process of storing the computed key and value vectors from previous tokens' self-attention layers to avoid redundant recomputation when generating text autoregressively. The key idea is that when computing the next token in a sequence, we only need these key and value tensors, because they (along with the key, query and value vectors at the newest sequence position) fully determine what information is moved to the new sequence position during the attention layers. For example, we don't need the query vectors of earlier positions because we don't care what information moves _to_ those early sequence positions, we only care about what information moves _from_ them _to_ the final sequence position!\n","\n","If this is confusing, the following diagram might help you understand this better. Note that you don't need to understand all the tensor operations this deeply, because we won't ask you to implement KV caching yourself here!\n","\n","<details>\n","<summary>KV caching diagram</summary>\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tl-cache-full.png\" width=\"1200\">\n","\n","</details>\n","\n","The most important part of KV caching isn't saving memory by reducing the number of tensors you have to deal with, rather it's saving on transformer computation. In particular, computing attention scores between all pairs of tokens is the only part of the transformer's forward pass that scales according to the sequence length squared rather than just the sequence length. But with KV caching we only need to compute the vector of attention scores from all tokens to the final token, and so our computational cost only scales linearly in sequence length.\n","\n","The code below demonstrates how to use caching in TransformerLens. If the input sequence has length `n`, then we require the KV cache to hold key & value vectors of shape `(batch_size, n-1, n_heads, d_head)` for every layer, since these represent the vectors computed at all positions **before** the last one.\n","\n","Make sure you understand how this code works (mostly the caching part rather than the logging-with-hooks part), since you'll have to essentially reproduce this code later on."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Non-cached fwd pass : torch.Size([1, 11, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","Cached fwd pass     : torch.Size([1, 1, 768])\n","\n","<|endoftext|>The answer to life, the universe and everything is in your hands.\\n\\nThe answer to life\n"]}],"source":["# TAGS: main\n","\n","prompt = \"The answer to life, the universe and everything is\"\n","input_tokens = base_model.to_tokens(prompt)\n","batch_size, seq_len = input_tokens.shape\n","\n","# Add hook function to print the shape of the layer-0 attention output\n","with base_model.hooks(\n","    fwd_hooks=[(utils.get_act_name(\"attn_out\", 0), lambda attn_out, hook: print(f\"{attn_out.shape}\"))]\n","):\n","    # Initialize the cache (holds no values at first)\n","    kv_cache = HookedTransformerKeyValueCache.init_cache(base_model.cfg, device, batch_size)\n","\n","    with t.inference_mode():\n","        # Forward pass with no caching (this fills the cache in-place)\n","        print(\"Non-cached fwd pass : \", end=\"\")\n","        logits = base_model.forward(input_tokens, past_kv_cache=kv_cache)\n","\n","        # Forward pass with caching - we only need to pass in the new tokens! Cache holds vectors for all but last seqpos\n","        for i in range(10):\n","            new_tokens = logits[:, [-1]].argmax(dim=-1)\n","            input_tokens = t.cat([input_tokens, new_tokens], dim=-1)\n","            assert kv_cache.entries[0].past_keys.shape == (\n","                batch_size,\n","                input_tokens.shape[1] - 1,\n","                base_model.cfg.n_heads,\n","                base_model.cfg.d_head,\n","            )\n","\n","            print(\"Cached fwd pass     : \", end=\"\")\n","            logits = base_model.forward(new_tokens, past_kv_cache=kv_cache)\n","\n","# Print the sampled completions\n","print()\n","for sampled_completion in base_model.to_string(input_tokens):\n","    print(sampled_completion.replace(\"\\n\", \"\\\\n\"))\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","\n","<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Non-cached fwd pass : torch.Size([1, 11, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","Cached fwd pass     : torch.Size([1, 1, 1024])\n","\n","<|endoftext|>The answer to life, the universe and everything is in the eye of the beholder.\\n\\n\n","</pre>"]},{"cell_type":"markdown","metadata":{},"source":["### Sampling from a transformer\n","\n","We'll also offer a brief refresher on sampling from a transformer, if you've forgotten how it works (or haven't completed the first day of last week's exercises).\n","\n","The two important concepts to understand in this section are:\n","\n","- **Temperature scaling** - we take the logits and divide by `temperature` before converting to probabilities. This controls how deterministic or stochastic the sampling is - higher temperatures means the logits are pushed closer together, so the model is more likely to sample from a wider range of tokens and produce more diverse outputs.\n","- **Top-k sampling** - we take the k highest value logits, convert those to probabilities, and then sample from those according to their values. For example, if our logits were `[5, 5, 4, 3]` and `k=2` then we'd be sampling uniformly from the first 2 tokens.\n","\n","<!-- The sampling here should be implemented as **top-k sampling**, which means we take the k highest value logits, convert those to probabilities, and then sample from those according to their values. For example, if our logits were `[5, 5, 4, 3]` and `k=2` then we'd be sampling uniformly from the first 2 tokens. You might find [`torch.distributions.Categorical`](https://pytorch.org/docs/stable/distributions.html) useful here.\n","\n","You should also use **temperature**, which means we take the logits and divide by `temperature` before converting to probabilities. This controls how deterministic or stochastic the sampling is - higher temperatures means the logits are pushed closer together, so the model is more likely to sample from a wider range of tokens and produce more diverse outputs. Note that it doesn't matter whether we apply temperature before or after we filter for the top-k logits - can you see why? -->\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise - implement `sample_from_transformer` with KV caching\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend up to 10-30 minutes on this exercise.\n","```\n","\n","You should implement the `sample_from_transformer` function below. A few notes:\n","\n","- You might find [`torch.distributions.Categorical`](https://pytorch.org/docs/stable/distributions.html) useful for the top-k sampling: if `logits` has shape `(batch_size, d_vocab)` with each `logits[:, 0]` being a different distribution over vocab, then `Categorical(logits=logits).sample()` returns a vector of length `batch_size` containing a single sampled token id from each batch\n","- It doesn't matter whether we apply temperature before or after we filter for the top-k logits - can you see why?\n","- You can assume the KV cache has already been initialized, i.e. its key & value vectors have length 1 less than the input sequence (like for the second forward pass in the demo code above)\n","\n","Note - we've called this function `transformer_step`, because it's equivalent to our `PPOAgent.play_step` method from yesterday. In that code, we created a `PPOAgent` class which was responsible for managing interactions between the agent and the environment. Here we don't need this abstraction, because our agent and environment aren't separated in a crisp way and so we get no benefit from it."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<|endoftext|>The answer to life, the universe and everything is not just a question of the past. It's\n","<|endoftext|>The answer to life, the universe and everything is not in your head, but in your mind.\n","<|endoftext|>The answer to life, the universe and everything is in the hands of a man. It's up\n"]}],"source":["from torch.distributions import Categorical\n","\n","\n","@t.no_grad()\n","def transformer_step(\n","    model: TransformerWithValueHead,\n","    new_tokens: Int[Tensor, \"batch\"],\n","    kv_cache: HookedTransformerKeyValueCache,\n","    k: int,\n","    temperature: float,\n",") -> tuple[Int[Tensor, \"batch\"], Float[Tensor, \"batch\"], Float[Tensor, \"batch\"]]:\n","    \"\"\"\n","    Samples from the model, returning a tensor of new token IDs.\n","\n","    Inputs:\n","        model:       the transformer we generate samples from\n","        new_tokens:  the most recent tokens generated by the model (which aren't in the KV cache)\n","        kv_cache:    the KV cache which we used when generating the new tokens\n","        k:           the number of top-k tokens to sample from\n","        temperature: the temperature to use for sampling\n","\n","    Returns:\n","        sample_ids:  the token IDs of the model's output, when we sample with temperature & top-k\n","        logprobs:    the logprobs of the model's output\n","        values:      the value estimates for the input sequences (i.e. before the new tokens were generated)\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    # Get logits (and update our cache), and remove dummy seq_len dimension\n","    # logits: Tensor = base_model.forward(input_ids[:, [-1]], past_kv_cache=kv_cache)[:, 0]\n","    batch_size = new_tokens.shape[0]\n","    logits, values = model.forward(new_tokens[:, None], past_kv_cache=kv_cache)\n","    logits = logits[:, 0]  # shape [batch_size, d_vocab]\n","    values = values[:, 0]  # shape [batch_size,]\n","\n","    # Apply temperature scaling\n","    logits_scaled = logits / temperature\n","\n","    # Apply top-k filtering, and sample tokens\n","    top_k_logits, top_k_token_ids = logits_scaled.topk(k, dim=-1)\n","    sampled_token_topk_indices = Categorical(logits=top_k_logits).sample()  # get indices within the group of topk\n","    sample_ids = top_k_token_ids[range(batch_size), sampled_token_topk_indices]  # get actual token ids\n","\n","    # Get logprobs of the sampled tokens\n","    logprobs = logits.log_softmax(-1)[range(batch_size), sample_ids]\n","\n","    return sample_ids, logprobs, values\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    # TODO - maybe have this be a blackboxed test function, since I'm basically repeating the demo code above?\n","\n","    # Initialize our KV cache, and fill it in by doing a single forward pass\n","    batch_size = 3\n","    input_tokens = base_model.to_tokens(prompt).repeat(batch_size, 1)\n","    kv_cache = HookedTransformerKeyValueCache.init_cache(base_model.cfg, device, batch_size)\n","    with t.inference_mode():\n","        logits = base_model.forward(input_tokens[:, :-1], past_kv_cache=kv_cache)\n","\n","    # Perform sampling (updating our cache inplace each time)\n","    for _ in range(10):\n","        new_tokens, logprobs, values = transformer_step(model, input_tokens[:, -1], kv_cache, k=5, temperature=0.8)\n","        assert new_tokens.shape == (batch_size,)\n","        assert logprobs.shape == (batch_size,)\n","        input_tokens = t.cat([input_tokens, new_tokens[:, None]], dim=-1)\n","\n","    # Print the sampled completions\n","    for sampled_completion in base_model.to_string(input_tokens):\n","        print(sampled_completion.replace(\"\\n\", \"\\\\n\"))\n","\n","    # Check KV cache has been updated correctly\n","    assert len(kv_cache.entries) == base_model.cfg.n_layers\n","    assert kv_cache.entries[0].past_keys.shape[:2] == (batch_size, input_tokens.shape[1] - 1)\n","    assert kv_cache.entries[0].past_values.shape[:2] == (batch_size, input_tokens.shape[1] - 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","TAGS: st-dropdown[Click to see the expected output]\n","\n","<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><|endoftext|>The answer to life, the universe and everything is simple.\\n\\nThe answer to life, the\n","<|endoftext|>The answer to life, the universe and everything is that it's not possible to be both.\\n\n","<|endoftext|>The answer to life, the universe and everything is in the smallest, simplest form imaginable, but it</pre>"]},{"cell_type":"markdown","metadata":{"id":"9YORqjAZqJ4N"},"source":["### Exercise - implement `reward_fn_char_count`\n","\n","```c\n","Difficulty: 🔴⚪⚪⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend 5-10 minutes on this exercise.\n","```\n","\n","We'll start with a very basic reward function: counting the total number of periods in the sequence.\n","\n","An interesting thing to note about this reward function - it counts over all characters, but the episode length is defined in terms of tokens. This means that theoretically our model could reward hack by outputting tokens with more than one `.` character. This particular model's vocabulary happens to include the token `'.' * 64`, so rewards would be through the roof if this was ever generated! However, remember that RL is about performing actions, getting feedback on those actions, and using that feedback to influence your policy. The token `'.' * 64` is so unlikely to ever be generated that it'll probably never be positively reinforced, and we avoid this problem."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"oH_F4DgNqJjr","outputId":"b3cb2b59-9ff0-42e9-e361-37b80f53de62"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests for `reward_fn_char_count` passed!\n"]}],"source":["def reward_fn_char_count(generated_sample: list[str], char: str = \".\") -> Float[Tensor, \"batch\"]:\n","    \"\"\"\n","    Reward function (counting number of instances of a particular character), evaluated on the generated samples. The\n","    return type should be a tensor of floats.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    return t.tensor([item.count(char) for item in generated_sample], device=device, dtype=t.float)\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    # Test your reward function\n","    A = \"This is a test.\"\n","    B = \"......\"\n","    C = \"Whatever\"\n","\n","    t.testing.assert_close(reward_fn_char_count([A]), t.tensor([1.0], device=device))\n","    t.testing.assert_close(reward_fn_char_count([A, B, C]), t.tensor([1.0, 6.0, 0.0], device=device))\n","    t.testing.assert_close(reward_fn_char_count([A], \" \"), t.tensor([3.0], device=device))\n","    print(\"All tests for `reward_fn_char_count` passed!\")\n"]},{"cell_type":"markdown","metadata":{"id":"4DCtaWmDqLbq"},"source":["### Exercise - brainstorm your reward function\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend ~5 minutes on this exercise.\n","```\n","\n","Take 5 minutes (on your own or with a partner) to brainstorm how the model might be able to maximize the output of periods in ways which don't produce incoherent output (e.g. collapsing into only outputting periods). Remember we have a KL penalty with the reference model, meaning the model is penalized for producing outputs which would be very unlikely under the original model. What ideas can you come up with? When you train your model and observe the output, you should come back here and see how many of the period-maximizing behaviours you predicted actually occur.\n","\n","This exercise is a great way to start thinking about the effects of different reward functions - although it's only a toy example, it still illustrates the important alignment concept that the behaviour induced by certain reward functions might not always be what you expect!\n","\n","<details>\n","<summary>Spoiler - which behaviours will your model pick up?</summary>\n","\n","The strategies adopted by the model very a lot depending on the prefix string, also thanks to mode collapse it will often find one of these behaviours and entirely ignore the others.\n","\n","Some common strategies include:\n","\n","- Shorter sentences\n","- Repeating `U.S.` or `U.S.A.` (using the prefix prompt `\"There is\"`, this seems to be by far the most common strategy)\n","- Library versions e.g. `Python 2.7.12` or `the 2.6.0.2 release`\n","- Names with initials e.g. `C. S. Lewis` or titles e.g. `Dr.` and `PhD.`\n","- Abbreviations e.g. `Data-R.A.R. series` or `\"L.A. Times\"`\n","- Decimals in numbers e.g. `9.5cm x 7.5 cm`\n","- Triple periods e.g. `the man . . . the woman . . .`\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"zx9HhRWbqMU9"},"source":["### Exercise - implement `normalize_reward`\n","\n","```c\n","Difficulty: 🔴⚪⚪⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend ~5 minutes on this exercise.\n","```\n","\n","Following advice from Ziegler el al. (2019), it's important to normalize the reward function over each batch (i.e. subtract mean and divide by std dev). We've been able to get away with not doing this so far because our reward functions were usually nicely bounded, e.g. the reward was always zero or one in cartpole (and even in our reward shaping it was still in the zero-one range). But if we're working with reward functions that could be much higher variance such as the number of periods in a generated sequence, then we should normalize.\n","\n","Note - we're not super strict about this function; the denominator being `std + eps` or `(var + eps).sqrt()` are both fine."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"7aH11E-wqNE2","outputId":"fdb45ad0-03ca-44ae-dc91-cdb0c8ea1b82"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests for `normalize_reward` passed!\n"]}],"source":["def normalize_reward(reward: Float[Tensor, \"batch\"], eps=1e-5) -> Float[Tensor, \"batch\"]:\n","    \"\"\"\n","    Normalizes the reward function values over the batch of sequences.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    return (reward - reward.mean()) / (reward.std() + eps)\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    # Test your reward normalization function\n","    reward = 10 + 5 * t.randn(10_000)\n","    reward_normalized = normalize_reward(reward)\n","    assert reward_normalized.mean().abs() < 1e-4\n","    assert (reward_normalized.std() - 1).abs() < 1e-4\n","    # Test edge case of zero reward\n","    reward = t.zeros(5)\n","    reward_normalized = normalize_reward(reward)\n","    assert reward_normalized.abs().sum() < 1e-4\n","\n","    print(\"All tests for `normalize_reward` passed!\")\n"]},{"cell_type":"markdown","metadata":{"id":"CnJSAFxFqPdK"},"source":["### Exercise - implement `get_advantages`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-20 minutes on this exercise.\n","```\n","\n","As we discussed earlier, your advantage function doesn't need to use GAE like yesterday. Instead, we'll base our estimates on the simple formula:\n","\n","$$\n","A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n","$$\n","\n","In place of $Q(s_t, a_t)$ we'll use the **one-step Q estimates**, i.e. our value function estimates after taking action $a_t$ at step $s_t$, meaning we're at new state $s_{t+1} = [s_t \\; a_t]$. If $t < T$ (i.e. we're before the final sequence position) then the one-step Q estimates just equal the value function estimates $V(s_{t+1})$, but if $t=T$ then we can just use the known reward $r_t$ for the whole sequence (e.g. in our case that's the number of periods in the generated sequence).\n","\n","The diagram below should help explain things. Note that the output should have shape `[minibatch_size, gen_length]` where `gen_length` is defined as `seq_len - prefix_len` i.e. the number of tokens our model generated. See the diagram below to help illustrate things, and make sure you slice your tensors carefully to match the diagram!\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rlhf-advantages-2.png\" width=\"900\">"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"s_SfXUF4qQHL","outputId":"c1e8b1ab-7bfa-4fbb-e42c-2762a156efd8"},"outputs":[],"source":["@t.no_grad()\n","def compute_advantages(\n","    values: Float[Tensor, \"minibatch_size seq_len\"],\n","    rewards: Float[Tensor, \"minibatch_size\"],\n","    prefix_len: int,\n",") -> Float[Tensor, \"minibatch_size gen_len\"]:\n","    \"\"\"\n","    Computes the advantages for the PPO loss function, i.e. A_pi(s, a) = Q_pi(s, a) - V_pi(s). In this formula we\n","    replace Q(s, a) with the 1-step Q estimates, and V(s) with the 0-step value estimates.\n","\n","    Inputs:\n","        values:  the value estimates immediately before each generated token (in diagram this is 2, 3, 4, 5, 6)\n","        rewards: the rewards for the entire generated sequence\n","        prefix_len: the length of the prefix (i.e. the length of the initial prompt)\n","\n","    Returns:\n","        advantages: the advantages for each token in the generated sequence (not the entire sequence)\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    # Concat value estimates with final rewards, to get the 1-step Q estimates\n","    one_step_q_est = t.cat([values[:, prefix_len:-1], rewards[:, None]], dim=-1)\n","    zero_step_value_est = values[:, prefix_len - 1 : -1]\n","    return one_step_q_est - zero_step_value_est\n","    # SOLUTION END\n","\n","\n","# TODO - decide how this will work\n","# if MAIN:\n","#     tests.test_compute_advantages(compute_advantages)\n"]},{"cell_type":"markdown","metadata":{"id":"0AFxpXyMqQ13"},"source":["## Memory\n","\n","We've given you an implementation of the `ReplayMemory` and `ReplayMinibatch` classes. Some notes on how these implementations differ from our corresponding PPO implementations, mostly in ways which make them strictly simpler:\n","\n","- The objects stored by `ReplayMinibatch` are different:\n","    - `actions` is not stored any more, because the actions (tokens generated) are in contained within the sequences themselves\n","    - `terminated` or `truncated` are not stored, because all our seqs last for exactly `gen_length` generations and there's no termination criteria\n","    - A new object `ref_logits` is stored, used to compute the KL penalty with our reference model\n","- Our `get_minibatches` method shuffles over the batch of sequences, rather than shuffling over individual experiences (which here would mean individual tokens)\n","- We don't do any computation inside `get_minibatches` e.g. advantages, instead we'll do it in `rollout_phase` (see later)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"PKog2wHQqRqJ"},"outputs":[],"source":["#\n","# TODO - this should be a TL function\n","\n","\n","def repeat_for_batch_size(kv_cache: HookedTransformerKeyValueCache, batch_size: int):\n","    for entry in kv_cache.entries:\n","        entry.past_keys = entry.past_keys.repeat(batch_size, 1, 1, 1)\n","        entry.past_values = entry.past_values.repeat(batch_size, 1, 1, 1)\n","    kv_cache.previous_attention_mask = kv_cache.previous_attention_mask.repeat(batch_size, 1)\n","\n","\n","@dataclass\n","class ReplayMinibatch:\n","    \"\"\"\n","    Samples from the replay memory.\n","    \"\"\"\n","\n","    sample_ids: Float[Tensor, \"minibatch_size seq_len\"]\n","    logprobs: Float[Tensor, \"minibatch_size seq_len\"]\n","    advantages: Float[Tensor, \"minibatch_size gen_len\"]\n","    returns: Float[Tensor, \"minibatch_size gen_len\"]\n","    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"]\n","\n","\n","class ReplayMemory:\n","    def __init__(\n","        self,\n","        prefix_len: int,\n","        batches_per_learning_phase: int,\n","        batch_size: int,\n","        num_minibatches: int,\n","        sample_ids: Float[Tensor, \"batch_size seq_len\"],\n","        logprobs: Float[Tensor, \"batch_size gen_len\"],\n","        advantages: Float[Tensor, \"batch_size gen_len\"],\n","        values: Float[Tensor, \"batch_size gen_len\"],\n","        rewards: Float[Tensor, \"batch_size\"],\n","        ref_logits: Float[Tensor, \"batch_size gen_len d_vocab\"],\n","    ):\n","        \"\"\"\n","        Initializes the replay memory, with all the data generated from the rollout phase at once.\n","\n","        NOTE - the advantages are (batch_size, gen_len) because we only compute advantages for the generated tokens, the\n","        other tensors are (batch_size, seq_len) because they are computed for all tokens.\n","\n","        TODO - maybe these shape annotations are wrong?\n","        \"\"\"\n","        self.prefix_len = prefix_len\n","        self.batches_per_learning_phase = batches_per_learning_phase\n","        self.batch_size = batch_size\n","        self.num_minibatches = num_minibatches\n","\n","        # TODO - add assertions to check shapes\n","\n","        self.sample_ids = sample_ids\n","        self.logprobs = logprobs\n","        self.advantages = advantages\n","        self.values = values\n","        self.ref_logits = ref_logits\n","        self.rewards = rewards\n","\n","    def get_minibatches(self) -> list[ReplayMinibatch]:\n","        \"\"\"\n","        Generates a list of minibatches by randomly sampling from the replay memory. Each sequence appears exactly\n","        `batches_per_learning_phase` times in total.\n","        \"\"\"\n","        minibatches = []\n","\n","        # TODO - what even is this now?\n","        returns = self.advantages + self.values[:, self.prefix_len - 1 : -1]  # changed from `-self.gen_len - 1: -1`\n","\n","        for _ in range(self.batches_per_learning_phase):\n","            for indices in t.randperm(self.batch_size).reshape(self.num_minibatches, -1):\n","                minibatches.append(\n","                    ReplayMinibatch(\n","                        sample_ids=self.sample_ids[indices],\n","                        logprobs=self.logprobs[indices],\n","                        advantages=self.advantages[indices],\n","                        returns=returns[indices],\n","                        ref_logits=self.ref_logits[indices],\n","                    )\n","                )\n","\n","        return minibatches\n"]},{"cell_type":"markdown","metadata":{},"source":["As mentioned above, we won't be implementing an `RLHFAgent` class, since this isn't a helpful abstraction for this project. Most of the code that we would have written for this class has already been implemented in the `transformer_step` function that you wrote earlier."]},{"cell_type":"markdown","metadata":{"id":"AM_perKpqYpc"},"source":["## Objective function"]},{"cell_type":"markdown","metadata":{"id":"ddJzbSq1qSh6"},"source":["### Exercise - implement `calc_kl_penalty`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","Now, you'll implement the KL penalty function. As discussed, the purpose of this function is to make sure your new model doesn't diverge too much from the old model. We'll be using the KL divergence between the old and new models' logit distributions.\n","\n","The formula for KL divergence of two distributions $(P, Q)$ is $\\sum_i P_i \\log (P_i / Q_i)$. Recall that we want our new logits to be $P$ and reference logits to be $Q$ (because this penalizes our new model for generating outputs which would be very unlikely under the original reference model).\n","\n","You should take the mean over batch and sequence position dims, since each token represents a separate observation and action.\n","\n","There are a couple more important points to address, before implementing this function:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JbnyRQVJqb8A"},"source":["#### Prefix tokens vs generated tokens\n","\n","**We only want to compute KL for the new tokens generated.** This means we want to look at the logits for the tokens at sequence positions `prefix_len-1:-1`, since these correspond to **predictions for tokens after the prefix, within the generated sequence**. There's no point penalizing the model for having a different probability distribution on the tokens prefix prompt, since this won't directly induce a behavioural change (our prefix prompt is always fixed). We only care about changing the model's behaviour on the actual tokens it generates."]},{"cell_type":"markdown","metadata":{"id":"9N1_VD6mqcxi"},"source":["\n","\n","#### Numerical stability\n","\n","There can be a problem when dealing with extreme values, since operations like log / exp are numerically unstable and can give you NaN values. To avoid this, make sure your order of operations is correct. **You should always start with `log_softmax` to compute logprobs, then take `exp` to compute probabilities,** and avoid functions `log` and `softmax`. This is because `log_softmax` subtracts a constant from all the values to avoid any of them being extremely large when we calculate the necessary exponentials (recall that the softmax operation is translation invariant i.e. we can add the same value to all logits and the probs / logprobs are still the same). We can then take `exp` of the result to get probabilities because taking `exp` of negative values is numerically stable. On the other hand, taking `softmax` to get (possibly very small) probabilities and then `log` to get (possibly very extreme negative) logprobs is not numerically stable."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"vipYqvhKqTIl","outputId":"6949e15c-b579-4738-9b85-b32b2a83d282"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests in `test_calc_kl_penalty` passed!\n","All tests in `test_calc_kl_penalty_stability` passed!\n"]}],"source":["def calc_kl_penalty(\n","    logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"],\n","    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"],\n","    kl_coef: float,\n","    prefix_len: int,\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"\n","    Computes the KL divergence between the logits and the reference logits, scaled\n","    by the penalty function. This is used to stop the learned policy from diverging\n","    too much from the original reference model's policy.\n","\n","    logits:\n","        The logits of the generated samples (under the new model).\n","    ref_logits:\n","        The logits of the generated samples (under the reference model).\n","    kl_coef:\n","        The coefficient of the KL penalty.\n","    prefix_len:\n","        The length of the prefix to ignore when computing the KL divergence.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    ref_logprobs = ref_logits.log_softmax(-1)\n","    logprobs = logits.log_softmax(-1)\n","    probs = logprobs.exp()\n","\n","    kl_div = (probs * (logprobs - ref_logprobs))[:, prefix_len - 1 : -1].sum(-1)\n","\n","    return kl_coef * kl_div.mean()\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    tests.test_calc_kl_penalty(calc_kl_penalty)\n","    tests.test_calc_kl_penalty_stability(calc_kl_penalty)\n"]},{"cell_type":"markdown","metadata":{"id":"oskplVR7qT7d"},"source":["### Exercise - (re)implement `compute_entropy_bonus`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to ~10 minutes on this exercise.\n","```\n","\n","Next, we'll implement the entropy bonus function again. Rather than working with `probs.entropy()` like yesterday, we'll need to compute entropy directly from the logits, and take the mean over batch and sequence position dimensions.\n","\n","The formula for entropy of a distribution $P$ is $- \\sum_i P_i \\log P_i$.\n","\n","As for the previous exercise, you'll need to slice the logits to only look at the new tokens generated. Also, you'll need to take the same numerical stability precautions."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"suwCpipcqUnG","outputId":"dc928415-889d-48c5-861a-0cb3338918a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests in `test_calc_entropy_bonus` passed!\n","All tests in `test_calc_entropy_bonus_stability` passed!\n"]}],"source":["def calc_entropy_bonus(\n","    logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"], ent_coef: float, prefix_len: int\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"\n","    Return the entropy bonus term, suitable for gradient ascent.\n","\n","    logits:\n","        the logits of the tokens generated by the model.\n","    ent_coef:\n","        the coefficient for the entropy loss, which weights its contribution to the overall objective function.\n","    prefix_len:\n","        The length of the prefix to ignore when computing the KL divergence.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    logprobs = logits.log_softmax(dim=-1)\n","    probs = logprobs.exp()\n","    entropy = -(probs * logprobs)[:, prefix_len - 1 : -1].sum(dim=-1)\n","    return ent_coef * entropy.mean()\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    tests.test_calc_entropy_bonus(calc_entropy_bonus)\n","    tests.test_calc_entropy_bonus_stability(calc_entropy_bonus)\n"]},{"cell_type":"markdown","metadata":{"id":"EQEhir6EqVZu"},"source":["### Other objective function terms\n","\n","Since the other two terms in our objective function (value function loss and clipped surrogate objective) are pretty much identical to yesterday's, we've provided them for you (taken from yesterday's solutions code).\n","\n","Note that **these functions all only take input of shape `gen_len`** (because advantages and returns are created with shape `gen_len`). So we don't need to do any positional slicing in these functions, like for the last two. When it comes to using these loss functions."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"QWN_Q9FjqWLH"},"outputs":[],"source":["def calc_value_function_loss(\n","    values: Float[Tensor, \"minibatch_size gen_len\"], mb_returns: Float[Tensor, \"minibatch_size gen_len\"], vf_coef: float\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"Compute the value function portion of the loss function.\n","\n","    values:\n","        the value function predictions for the sampled minibatch (using the updated critic network)\n","    mb_returns:\n","        the target for our updated critic network (computed as `advantages + values` from the old network)\n","    vf_coef:\n","        the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n","    \"\"\"\n","    assert (\n","        values.shape == mb_returns.shape\n","    ), f\"Shape mismatch: {values.shape=}, {mb_returns.shape=}. Did you slice 'values' tokens correctly?\"\n","    return 0.5 * vf_coef * (values - mb_returns).pow(2).mean()\n","\n","\n","def calc_clipped_surrogate_objective(\n","    logprobs: Float[Tensor, \"minibatch_size gen_len\"],\n","    mb_logprobs: Float[Tensor, \"minibatch_size gen_len\"],\n","    mb_advantages: Float[Tensor, \"minibatch_size gen_len\"],\n","    clip_coef: float,\n","    eps: float = 1e-8,\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"Return the clipped surrogate objective, suitable for maximisation with gradient ascent.\n","\n","    logprobs:\n","        the logprobs of the action taken by the agent, according to the new policy\n","    mb_logprobs:\n","        logprobs of the actions taken in the sampled minibatch (according to the old policy)\n","    mb_advantages:\n","        advantages calculated from the sampled minibatch\n","    clip_coef:\n","        amount of clipping, denoted by epsilon in Eq 7.\n","    eps:\n","        used to add to std dev of mb_advantages when normalizing (to avoid dividing by zero)\n","    \"\"\"\n","    assert (\n","        logprobs.shape == mb_logprobs.shape == mb_advantages.shape\n","    ), f\"Shape mismatch: {logprobs.shape=}, {mb_logprobs.shape=}, {mb_advantages.shape=}. Did you create logprobs using 'get_logprobs' correctly?\"\n","\n","    logits_diff = logprobs - mb_logprobs\n","\n","    r_theta = t.exp(logits_diff)\n","\n","    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n","\n","    non_clipped = r_theta * mb_advantages\n","    clipped = t.clip(r_theta, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n","\n","    return t.minimum(non_clipped, clipped).mean()\n"]},{"cell_type":"markdown","metadata":{"id":"jpz4t-54qXIM"},"source":["### Exercise - implement `get_logprobs`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","You'll notice that the functions above take logprobs of shape `(minibatch_size, gen_len)`, i.e. the logprobs on correct tokens for all the tokens generated by the model. It'll be useful to implement the following function (which should be pretty familiar to you by now).\n","\n","This function takes `logits` and the corresponding `tokens` which were used as input, and returns an array of logprobs for the correct next tokens. Note, we're only taking the logits which are predicting tokens **after the prefix**, hence we've given you a `prefix_len` argument for this function. The diagram below should help explain this. Note, you can assume `prefix_len` will always be at least 1.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/get-correct-logprobs-3-solid.png\" width=\"520\">\n","\n","You can implement this function using regular indexing, tools like `torch.gather`, or with the `eindex` library which should be included in your dependencies (see [here](https://www.perfectlynormal.co.uk/blog-eindex) for how to use this library)."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"hiMI_1krqeqa","outputId":"73abd0b4-dd66-4808-b5f9-41a649f46f84"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests for `get_logprobs` passed (for prefix_len = None)!\n","All tests for `get_logprobs` passed (for prefix_len > 0)!\n"]}],"source":["def get_logprobs(\n","    logits: Float[Tensor, \"batch seq_len vocab\"],\n","    tokens: Int[Tensor, \"batch seq_len\"],\n","    prefix_len: int | None = None,\n",") -> Float[Tensor, \"batch gen_len\"]:\n","    \"\"\"\n","    Returns correct logprobs for the given logits and tokens, for all the tokens after the prefix tokens (which have\n","    length equal to `prefix_len`).\n","\n","    If prefix_len = None then we return shape (batch, seq_len-1).\n","    If not, then we return shape (batch, seq_len-prefix_len) representing the predictions for all toks after the prefix.\n","    \"\"\"\n","    # Using no prefix_len argument is equivalent to prefix_len=1\n","    prefix_len = prefix_len or 1\n","\n","    # Slice logprobs and tokens, so that each logprob matches up with the token which it predicts\n","    logprobs = logits[:, prefix_len - 1 : -1].log_softmax(-1)\n","    correct_tokens = tokens[:, prefix_len:]\n","\n","    # correct_logprobs[batch, seq] = logprobs[batch, seq, correct_tokens[batch, seq]]\n","    correct_logprobs = eindex(logprobs, correct_tokens, \"batch seq [batch seq] -> batch seq\")\n","\n","    assert correct_logprobs.shape == (tokens.shape[0], tokens.shape[1] - prefix_len)\n","    return correct_logprobs\n","\n","\n","if MAIN:\n","    tests.test_get_logprobs(get_logprobs)\n"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary>Solution</summary>\n","\n","```python\n","SOLUTION\n","```\n","\n","Alternate solution, using `torch.gather`:\n","\n","```python\n","def get_logprobs(logits, tokens, prefix_len):\n","    prefix_len = prefix_len or 1\n","    logprobs = logits[:, prefix_len - 1 : -1].log_softmax(-1)\n","    correct_tokens = tokens[:, prefix_len:]\n","    return t.gather(logprobs, -1, correct_tokens[:, :, None])[:, :, 0]\n","```\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"URUeKDz7qiym"},"source":["## Optimizer & Scheduler"]},{"cell_type":"markdown","metadata":{"id":"4tZwkUSPqjfs"},"source":["### Exercise - implement `get_optimizer`\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","We need to be a bit careful when defining our optimizer. It makes no sense to have the same learning rate for our original model as we do for our value head. The value head was randomly initialized and has no idea what it's doing, but our model is pretrained and so it already has weights which have been trained to effectively extract features from text.\n","\n","The syntax for using parameter groups in an optimizer is as follows:\n","\n","```python\n","parameter_groups = [\n","    {\"params\": [param1, param2, ...], \"lr\": lr1},\n","    {\"params\": [param3, param4, ...], \"lr\": lr2},\n","]\n","```\n","\n","where `params` is a list (or iterable) of parameters, and `lr` is the learning rate for these parameters.\n","\n","You should fill in the function `get_optimizer` below, so that the value head's parameters all have learning rate `args.head_learning_rate` and the base model's parameters all have learning rate `args.base_learning_rate`.\n","\n","Remember that we're using `maximize=True` with our optimizer (since we're maximizing an objective function rather than minimizing a loss function)."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706293226664,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"tjl0NSgJqkJi","outputId":"c95ea7ab-98c9-4ce6-fce9-326ade684c41"},"outputs":[{"name":"stdout","output_type":"stream","text":["All tests for `get_optimizer` passed!\n"]}],"source":["def get_optimizer(args: RLHFTrainingArgs, model: TransformerWithValueHead) -> t.optim.Optimizer:\n","    \"\"\"\n","    Returns an Adam optimizer for the model, with the correct learning rates for the base and head.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    return t.optim.Adam(\n","        [\n","            {\"params\": model.base_model.parameters(), \"lr\": args.base_lr},\n","            {\"params\": model.value_head.parameters(), \"lr\": args.head_lr},\n","        ],\n","        maximize=True,\n","    )\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    args = RLHFTrainingArgs()\n","    optimizer = get_optimizer(args, model)\n","\n","    assert len(optimizer.param_groups) == 2, \"Your optimizer should have two parameter groups.\"\n","\n","    for param_group in optimizer.param_groups:\n","        assert param_group[\"maximize\"], \"Should be maximize=True.\"\n","        if len(param_group[\"params\"]) <= 4:\n","            assert param_group[\"lr\"] == args.head_lr, \"LR for value head should be `args.head_lr`.\"\n","        else:\n","            assert param_group[\"lr\"] == args.base_lr, \"LR for base should be `args.base_lr`.\"\n","\n","    total_params = sum(len(param_group[\"params\"]) for param_group in optimizer.param_groups)\n","    assert total_params == len(\n","        list(model.parameters())\n","    ), \"Your optimizer should have the same number of parameters as the model.\"\n","\n","    print(\"All tests for `get_optimizer` passed!\")\n"]},{"cell_type":"markdown","metadata":{"id":"0XCD5_nJqvYn"},"source":["### Scheduler\n","\n","In PPO, we had you write a custom class for implementing learning rate scheduling. This was useful to help you engage with the nuts and bolts of changing learning rates in Pytorch. However, PyTorch does provide a handy class for implementing custom learning rate scheduling. The syntax looks like this:\n","\n","```python\n","optimizer = t.optim.Adam(...)\n","scheduler = t.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n","```\n","\n","where `lr_lambda` is a function which takes in an integer (the total number of times `scheduler.step()` has been called) and returns a float (which **gets multiplied by the base learning rate** to create the new learning rate). Note that there are also schedulers other than `LambdaLR` which have specific behaviour and so you wouldn't need to write custom functions for them - you can read about these midway down [this documentation page](https://pytorch.org/docs/stable/optim.html).\n","\n","<details>\n","<summary>Aside - why we use warmup</summary>\n","\n","Warmup is a common strategy early in training, to make sure we don't get excessive updates early on. It seems to work pretty well empirically. Some possible reasons for this are:\n","\n","* It helps avoid large updates when the Adam moving averages of first and second moments are not yet well calibrated.\n","* Early on in training, the gradients might be very large (especially for the value function) because the model's prediction is nowhere near where it needs to be. So an LR warmup is more useful early on, to help avoid massive steps.\n","\n","</details>\n","\n","We've given you the code you'll be using for returning a custom `lr_lambda` function with a **linear warmup then linear decay**. We've also provided code for you in the trainer class's init method below which creates your scheduler. All you need to do is make sure you're stepping it appropriately.\n","\n","Note - yesterday we stepped our scheduler after every optimizer step. But it's more common practice in PyTorch to step the scheduler once every epoch. In this case, we'll be stepping it **at the end of each training phase**."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"OQnzPFKzqwCW"},"outputs":[],"source":["def get_lr_scheduler(warmup_steps, total_steps, final_scale):\n","    \"\"\"\n","    Creates an LR scheduler that linearly warms up for `warmup_steps` steps,\n","    and then linearly decays to `final_scale` over the remaining steps.\n","    \"\"\"\n","\n","    def lr_lambda(step):\n","        assert step <= total_steps, f\"Step = {step} should be less than total_steps = {total_steps}.\"\n","        if step < warmup_steps:\n","            return step / warmup_steps\n","        else:\n","            return 1 - (1 - final_scale) * (step - warmup_steps) / (total_steps - warmup_steps)\n","\n","    return lr_lambda\n","\n","\n","def get_optimizer_and_scheduler(args: RLHFTrainingArgs, model: TransformerWithValueHead):\n","    optimizer = get_optimizer(args, model)\n","    lr_lambda = get_lr_scheduler(args.warmup_steps, args.total_phases, args.final_scale)\n","    scheduler = t.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n","    return optimizer, scheduler\n"]},{"cell_type":"markdown","metadata":{"id":"pF-HsOkzqxYb"},"source":["If we want to log the learning rate, then we can use `scheduler.get_last_lr()[0]`."]},{"cell_type":"markdown","metadata":{"id":"C2U6gqaZqx7E"},"source":["## Training your model\n","\n","We're now ready to put everything together! We've provided you with the skeleton of a training loop which should be very similar to yesterday's.\n"]},{"cell_type":"markdown","metadata":{"id":"-pE4TNY42OOv"},"source":["### Exercise - complete `RLHFTrainer`\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴🔴\n","Importance: 🔵🔵🔵🔵🔵\n","\n","You should spend up to 40-60 minutes on this exercise.\n","```\n","\n","The functions you need to fill in have docstrings which should help. A few bits of guidance:\n","\n","- Make sure the shapes of objects you are passing into functions is correct - especially when it comes to slicing objects of shape `(batch_size, seq_len)` across the `seq_len` dimension to remove the prefix prompt (or not).\n","- For faster feedback loops, don't use `wandb` until you've stopped getting errors!\n","- For an easy test of whether your model is working, use the hyperparameter `kl_coef=0.0` and print your generated sequences; you should quickly see the model collapse into saying `\"This is......\"`.\n","\n","#### Logging to wandb - recap\n","\n","If you want to log text to Weights & Biases, there are 2 main ways:\n","\n","1. Just print output, this is logged to weights & biases under the \"Logs\" section!\n","2. Log tables. This should usually be done just once at the end of training (because you can't log tables incrementally, only all at once). Here's some example code I used here for logging all my samples in a single table, as well as my hyperparameters (useful when creating a run report):\n","\n","```python\n","wandb.log({\n","    \"samples_table\": wandb.Table([\"sample\"], self.samples),\n","    \"config_params\": wandb.Table([\"param\", \"values\"], [[k, v.__name__ if callable(v) else str(v)] for k, v in self.args.__dict__.items()])\n","})\n","```\n","\n","This works when `self.samples` is a list of length-1 lists, each containing a single sample (i.e. one of the strings returned frmo the `get_samples` method)."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"UAyxcEItqzS3"},"outputs":[],"source":["class RLHFTrainer:\n","    model: TransformerWithValueHead\n","    ref_model: HookedTransformer\n","    memory: ReplayMemory  # we'll set this during rollout\n","\n","    def __init__(self, args: RLHFTrainingArgs):\n","        t.manual_seed(args.seed)\n","        self.args = args\n","        self.run_name = f\"{args.wandb_project_name}__{args.seed}__{int(time.time())}\"\n","        self.base_model = HookedTransformer.from_pretrained(args.base_model)\n","        self.model = TransformerWithValueHead(self.base_model).to(device).train()\n","        self.ref_model = HookedTransformer.from_pretrained(args.base_model).to(device).eval()\n","        print(\"Loaded base & reference models\")\n","        self.optimizer, self.scheduler = get_optimizer_and_scheduler(self.args, self.model)\n","        self.prefix_tokens = self.model.base_model.to_tokens(self.args.prefix, prepend_bos=self.args.prepend_bos)\n","        self.prefix_len = len(self.prefix_tokens[0])\n","\n","    def compute_rlhf_objective(self, mb: ReplayMinibatch):\n","        \"\"\"\n","        Computes the RLHF objective function to maximize, which equals the PPO objective function minus\n","        the KL penalty term.\n","\n","        Steps of this function are:\n","            - Get logits & values for the samples in minibatch\n","            - Get the logprobs of the minibatch actions taken\n","            - Use this data to compute all 4 terms of the RLHF objective function, and create function\n","        \"\"\"\n","\n","        # Get logits & values for our generated minibatch samples, and index values cause we never need all of them\n","        logits, values = self.model(mb.sample_ids)\n","        values = values[:, self.prefix_len - 1 : -1]\n","\n","        # Get logprobs for the the tokens generated (i.e. the logprobs of our actions)\n","        logprobs = get_logprobs(logits, mb.sample_ids, self.prefix_len)\n","\n","        # Compute all terms of the loss function (including KL penalty)\n","        clipped_surrogate_objective = calc_clipped_surrogate_objective(\n","            logprobs, mb.logprobs, mb.advantages, self.args.clip_coef\n","        )\n","        value_loss = calc_value_function_loss(values, mb.returns, self.args.vf_coef)\n","        entropy_bonus = calc_entropy_bonus(logits, self.args.ent_coef, self.prefix_len)\n","        kl_penalty = calc_kl_penalty(logits, mb.ref_logits, self.args.kl_coef, self.prefix_len)\n","\n","        # Compute net objective function\n","        ppo_objective_fn = clipped_surrogate_objective - value_loss + entropy_bonus\n","        total_objective_function = ppo_objective_fn - kl_penalty\n","\n","        # Log stuff\n","        with t.inference_mode():\n","            logratio = logprobs - mb.logprobs\n","            ratio = logratio.exp()\n","            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n","        if self.args.use_wandb:\n","            wandb.log(\n","                dict(\n","                    total_steps=self.step,\n","                    lr=self.scheduler.get_last_lr()[0],\n","                    clipped_surrogate_objective=clipped_surrogate_objective.item(),\n","                    clipfrac=np.mean(clipfracs),\n","                    value_loss=value_loss.item(),\n","                    values=values.mean().item(),\n","                    entropy_bonus=entropy_bonus.item(),\n","                    kl_penalty=kl_penalty.item(),\n","                ),\n","                step=self.step,\n","            )\n","\n","        return total_objective_function\n","\n","    def rollout_phase(self) -> ReplayMemory:\n","        \"\"\"\n","        Performs a single rollout phase, retyrning a ReplayMemory object containing the data generated\n","        during this phase. Note that all forward passes here should be done in inference mode.\n","\n","        Steps of this function are:\n","            - Generate samples from our model\n","            - Get logits of those generated samples (from model & reference model)\n","            - Get other data for memory (logprobs, normalized rewards, advantages)\n","            - Return this data in a ReplayMemory object\n","\n","        New steps of this function are:\n","            - Create a KV cache & initialize it by passing through the prefix tokens\n","            - Take `gen_len` steps, each one giving you new sampled tokens, logprobs & values\n","        \"\"\"\n","        # Create tensors for the data we'll be storing\n","        # TODO - maybe add these one by one instead? idk\n","        # TODO - correct shapes here? idk\n","        sample_ids = t.zeros((self.args.batch_size, self.prefix_len + self.args.gen_len), device=device, dtype=t.long)\n","        logprobs = t.zeros((self.args.batch_size, self.args.gen_len), device=device, dtype=t.float)\n","        values = t.zeros((self.args.batch_size, self.prefix_len + self.args.gen_len), device=device, dtype=t.float)\n","        sample_ids[:, : self.prefix_len] = self.prefix_tokens\n","\n","        # Define & initialize KV cache using the prefix tokens\n","        kv_cache = HookedTransformerKeyValueCache.init_cache(self.model.cfg, device, self.args.batch_size)\n","        prefix_tokens = self.prefix_tokens.repeat(self.args.batch_size, 1)\n","        with t.inference_mode():\n","            _, values[:, : self.prefix_len] = self.model.forward(prefix_tokens, past_kv_cache=kv_cache)\n","\n","        # Take `gen_len` generation steps, filling in our data tensors\n","        from tqdm import tqdm\n","\n","        t.cuda.empty_cache()\n","        for i in tqdm(range(self.args.gen_len), desc=\"Generating samples\"):\n","            sample_ids[:, i + self.prefix_len], logprobs[:, i], values[:, i + self.prefix_len] = transformer_step(\n","                self.model, sample_ids[:, i + self.prefix_len - 1], kv_cache, self.args.k, self.args.temperature\n","            )\n","\n","        # Get logits from the reference model\n","        with t.inference_mode():\n","            ref_logits = self.ref_model(sample_ids)\n","\n","        # Get rewards (normalized), and advantages\n","        samples = self.model.base_model.to_string(sample_ids)\n","        rewards = self.args.reward_fn(samples)\n","        rewards_normed = normalize_reward(rewards) if self.args.normalize_reward else rewards\n","        advantages = compute_advantages(values, rewards_normed, self.prefix_len)\n","\n","        # Log stuff, and print output in a readable way (you could easily just regular print here instead of rprint table)\n","        if self.args.use_wandb:\n","            wandb.log({\"mean_reward\": rewards.mean().item()}, step=self.step)\n","\n","        ref_logprobs = get_logprobs(ref_logits[:3], sample_ids[:3], self.prefix_len).sum(-1)\n","        table = Table(\n","            \"Reward\",\n","            \"Ref logprobs\",\n","            \"Sample\",\n","            title=f\"Phase {self.phase:03}/{self.args.total_phases}, Mean reward: {rewards.mean().item():.4f}\",\n","            show_lines=True,\n","        )\n","        for r, lp, s in zip(rewards.tolist(), ref_logprobs, samples):\n","            table.add_row(str(int(r)), f\"{lp:.2f}\", repr(s))\n","        rprint(table)\n","        print(\"\")\n","\n","        return ReplayMemory(\n","            prefix_len=self.prefix_len,\n","            batches_per_learning_phase=self.args.batches_per_learning_phase,\n","            batch_size=self.args.batch_size,\n","            num_minibatches=self.args.num_minibatches,\n","            sample_ids=sample_ids,\n","            logprobs=logprobs,\n","            advantages=advantages,\n","            values=values,\n","            rewards=rewards,\n","            ref_logits=ref_logits,\n","        )\n","\n","    def learning_phase(self, memory: ReplayMemory) -> None:\n","        \"\"\"\n","        Performs a learning step on `self.memory`. This involves the standard gradient descent steps\n","        (i.e. zeroing gradient, computing objective function, doing backprop, stepping optimizer).\n","\n","        You should also remember the following:\n","            - Clipping grad norm to the value given in `self.args.max_grad_norm`\n","            - Incrementing `self.step` by 1 for each minibatch\n","            - Stepping the scheduler (once per calling of this function)\n","        \"\"\"\n","        for minibatch in memory.get_minibatches():\n","            self.optimizer.zero_grad()\n","            total_objective_function = self.compute_rlhf_objective(minibatch)\n","            total_objective_function.backward()\n","            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.args.max_grad_norm)\n","            self.optimizer.step()\n","            self.step += 1\n","\n","        self.scheduler.step()\n","\n","    def train(self) -> None:\n","        \"\"\"\n","        Performs a full training run.\n","        \"\"\"\n","        self.step = 0\n","        self.samples = []\n","\n","        if self.args.use_wandb:\n","            wandb.init(\n","                project=self.args.wandb_project_name,\n","                entity=self.args.wandb_entity,\n","                name=self.run_name,\n","                config=self.args,\n","            )\n","\n","        for self.phase in range(self.args.total_phases):\n","            memory = self.rollout_phase()\n","            self.learning_phase(memory)\n","\n","        if self.args.use_wandb:\n","            wandb.log(\n","                {\n","                    \"samples_table\": wandb.Table([\"sample\"], self.samples),\n","                    \"config_params\": wandb.Table(\n","                        [\"param\", \"values\"],\n","                        [[k, v.__name__ if callable(v) else str(v)] for k, v in self.args.__dict__.items()],\n","                    ),\n","                }\n","            )\n","            wandb.finish()\n"]},{"cell_type":"markdown","metadata":{"id":"QrlL7vtFq1Fq"},"source":["<details>\n","<summary>Solution (simple, no logging)</summary>\n","\n","\n","\n","</details>\n","\n","<details>\n","<summary>Solution (full logging)</summary>\n","\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"DjEyaY_gq101"},"source":["Once you've implemented your trainer class, you can run the code below to train your model. You can also play around with the parameters - in particular, try a few different prefix strings. The behaviour of the model (e.g. which kinds of techniques it converges onto for period maximization) or whether it easily mode collapses into insanity can be highly dependent on the prefix string!\n","\n","Some common strategies you should observe include:\n","\n","- Shorter sentences\n","- Repeating `U.S.` or `U.S.A.` (using the prefix prompt `\"There is\"`, this seems to be by far the most common strategy)\n","- Library versions e.g. `Python 2.7.12` or `the 2.6.0.2 release`\n","- Names with initials e.g. `C. S. Lewis` or titles e.g. `Dr.` and `PhD.`\n","- Abbreviations e.g. `Data-R.A.R. series` or `\"L.A. Times\"`\n","- Decimals in numbers e.g. `9.5cm x 7.5 cm`\n","- Triple periods e.g. `the man . . . the woman . . .`\n","\n","You might also observe increasingly incoherent mode collapse if you train for too long. Here are a few that I got:\n","\n","- `This is really helpful. The U.S. U.S. U.S. U.S.`\n","- `This is the A.A.G.A.R.M.A.R.M.A.R.M.A.R.M`\n","- `This is my mother. . . me. . . . . . . . . . . . . . . . . . . . . . . .`\n","\n","[Here's](https://api.wandb.ai/links/callum-mcdougall/es12ygmm) an example run, which was generated using the default parameters (i.e. the code below).\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4f65a423ee9e430197326392412308af","29d08bb00e884c2b818c784047464a64","39d58ee5bf3d4dc0be0d161ac665ecca","183156c8bac94ea9821d7d6854c9c84b","98408ad43e6a4b66908c29d1b6d117b3","d9c5af13e7c840beb53b0dcad0b153c6","70adcf9538354b93977fd5768b9e166a","4e93b5f639c04058ad0a5a89dc7e14b4"]},"executionInfo":{"elapsed":1174655,"status":"ok","timestamp":1706294401317,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"rBxjP7XdtLNE","outputId":"6135f52d-6bb5-477e-f0d3-324d5e0a577b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained model gpt2-small into HookedTransformer\n","Loaded pretrained model gpt2-small into HookedTransformer\n","Moving model to device:  cuda\n","Loaded base & reference models\n"]},{"name":"stderr","output_type":"stream","text":["Generating samples: 100%|██████████| 30/30 [00:57<00:00,  1.90s/it]\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                        Phase 000/200, Mean reward: 1.4414                                         </span>\n","┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Reward </span>┃<span style=\"font-weight: bold\"> Ref logprobs </span>┃<span style=\"font-weight: bold\"> Sample                                                                                  </span>┃\n","┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ 2      │ -27.03       │ '&lt;|endoftext|&gt;This is a rush transcript. Copy may not be in its final form.\\n\\nAMY      │\n","│        │              │ GOODMAN: We turn now to a new issue, The Intercept, which'                              │\n","├────────┼──────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n","│ 2      │ -11.04       │ \"&lt;|endoftext|&gt;This is a rush transcript. Copy may not be in its final form.\\n\\nJUAN     │\n","│        │              │ GONZÁLEZ: We're going to\"                                                               │\n","├────────┼──────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n","│ 0      │ -70.50       │ '&lt;|endoftext|&gt;This is a list of all the items in our store that are part of the TES5:   │\n","│        │              │ The Last Frontier, TES4: The Last Frontier,'                                            │\n","└────────┴──────────────┴─────────────────────────────────────────────────────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m                                        Phase 000/200, Mean reward: 1.4414                                         \u001b[0m\n","┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mReward\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRef logprobs\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSample                                                                                 \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ 2      │ -27.03       │ '<|endoftext|>This is a rush transcript. Copy may not be in its final form.\\n\\nAMY      │\n","│        │              │ GOODMAN: We turn now to a new issue, The Intercept, which'                              │\n","├────────┼──────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n","│ 2      │ -11.04       │ \"<|endoftext|>This is a rush transcript. Copy may not be in its final form.\\n\\nJUAN     │\n","│        │              │ GONZÁLEZ: We're going to\"                                                               │\n","├────────┼──────────────┼─────────────────────────────────────────────────────────────────────────────────────────┤\n","│ 0      │ -70.50       │ '<|endoftext|>This is a list of all the items in our store that are part of the TES5:   │\n","│        │              │ The Last Frontier, TES4: The Last Frontier,'                                            │\n","└────────┴──────────────┴─────────────────────────────────────────────────────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"ename":"RuntimeError","evalue":"CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m RLHFTrainingArgs(use_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reward_fn\u001b[38;5;241m=\u001b[39mreward_fn_char_count)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m RLHFTrainer(args)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[28], line 185\u001b[0m, in \u001b[0;36mRLHFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtotal_phases):\n\u001b[0;32m    184\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_phase()\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_wandb:\n\u001b[0;32m    188\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m    189\u001b[0m         {\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples_table\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb\u001b[38;5;241m.\u001b[39mTable([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m         }\n\u001b[0;32m    196\u001b[0m     )\n","Cell \u001b[1;32mIn[28], line 160\u001b[0m, in \u001b[0;36mRLHFTrainer.learning_phase\u001b[1;34m(self, memory)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m memory\u001b[38;5;241m.\u001b[39mget_minibatches():\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 160\u001b[0m     total_objective_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_rlhf_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     total_objective_function\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    162\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n","Cell \u001b[1;32mIn[28], line 34\u001b[0m, in \u001b[0;36mRLHFTrainer.compute_rlhf_objective\u001b[1;34m(self, mb)\u001b[0m\n\u001b[0;32m     31\u001b[0m values \u001b[38;5;241m=\u001b[39m values[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get logprobs for the the tokens generated (i.e. the logprobs of our actions)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mget_logprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Compute all terms of the loss function (including KL penalty)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m clipped_surrogate_objective \u001b[38;5;241m=\u001b[39m calc_clipped_surrogate_objective(\n\u001b[0;32m     38\u001b[0m     logprobs, mb\u001b[38;5;241m.\u001b[39mlogprobs, mb\u001b[38;5;241m.\u001b[39madvantages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mclip_coef\n\u001b[0;32m     39\u001b[0m )\n","Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mget_logprobs\u001b[1;34m(logits, tokens, prefix_len)\u001b[0m\n\u001b[0;32m     18\u001b[0m correct_tokens \u001b[38;5;241m=\u001b[39m tokens[:, prefix_len:]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# correct_logprobs[batch, seq] = logprobs[batch, seq, correct_tokens[batch, seq]]\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m correct_logprobs \u001b[38;5;241m=\u001b[39m \u001b[43meindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch seq [batch seq] -> batch seq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m correct_logprobs\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m prefix_len)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correct_logprobs\n","File \u001b[1;32mc:\\Users\\calsm\\anaconda3\\envs\\arena\\Lib\\site-packages\\eindex\\indexing.py:296\u001b[0m, in \u001b[0;36meindex\u001b[1;34m(*tensors_and_pattern, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m         full_idx\u001b[38;5;241m.\u001b[39mappend(full_idx_item )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Index using the full array\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m arr_indexed \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfull_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# If there was an einops operation, apply it\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m einops_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["# TAGS: main\n","\n","args = RLHFTrainingArgs(use_wandb=False, reward_fn=reward_fn_char_count)\n","trainer = RLHFTrainer(args)\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"ftieF42RxDyb"},"source":["Example params for gpt2-large (I think this would work a lot better with layer freezing but haven't tried it, see bonus exercises!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlfksHlXq32c"},"outputs":[],"source":["# args = RLHFTrainingArgs(use_wandb=True, base_model=\"gpt2-large\", num_minibatches=16, kl_coef=0.5)\n","# trainer = RLHFTrainer(args)\n","# trainer.train()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M4d1GreLq7Aq"},"source":["### Exercise - use a more complex reward function\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 30-50 minutes on this exercise.\n","```\n","\n","> **Note: You will need a lot more VRAM to proceed with many following exercises. With `LOW_GPU_MEM = True` it's just barely possible to do this with 24GB VRAM, but in general we would recommend at least 40GB for some breathing room. Don't worry if you can't run them, these exercises are mostly for playing around with the reward model. You've already conceptually gained pretty much everything about RLHF if you've completed the above. We just now replace our toy reward model with something more complex.**\n","\n","We recommend you experiment with a few different reward functions, in particular some sentiment-based reward functions which are based on pretrained text classification models. For example, you might want to start with one of the following:\n","\n","- [`lvwerra/distilbert-imdb`](https://huggingface.co/lvwerra/distilbert-imdb), which was trained to classify IMDB film reviews as positive or negative.\n","- [`cardiffnlp/twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment), which is a model trained on tweets and finetuned for sentiment analysis (categories are positive, neutral and negative).\n","- [`distilbert-base-uncased-emotion`](bhadresh-savani/distilbert-base-uncased-emotion), which was finetuned on the [Emotion Dataset for Emotion Recognition Tasks](https://www.kaggle.com/datasets/parulpandey/emotion-dataset), i.e. it's trained to classify text according to emotional tone (classes are sadness, joy, love, anger, fear and surprise).\n","\n","Note that for some of these, you should be using a prompt string which is appropriate for the reward function you're fine-tuning on, e.g. `\"This movie was really\"` for the IMDB model. Similarly, you might also want to change other parameters e.g. generation length. You can find a list of other models [here](https://huggingface.co/models?filter=text-classification).\n","\n","The solutions contain the following:\n","\n","- A reward function derived from the `lvwerra/distilbert-imdb` model (with demo of it working),\n","- A wandb run of GPT2-medium using this reward function (both in positive and negative sentiment directions).\n","\n","We recommend you try to do something similar before you compare your answers to the solutions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtoQ47zOq94j"},"outputs":[],"source":["import transformers\n","\n","cls_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\").half().to(device)\n","cls_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n","\n","\n","@t.no_grad()\n","def reward_fn_sentiment_imdb(gen_sample: str | list[str], direction=\"pos\") -> float | Float[Tensor, \"batch\"]:\n","    tokens = cls_tokenizer(gen_sample, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"].to(device)\n","    logits = cls_model(tokens).logits\n","    positive_cls = logits.softmax(dim=-1)[:, 1 if (direction == \"pos\") else 0]\n","    return positive_cls.to(device)\n","\n","\n","def get_reward_fn_sentiment_imdb(direction: str = \"pos\"):\n","    assert direction in [\"pos\", \"neg\"], \"direction should be either 'pos' or 'neg'\"\n","    return partial(reward_fn_sentiment_imdb, direction=direction)\n","\n","\n","# Some samples taken from the IMDB dataset used to finetune this model\n","classes, samples = map(\n","    list,\n","    zip(\n","        *[\n","            (\n","                \"pos\",\n","                \"Just finished watching this movie for maybe the 7th or 8th time, picked it up one night previously viewed at Blockbuster and absolutely loved it, I've shown it to 4 people so far and they have enjoyed it as well.\",\n","            ),\n","            (\n","                \"pos\",\n","                \"This was the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days.\",\n","            ),\n","            (\"neg\", \"I can't believe that those praising this movie herein aren't thinking of some other film.\"),\n","            (\"neg\", \"This film seemed way too long even at only 75 minutes.\"),\n","            (\n","                \"neg\",\n","                \"Really, I can't believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the movie might be really good. It had zombies in it right? Was I wrong!\",\n","            ),\n","        ]\n","    ),\n",")\n","sentiment = reward_fn_sentiment_imdb(samples).tolist()\n","\n","table = Table(\"Sample\", \"Classification\", \"Sentiment\", title=\"Demo of `reward_fn_sentiment_imdb`\", show_lines=True)\n","for sample, cls, sent in zip(samples, classes, sentiment):\n","    table.add_row(repr(sample), cls, f\"{sent:.4f}\")\n","rprint(table)\n"]},{"cell_type":"markdown","metadata":{"id":"Nh9XrYCRnU0O"},"source":["For reference, you can see the parameters & results of a positive-sentiment IMDB run [here](https://api.wandb.ai/links/callum-mcdougall/3a1bl3y4), and a negative-sentiment run [here](https://api.wandb.ai/links/callum-mcdougall/misa79ct). The code to generate these two outputs respectively can be found below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5N1muaTuq-w3"},"outputs":[],"source":["assert not LOW_GPU_MEM, \"You will need more memory to use the imdb reward model.\"\n","\n","args = RLHFTrainingArgs(\n","    reward_fn=get_reward_fn_sentiment_imdb(\"pos\"),\n","    prefix=\"I thought the Céline Sciamma film 'Portrait of a Lady on Fire' was\",\n","    total_phases=150,\n","    use_wandb=True,\n","    gen_len=50,\n",")\n","trainer = RLHFTrainer(args)\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSbE9neGneXH"},"outputs":[],"source":["args = RLHFTrainingArgs(\n","    reward_fn=get_reward_fn_sentiment_imdb(\"pos\"),\n","    prefix=\"I thought the Céline Sciamma film 'Portrait of a Lady on Fire' was\",\n","    total_phases=150,\n","    use_wandb=True,\n","    gen_len=50,\n",")\n","trainer = RLHFTrainer(args)\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"dT_3pgVSrhCo"},"source":["# 2️⃣ Bonus"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GAEm"]},{"cell_type":"markdown","metadata":{"id":"XVXjpARTrkk1"},"source":["## Extensions of today's RLHF exercises"]},{"cell_type":"markdown","metadata":{},"source":["GAE!"]},{"cell_type":"markdown","metadata":{"id":"GKLefEIurhs4"},"source":["### Large models\n","\n","We're already working with `gpt2-medium` which is considerably larger than most of the models you worked with in most of the transformers & interpretability material. Can you go even larger?\n","\n","See [this page](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html) for a table of model properties, for all models currently supported by TransformerLens. Note that if you use different model classes then you might need to change some parts of your code (e.g. if the name of the hook point where you added the value head happens to be different). You might also need to make other adjustments e.g. a smaller batch size (or a larger number of minibatches per batch, which is equivalent to smaller minibatch sizes).\n","\n","<br>\n","\n","### Differential Learning Rates / Frozen Layers\n","\n","When doing any kind of finetuning, it's common practice to either freeze earlier layers or have a smaller learning rate for them. You may have seen this in the feature extraction with ResNet34 exercises in the first week. In the exercises here we've trained all layers of the model equally, but you might want to play around with differential learning rates.\n","\n","Note that you can accomplish this using parameter groups - we already used parameter groups above to have a different learning rate for our base model and value head. It should be relatively straightforward to extend this to splitting parameters over different layers into different groups (hint - you can use `itertools.chain` to convert several iterables into a single iterable).\n","\n","You can also try entirely freezing earlier layers - this might also reduce your memory usage, and allow you to train larger models without getting cuda errors.\n","\n","<br>\n","\n","### Hyperparameter sweeps\n","\n","You can do this to find the best possible hyperparamters for your RLHF training. Don't just measure on reward, can you use some combination of reward and avg kl diff to create a better metric? Can you use wandb's built-in [Bayesian search methods](https://docs.wandb.ai/guides/sweeps/sweep-config-keys#bayesian-search) to more effectively sweep?\n","\n","Note - don't forget **temperature** when it comes to hyperparameter tuning. Temperature has an important effect on how the model learns, e.g. if the temperature is too high then the model will produce very high-variance outputs which will have very high KL with the reference distribution, and it'll be more likely to collapse into some incoherent mode.\n","\n","<br>\n","\n","### Adaptive KL penalty\n","\n","The KL divergence penalty coefficient can be modified adaptively based on the KL divergence between the current policy and the previous policy. If the KL divergence is outside a predefined target range, we can adjust the penalty coefficient to bring it closer to the target range. Here is an example implementation:\n","\n","```python\n","class AdaptiveKLController:\n","    def __init__(self, init_kl_coef, hparams):\n","        self.value = init_kl_coef\n","        self.hparams = hparams\n","\n","    def update(self, current, n_steps):\n","        target = self.hparams.target\n","        proportional_error = np.clip(current / target - 1, -0.2, 0.2)\n","        mult = 1 + proportional_error * n_steps / self.hparams.horizon\n","        self.value *= mult\n","```\n","\n","<br>\n","\n","### TRL / trlX\n","\n","We've been focusing on building RLHF from the ground up, but there are several libraries which exist to abstract away manuy of the low-level implementation details we had to wrestle with. One of the best-known is TRL (Transformer Reinforcement Learning). The main docs page can be found [here](https://huggingface.co/docs/trl/index), and [this page](https://huggingface.co/docs/trl/quickstart) gives a quickstart guide. You may find it much easier to use this library than to implement everything yourself!\n","\n","Read their documentation pages, and see what techniques they use to make RLHF more effective. Are there any that we haven't implemented here? Can you implement them yourself?\n","\n","You might also be interested in trlX, an expanded fork of TRL built by CarperAI to handle larger models for online and offline training (although their APIs are pretty similar).\n","\n","<br>\n","\n","### Learn a human preference reward model\n","\n","We've been working with a pre-supplied reward function, but you can try and train your own!\n","\n","We'll give some brief points of guidance here, for the task of training a reward function on the **summarization task**. Note that these instructions have been provided externally, so they've not yet been tested and might not work particularly well.\n","\n","1. Get a supervised baseline\n","    * [Here](https://zenodo.org/records/1168855) is a link to download the dataset for the TL;DR challenge containing posts from the Reddit corpus. Each post contains keys `content` and `summary` which are the original post and the human-written summary respectively.\n","    * You should throw out all summaries shorter than 24 tokens or longer than 48 tokens (to diminish the effects of length on quality); and choose a random subset of ~100k summaries to train on.\n","    * Run training to maximize the log-likelihood of these summaries.\n","2. Get reward model by training supervised baseline on human feedback\n","    * Download comparison data with the code `azcopy copy \"https://openaipublic.blob.core.windows.net/summarize-from-feedback/dataset/*\" . --recursive`\n","    * Modify GPT-2 architecture by adding a randomly-initialized **reward head** at the end of your model.\n","        * Architecturally this is similar to the value head from earlier, but it's not the same thing - here we're trying to learn what the human reward will be; we're not doing RL yet.\n","    * Train your model (starting with base model given by supervised baseline weights, and reward head randomly initialized) to minimize `loss = log(sigmoid(reward_model(summary_0) - reward_model(summary_1)))`, `summary_0` is preferred by a human labeler (this data should be in the comparison data you downloaded).\n","    * You should normalize reward model outputs, like we normalized rewards in RLHF in previous exercises.\n","3. Fine-tune supervised baseline using PPO with reward model.\n","    * For these exercises we suggest using a larger model, ideally GPT2-Large or bigger. Remember you can freeze weights! Regardless, this will still take longer to train than your previous models.\n","\n","<br>\n","\n","### Interp on RLHF'd models\n","\n","Currently, very little mechanistic interpretability research ahs focused on RLHF'd models. In [this blog post](https://blog.eleuther.ai/trlx-exploratory-analysis/), Curt Tigges walks through an example of how we can use mech interp to analyze a model which has been finetuned with a sentiment based reward function using trlX.\n","\n","The flavour of the actual mech interp done here is very similar to the indirect object identification exercises you might have done during the transformers & interp week. If you didn't do these exercises, we recommend you do them before diving deep into this material.\n","\n","Lastly, here's a [Google Doc](https://docs.google.com/document/d/1eUdvlJNqY9X0NAw9UUseZz6dFyRklCcOHQy8x3CbcBk/edit?usp=sharing) brainstorming some ideas for RLHF interpretability. You might find some ideas there (although most of these will be pretty vague goals so possibly too ambitious for a bonus exercise or 1-week project)."]},{"cell_type":"markdown","metadata":{"id":"Dt5CNXEOBkdd"},"source":["## Suggested paper replications\n","\n","As well as the three papers in this section, you might be interested in browsing this [GitHub repo](https://github.com/opendilab/awesome-RLHF), which contains links to a large number of RLHF-related papers."]},{"cell_type":"markdown","metadata":{"id":"18UFz3IDBj7C"},"source":["### [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)\n","\n","This was the seminal paper in RLHF. They applied it to the domain of tasks like MuJoCo (which you might already have worked with during your PPO day). Can you set up a reward function and an interface which allows you to choose between two different sets of trajectories, and learn a reward function to maximize?\n","\n","Some more technical details here - the authors train the reward function at the same time as they train the model. In other words, after a certain number of iterations of (rollout phase, learning phase), they add a third reward model learning phase, where the current policy generates many pairs of trajectories of some fixed timestep and the human rater chooses which one is best. They famously trained the Hopper agent to perform repeated backflips using just 900 queries.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/hopper-backflip.png\" width=\"700\">\n","\n","[Here](https://drive.google.com/drive/folders/0BwcFziBYuA8RM2NTdllSNVNTWTg?resourcekey=0-w4PuSuFvi3odgQXdBDPQ0g) is the link mentioned in the image caption.\n","\n","Note - we strongly recommend doing the PPO exercises on MuJoCo before attempting this replication. We also recommend using Colab, since MuJoCo is notoriously difficult to install all the dependencies for!\n","\n","<br>\n","\n","### [Measuring Faithfulness in Chain-of-Thought Reasoning](https://arxiv.org/abs/2307.13702)\n","\n","This paper investigates the **chain-of-thought prompting method**, by examining how model predictions change when this chain is intervened on. They find that models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it.\n","\n","<br>\n","\n","### [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)\n","\n","A major challenge for scaling ML is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. To test scalable alignment techniques, the authors trained a model to summarize entire books, by first summarizing small sections of a book, then summarizing those summaries into a higher-level summary, and so on. A demonstration can be found [here](https://openai.com/research/summarizing-books). There is also a [repository](https://github.com/openai/summarize-from-feedback) containing code to run their models, including the supervised baseline, the trained reward model, and the RL fine tuned policy.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/alice.png\" width=\"500\">\n","\n","You may also wish to do this in a less directed way - see the bonus exercise “Learn a human preference reward model” above."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QinA9Nq7o3Gl","eoTwssxTo6-M","Zz_P1RIGp7oD","dT_3pgVSrhCo"],"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1f_RBuosHddwQrydZ7-iAnBs6lhkMMMSG","timestamp":1704215013474}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"183156c8bac94ea9821d7d6854c9c84b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29d08bb00e884c2b818c784047464a64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98408ad43e6a4b66908c29d1b6d117b3","placeholder":"​","style":"IPY_MODEL_d9c5af13e7c840beb53b0dcad0b153c6","value":"0.067 MB of 0.067 MB uploaded\r"}},"39d58ee5bf3d4dc0be0d161ac665ecca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_70adcf9538354b93977fd5768b9e166a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e93b5f639c04058ad0a5a89dc7e14b4","value":1}},"4e93b5f639c04058ad0a5a89dc7e14b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f65a423ee9e430197326392412308af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_29d08bb00e884c2b818c784047464a64","IPY_MODEL_39d58ee5bf3d4dc0be0d161ac665ecca"],"layout":"IPY_MODEL_183156c8bac94ea9821d7d6854c9c84b"}},"70adcf9538354b93977fd5768b9e166a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98408ad43e6a4b66908c29d1b6d117b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9c5af13e7c840beb53b0dcad0b153c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
