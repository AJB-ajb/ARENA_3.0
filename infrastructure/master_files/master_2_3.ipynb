{"cells":[{"cell_type":"markdown","metadata":{},"source":["```python\n","[\n","    {\"title\": \"Setting up our agent\", \"icon\": \"1-circle-fill\", \"subtitle\" : \"(35%)\"},\n","    {\"title\": \"Learning Phase\", \"icon\": \"2-circle-fill\", \"subtitle\" : \"(30%)\"},\n","    {\"title\": \"Training Loop\", \"icon\": \"3-circle-fill\", \"subtitle\" : \"(20%)\"},\n","    {\"title\": \"Atari\", \"icon\": \"4-circle-fill\", \"subtitle\" : \"(5%)\"},\n","    {\"title\": \"MuJoCo\", \"icon\": \"5-circle-fill\", \"subtitle\" : \"(10%)\"},\n","    {\"title\": \"Bonus\", \"icon\": \"star\", \"subtitle\" : \"\"}\n","]\n","```"]},{"cell_type":"markdown","metadata":{"id":"whCcGn3OQOaP"},"source":["# [2.3] - PPO\n"]},{"cell_type":"markdown","metadata":{"id":"NzpGGRLH1tc8"},"source":["### Colab: [exercises](https://colab.research.google.com/drive/1UgXZRsIDsGmv6FhqkEuBCRMfkRsBC6nb?usp=sharing) | [solutions](https://colab.research.google.com/drive/1aMzOHbw-CAy6g81Vue7SXs5Hqy8JcxJd?usp=sharing)\n","\n","[Streamlit page](https://arena-ch2-rl.streamlit.app/[2.3]_PPO)\n","\n","Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), and ask any questions on the dedicated channels for this chapter of material."]},{"cell_type":"markdown","metadata":{"id":"ST7GZ0xkxW6j"},"source":["<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/football.jpeg\" width=\"350\">\n"]},{"cell_type":"markdown","metadata":{"id":"jd3LpCav3UXu"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"0OxVf4WaJWQX"},"source":["Proximal Policy Optimization (PPO) is a cutting-edge reinforcement learning algorithm that has gained significant attention in recent years. As an improvement over traditional policy optimization methods, PPO addresses key challenges such as sample efficiency, stability, and robustness in training deep neural networks for reinforcement learning tasks. With its ability to strike a balance between exploration and exploitation, PPO has demonstrated remarkable performance across a wide range of complex environments, including robotics, game playing, and autonomous control systems.\n","\n","In this section, you'll build your own agent to perform PPO on the CartPole environment. By the end, you should be able to train your agent to near perfect performance in about 30 seconds. You'll also be able to try out other things like **reward shaping**, to make it easier for your agent to learn to balance, or to do fun tricks! There are also additional exercises which allow you to experiment with other tasks, including **Atari** and the 3D physics engine **MuJoCo**.\n","\n","A lot of the setup as we go through these exercises will be similar to what we did yesterday for DQN, so you might find yourself moving quickly through certain sections."]},{"cell_type":"markdown","metadata":{"id":"ZImx8sBRJWQY"},"source":["## Content & Learning Objectives\n","\n","\n","### 1️⃣ Setting up our agent\n","\n","> ##### Learning Objectives\n",">\n","> - Understand the difference between the actor & critic networks, and what their roles are\n","> - Learn about & implement generalised advantage estimation\n","> - Build a replay memory to store & sample experiences\n","> - Design an agent class to step through the environment & record experiences\n","\n","### 2️⃣ Learning Phase\n","\n","> ##### Learning Objectives\n",">\n","> - Implement the total objective function (sum of three separate terms)\n","> - Understand the importance of each of these terms for the overall algorithm\n","> - Write a function to return an optimizer and learning rate scheduler for your model\n","\n","### 3️⃣ Training Loop\n","\n","> ##### Learning Objectives\n",">\n","> - Build a full training loop for the PPO algorithm\n","> - Train our agent, and visualise its performance with Weights & Biases media logger\n","> - Use reward shaping to improve your agent's training (and make it do tricks!)\n","\n","### 4️⃣ Atari\n","\n","> ##### Learning Objectives\n",">\n","> - Understand how PPO can be used in visual domains, with appropriate architectures (CNNs)\n","> - Understand the idea of policy and value heads\n","> - Train an agent to solve the Breakout environment\n","\n","### 5️⃣ MuJoCo\n","\n","> ##### Learning Objectives\n",">\n","> - Understand how PPO can be used to train agents in continuous action spaces\n","> - Install and interact with the MuJoCo physics engine\n","> - Train an agent to solve the Hopper environment\n","\n","### ☆ Bonus\n","\n","We conclude with a set of optional bonus exercises, which you can try out before moving on to the RLHF sections."]},{"cell_type":"markdown","metadata":{"id":"W0RWgomzJWQZ"},"source":["## PPO vs DQN\n","\n","Today, we'll be working on PPO (Proximal Policy Optimization). It has some similarities to DQN, but is based on a fundamentally different approach:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-vs-dqn-2.png\" width=\"1000\">\n","\n","A quick note again on the distinction between **states** and **observations**. In reality these are two different things (denoted $s_t$ and $o_t$), because our agent might not be able to see all relevant information. However, the games we'll be working with for the rest of this section make no distinction between them. Thus, we will only refer to $s_t$ going forwards.\n"]},{"cell_type":"markdown","metadata":{"id":"ACGjrQSIJWQZ"},"source":["## Conceptual overview of PPO\n","\n","Below is an algorithm showing the conceptual overview of PPO. It's split into 2 main phases: **learning** and **rollout**.\n","\n","In **rollout**, we sample experiences using the current values of our actor and critic networks, and store them in memory. This is all done in inference mode. In **learning**, we use our current actor and critic networks (*not* in inference mode) plus these logged experiences to calculate an objective function and use it to update our network.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-alg-conceptual.png\" width=\"800\">\n"]},{"cell_type":"markdown","metadata":{"id":"O1i-SaV1JWQa"},"source":["## Implementational overview of PPO\n","\n","There are 3 main classes you'll be using today:\n","\n","* `ReplayMemory`\n","    * Stores experiences generated by agent\n","    * Has a `get_minibatches` method, which samples data from memory to actually be used in training\n","* `Agent`\n","    * Contains the actor and critic networks, the `play_step` function, and a replay memory\n","        * In other words, it contains both the thing doing the inference and the thing which interacts with environment & stores results\n","        * This is a design choice, other designs might keep these separate\n","    * Also has a `get_minibatches` method which calls the corresponding `ReplayMemory` method (and uses the agent's current state)\n","* `PPOTrainer`\n","    * This is the main class for training our model, it helps us keep methods like `rollout_phase` and `learning_phase` separate"]},{"cell_type":"markdown","metadata":{"id":"_-7Wz490JWQa"},"source":["## Notes on today's workflow\n","\n","Your implementation might get huge benchmark scores by the end of the day, but don't worry if it struggles to learn the simplest of tasks. RL can be frustrating because the feedback you get is extremely noisy: the agent can fail even with correct code, and succeed with buggy code. Forming a systematic process for coping with the confusion and uncertainty is the point of today, more so than producing a working PPO implementation.\n","\n","Some parts of your process could include:\n","\n","- Forming hypotheses about why it isn't working, and thinking about what tests you could write, or where you could set a breakpoint to confirm the hypothesis.\n","- Implementing some of the even more basic Gym environments and testing your agent on those.\n","- Getting a sense for the meaning of various logged metrics, and what this implies about the training process\n","- Noticing confusion and sections that don't make sense, and investigating this instead of hand-waving over it.\n"]},{"cell_type":"markdown","metadata":{"id":"Nfw_h-YlJWQa"},"source":["## Readings\n","\n","* [An introduction to Policy Gradient methods - Deep RL\n","](https://www.youtube.com/watch?v=5P7I-xPq8u8) (20 mins)\n","    * This is a useful video which motivates the core setup of PPO (and in particular the clipped objective function) without spending too much time with the precise derivations. We recommend watching this video before doing the exercises.\n","    * Note - you can ignore the short section on multi-GPU setup.\n","    * Also, near the end the video says that PPO outputs parameters $\\mu$ and $\\sigma$ from which actions are sampled, this is true for non-discrete action spaces (which we'll be using later on) but we'll start by implementing PPO on CartPole meaning our observation and action space is discrete just like yesterday.\n","* [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool)\n","    * **This is not required reading before the exercises**, but it will be a useful reference point as you go through the exercises.\n","    * The good news is that you won't need all 37 of these today, so no need to read to the end.\n","    * We will be tackling the 13 \"core\" details, not in the same order as presented here. Some of the sections below are labelled with the number they correspond to in this page (e.g. **Minibatch Update ([detail #6](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Mini%2Dbatch%20Updates))**).\n","* [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)\n","    * **This is not required reading before the exercises**, but it will be a useful reference point for many of the key equations as you go through the exercises. In particular, you will find up to page 5 useful.\n","\n","\n","### Optional Reading\n","\n","* [Spinning Up in Deep RL - Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html#background)\n","    * PPO is a fancier version of vanilla policy gradient, so if you're struggling to understand PPO it may help to look at the simpler setting first.\n","* [Spinning Up in Deep RL - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n","    * You don't need to follow all the derivations here, although as a general idea by the end you should at least have a qualitative understanding of what all the symbols represent.\n","* [Andy Jones - Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html)\n","    * You've already read this previously but it will come in handy again.\n","    * You'll want to reuse your probe environments from yesterday, or you can import them from the solution if you didn't implement them all.\n","* [Tricks from Deep RL Bootcamp at UC Berkeley](https://github.com/williamFalcon/DeepRLHacks/blob/master/README.md) - more debugging tips that may be of use.\n","* [Lilian Weng Blog on PPO](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo) - her writing on ML topics is consistently informative and informationally dense."]},{"cell_type":"markdown","metadata":{"id":"XcgAnZZOyBYk"},"source":["## Setup code"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# FILTERS: ~\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","ipython.run_line_magic(\"autoreload\", \"2\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# FILTERS: colab\n","# TAGS: master-comment\n","\n","import os\n","import sys\n","from importlib.metadata import distributions\n","from pathlib import Path\n","\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","chapter = \"chapter2_rl\"\n","repo = \"ARENA_3.0\"\n","branch = \"master_file\"\n","\n","# Install dependencies\n","if \"jaxtyping\" not in [dist.metadata[\"Name\"] for dist in distributions()]:\n","    %pip install wandb==0.18.7 einops gymnasium[atari, accept-rom-license, othe, mujoco-pyr]==0.29.0 pygame jaxtyping mujoco free-mujoco-py\n","\n","    # Mujoco-specific things\n","    !sudo apt-get install -y libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev software-properties-common\n","    !sudo apt-get install -y patchelf\n","\n","# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n","root = (\n","    \"/content\"\n","    if IN_COLAB\n","    else \"/root\"\n","    if repo not in os.getcwd()\n","    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",")\n","\n","if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n","    if not IN_COLAB:\n","        !sudo apt-get install unzip\n","        %pip install jupyter ipython --upgrade\n","\n","    if not os.path.exists(f\"{root}/{chapter}\"):\n","        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n","        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n","        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n","        !rm {root}/{branch}.zip\n","        !rmdir {root}/{repo}-{branch}\n","\n","\n","if f\"{root}/{chapter}/exercises\" not in sys.path:\n","    sys.path.append(f\"{root}/{chapter}/exercises\")\n","\n","os.chdir(f\"{root}/{chapter}/exercises\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DRQ9j4ftyHXf"},"outputs":[],"source":["import os\n","import sys\n","import time\n","import warnings\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Literal\n","\n","import einops\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch as t\n","import torch.nn as nn\n","import torch.optim as optim\n","import wandb\n","from IPython.display import HTML, display\n","from jaxtyping import Bool, Float, Int\n","from matplotlib.animation import FuncAnimation\n","from numpy.random import Generator\n","from torch import Tensor\n","from torch.distributions.categorical import Categorical\n","from torch.optim.optimizer import Optimizer\n","from tqdm import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Make sure exercises are in the path\n","chapter = \"chapter2_rl\"\n","section = \"part3_ppo\"\n","root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n","exercises_dir = root_dir / chapter / \"exercises\"\n","section_dir = exercises_dir / section\n","if str(exercises_dir) not in sys.path:\n","    sys.path.append(str(exercises_dir))\n","\n","# FILTERS: ~colab\n","MAIN = __name__ == \"__main__\"\n","# END FILTERS\n","\n","import part3_ppo.tests as tests\n","from part1_intro_to_rl.utils import set_global_seeds\n","from part2_q_learning_and_dqn.solutions import Probe1, Probe2, Probe3, Probe4, Probe5, get_episode_data_from_infos\n","from part2_q_learning_and_dqn.utils import prepare_atari_env\n","from part3_ppo.utils import arg_help, make_env\n","from plotly_utils import plot_cartpole_obs_and_dones\n","\n","# Register our probes from last time\n","for idx, probe in enumerate([Probe1, Probe2, Probe3, Probe4, Probe5]):\n","    gym.envs.registration.register(id=f\"Probe{idx+1}-v0\", entry_point=probe)\n","\n","Arr = np.ndarray\n","\n","device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["# 0️⃣ A whirlwind tour of PPO\n","\n","TODO!"]},{"cell_type":"markdown","metadata":{"id":"Wrc4voZ6JWQc"},"source":["# 1️⃣ Setting up our agent\n"]},{"cell_type":"markdown","metadata":{"id":"y0H_VB_gJWQc"},"source":["In this section, we'll do the following:\n","\n","* Define a dataclass to hold our PPO arguments\n","* Write functions to create our actor and critic networks (which will eventually be stored in our `PPOAgent` instance)\n","* Write a function to do **generalized advantage estimation** (this will be necessary when computing our objective function during the learning phase)\n","* Fill in our `ReplayMemory` class (for storing and sampling experiences)\n","* Fill in our `PPOAgent` class (a wrapper around our networks and our replay memory, which will turn them into an agent)\n","\n","As a reminder, we'll be continually referring back to [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool) as we go through these exercises. Most of our sections wil refer to one or more of these details."]},{"cell_type":"markdown","metadata":{"id":"u1mKwRBpyF9c"},"source":["## PPO Arguments\n","\n","Just like for DQN, we've provided you with a dataclass containing arguments for your `train_ppo` function. We've also given you a function from `utils` to display all these arguments (including which ones you've changed). Lots of these are the same as for the DQN dataclass.\n","\n","Don't worry if these don't all make sense right now, they will by the end.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":896},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1704482895558,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"Yb3N757zJWQc","outputId":"7577fe14-2f42-48c9-c7d0-d409219dbcb1"},"outputs":[],"source":["@dataclass\n","class PPOArgs:\n","    # Basic / global\n","    seed: int = 1\n","    env_id: str = \"CartPole-v1\"\n","    mode: Literal[\"classic-control\", \"atari\", \"mujoco\"] = \"classic-control\"\n","\n","    # Wandb / logging\n","    use_wandb: bool = False\n","    video_log_freq: int | None = None\n","    wandb_project_name: str = \"PPOCartPole\"\n","    wandb_entity: str = None\n","\n","    # Duration of different phases\n","    total_timesteps: int = 500_000\n","    num_envs: int = 4\n","    num_steps_per_rollout: int = 128\n","    num_minibatches: int = 4\n","    batches_per_learning_phase: int = 4\n","\n","    # Optimization hyperparameters\n","    lr: float = 2.5e-4\n","    max_grad_norm: float = 0.5\n","\n","    # RL hyperparameters\n","    gamma: float = 0.99\n","\n","    # PPO-specific hyperparameters\n","    gae_lambda: float = 0.95\n","    clip_coef: float = 0.2\n","    ent_coef: float = 0.01\n","    vf_coef: float = 0.25\n","\n","    def __post_init__(self):\n","        self.batch_size = self.num_steps_per_rollout * self.num_envs\n","\n","        assert self.batch_size % self.num_minibatches == 0, \"batch_size must be divisible by num_minibatches\"\n","        self.minibatch_size = self.batch_size // self.num_minibatches\n","        self.total_phases = self.total_timesteps // self.batch_size\n","        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches\n","\n","        self.video_save_path = section_dir / \"videos\"\n","\n","\n","# FILTERS: py\n","ARG_HELP_STRINGS = dict(\n","    seed=\"seed of the experiment\",\n","    env_id=\"the id of the environment\",\n","    mode=\"can be 'classic-control', 'atari' or 'mujoco'\",\n","    #\n","    use_wandb=\"if toggled, this experiment will be tracked with Weights and Biases\",\n","    video_log_freq=\"if not None, we log videos this many episodes apart (so shorter episodes mean more frequent logging)\",\n","    wandb_project_name=\"the name of this experiment (also used as the wandb project name)\",\n","    wandb_entity=\"the entity (team) of wandb's project\",\n","    #\n","    total_timesteps=\"total timesteps of the experiments\",\n","    num_envs=\"number of synchronized vector environments in our `envs` object (this is N in the '37 Implementational Details' post)\",\n","    num_steps_per_rollout=\"number of steps taken in the rollout phase (this is M in the '37 Implementational Details' post)\",\n","    num_minibatches=\"the number of minibatches you divide each batch up into\",\n","    batches_per_learning_phase=\"how many times you train on the full batch of data generated in each rollout phase\",\n","    #\n","    lr=\"the learning rate of the optimizer\",\n","    max_grad_norm=\"value used in gradient clipping\",\n","    #\n","    gamma=\"the discount factor gamma\",\n","    gae_lambda=\"the discount factor used in our GAE estimation\",\n","    clip_coef=\"the epsilon term used in the clipped surrogate objective function\",\n","    ent_coef=\"coefficient of entropy bonus term\",\n","    vf_coef=\"cofficient of value loss function\",\n","    #\n","    batch_size=\"N * M in the '37 Implementational Details' post (calculated from other values in PPOArgs)\",\n","    minibatch_size=\"the size of a single minibatch we perform a gradient step on (calculated from other values in PPOArgs)\",\n","    total_phases=\"total number of phases during training (calculated from other values in PPOArgs)\",\n","    total_training_steps=\"total number of minibatches we will perform an update step on during training (calculated from other values in PPOArgs)\",\n",")\n","# END FILTERS\n","\n","\n","if MAIN:\n","    args = PPOArgs(num_minibatches=2)  # changing this also changes minibatch_size and total_training_steps\n","    arg_help(args)"]},{"cell_type":"markdown","metadata":{"id":"S6_PYa6iJWQc"},"source":["## Actor-Critic Implementation ([detail #2](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Orthogonal%20Initialization%20of%20Weights%20and%20Constant%20Initialization%20of%20biases))\n","\n","Implement the `Agent` class according to the diagram, inspecting `envs` to determine the observation shape and number of actions. We are doing separate Actor and Critic networks because [detail #13](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Shared%20and%20separate%20MLP%20networks%20for%20policy%20and%20value%20functions) notes that is performs better than a single shared network in simple environments.\n","\n","Note that today `envs` will actually have multiple instances of the environment inside, unlike yesterday's DQN which had only one instance inside. From the **37 implementation details** post:\n","\n","> In this architecture, PPO first initializes a vectorized environment `envs` that runs $N$ (usually independent) environments either sequentially or in parallel by leveraging multi-processes. `envs` presents a synchronous interface that always outputs a batch of $N$ observations from $N$ environments, and it takes a batch of $N$ actions to step the $N$ environments. When calling `next_obs = envs.reset()`, next_obs gets a batch of $N$ initial observations (pronounced \"next observation\"). PPO also initializes an environment `done` flag variable next_done (pronounced \"next done\") to an $N$-length array of zeros, where its i-th element `next_done[i]` has values of 0 or 1 which corresponds to the $i$-th sub-environment being *not done* and *done*, respectively."]},{"cell_type":"markdown","metadata":{"id":"iyc6QMjFJWQc"},"source":["### Exercise - implement `get_actor_and_critic`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-20 minutes on this exercise.\n","```\n","\n","Use `layer_init` to initialize each `Linear`, overriding the standard deviation argument `std` according to the diagram.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo_mermaid.svg\" width=\"500\">\n","\n","Note, we've given you a \"high level function\" `get_actor_and_critic` which calls one of three possible functions, depending on the `mode` argument. You'll implement the other two modes later. This is one way to keep our code modular."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704487371287,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"R257WXUBJWQc","outputId":"979b5352-27fd-4cd3-c6e5-7de3ea82d0e1"},"outputs":[],"source":["def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):\n","    t.nn.init.orthogonal_(layer.weight, std)\n","    t.nn.init.constant_(layer.bias, bias_const)\n","    return layer\n","\n","\n","def get_actor_and_critic(\n","    envs: gym.vector.SyncVectorEnv,\n","    mode: Literal[\"classic-control\", \"atari\", \"mujoco\"] = \"classic-control\",\n",") -> tuple[nn.Module, nn.Module]:\n","    \"\"\"\n","    Returns (actor, critic), the networks used for PPO, in one of 3 different modes.\n","    \"\"\"\n","    assert mode in [\"classic-control\", \"atari\", \"mujoco\"]\n","\n","    obs_shape = envs.single_observation_space.shape\n","    num_obs = np.array(obs_shape).prod()\n","    num_actions = (\n","        envs.single_action_space.n\n","        if isinstance(envs.single_action_space, gym.spaces.Discrete)\n","        else np.array(envs.single_action_space.shape).prod()\n","    )\n","\n","    if mode == \"classic-control\":\n","        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)\n","    if mode == \"atari\":\n","        actor, critic = get_actor_and_critic_atari(obs_shape, num_actions)  # you'll implement these later\n","    if mode == \"mujoco\":\n","        actor, critic = get_actor_and_critic_mujoco(num_obs, num_actions)  # you'll implement these later\n","\n","    return actor.to(device), critic.to(device)\n","\n","\n","def get_actor_and_critic_classic(num_obs: int, num_actions: int):\n","    \"\"\"\n","    Returns (actor, critic) in the \"classic-control\" case, according to diagram above.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    critic = nn.Sequential(\n","        layer_init(nn.Linear(num_obs, 64)),\n","        nn.Tanh(),\n","        layer_init(nn.Linear(64, 64)),\n","        nn.Tanh(),\n","        layer_init(nn.Linear(64, 1), std=1.0),\n","    )\n","\n","    actor = nn.Sequential(\n","        layer_init(nn.Linear(num_obs, 64)),\n","        nn.Tanh(),\n","        layer_init(nn.Linear(64, 64)),\n","        nn.Tanh(),\n","        layer_init(nn.Linear(64, num_actions), std=0.01),\n","    )\n","    return actor, critic\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    tests.test_get_actor_and_critic(get_actor_and_critic, mode=\"classic-control\")"]},{"cell_type":"markdown","metadata":{"id":"QvvUHogSJWQd"},"source":["<details>\n","<summary>Question - what do you think is the benefit of using a small standard deviation for the last actor layer?</summary>\n","\n","The purpose is to center the initial `agent.actor` logits around zero, in other words an approximately uniform distribution over all actions independent of the state. If you didn't do this, then your agent might get locked into a nearly-deterministic policy early on and find it difficult to train away from it.\n","\n","[Studies suggest](https://openreview.net/pdf?id=nIAxjsniDzg) this is one of the more important initialisation details, and performance is often harmed without it.\n","</details>\n"]},{"cell_type":"markdown","metadata":{"id":"WiYjTplzJWQd"},"source":["## Generalized Advantage Estimation ([detail #5](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Generalized%20Advantage%20Estimation))\n","\n","Let $\\pi_\\theta$ be our policy parameterized by $\\theta$, and $J(\\pi_\\theta)$ denote the expected finite-horizon undiscounted return of the policy. The **gradient of $J(\\pi_\\theta)$ is:\n","$$\n","\\nabla_\\theta J\\left(\\pi_\\theta\\right)=\\underset{\\tau \\sim \\pi_\\theta}{\\mathrm{E}}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t\\right) A^{\\pi_\\theta}\\left(s_t, a_t\\right)\\right]\n","$$\n","where $\\tau$ is a trajectory sampled from $\\pi_\\theta$, and $A^{\\pi_\\theta}(s_t, a_t)$ is the **advantage function**, defined as $Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)$, i.e. how much better it is to choose action $a_t$ in state $s_t$ as compared to the value obtained by following $\\pi_\\theta$ from that state onwards. This equation is sometimes known as the **policy gradient theorem**.\n","\n","We omit the full derivation of this equation, but to sketch out the intuition for it: if the advantage $A^{\\pi_\\theta}(s_t, a_t)$ is positive, this tells us that to increase the expected return $J(\\pi_\\theta)$ we should increase $\\pi_\\theta(a_t \\mid s_t)$ - in other words, make it more likely that we will choose this advantageous action $a_t$ in that state $s_t$.\n","\n","Why don't we just estimate the advantages by the residuals, $\\delta_t = r_t + (1-d_{t+1})\\gamma V^{\\pi_\\theta}(s_{t+1}) - V^{\\pi_\\theta}(s_t)$? After all, this is just substituting the expected value $Q^{\\pi_\\theta}(s_t, a_t) of taking action $a_t$ with the next-step evaluation $r_t + (1-d_{t+1}) \\gamma V^{\\pi_\\theta}(s_{t+1})$ computed from the state we actually transitioned to. The problem with this - **basing advantage estimation on a single action taken is highly unstable**. We don't just want to take into account the change in Q-values at one step, ideall we'd like to see if this action puts us on a better trajectory by looking several steps in advance. For example, if we're playing chess and our Q-values don't understand the idea of sacrificing a piece to gain a better position, the move will look a lot better if we estimate its advantage by projecting several moves ahead!\n","\n","To estimate our advantages, we'll be following [detail #5](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Generalized%20Advantage%20Estimation) - in other words, we'll use the **generalized advantage estimator** which is computed by taking a geometrically decaying sum of the TD residuals:\n","$$\n","\\begin{aligned}\n","& \\hat{A}_t=\\delta_t+(\\gamma \\lambda) \\delta_{t+1}+\\cdots+\\cdots+(\\gamma \\lambda)^{T-t+1} \\delta_{T-1} \\\\\n","& \\text { where } \\quad \\delta_t=r_t+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_t\\right)\n","\\end{aligned}\n","$$\n","Here, $T$ is the maximum number of timesteps sampled in our trajectory,  $\\gamma$ is the discount factor (reflecting the fact that later rewards matter less), and $\\lambda$ (which we've called `gae_lambda` in the code below) essentially controls how far we look ahead to estimate the advantage. When this value is 0 we only take into account the next step i.e. we have $\\hat{A}(s_t, a_t) = \\delta_t$, but when this value is 1 we're essentially saying that (other than time discounting effects) every step in our trajectory is equally important when deciding whether the action $a_t$ was good.\n","\n","We can actually compute the generalized advantage estimator (GAE) recursively, starting from the final step and working backwards, using this nice trick:\n","$$\n","\\hat{A}_t = \\delta_t + (1 - d_{t+1}) (\\gamma \\lambda) \\hat{A}_{t+1}\n","$$\n","\n","<details>\n","<summary>Derivation (short)</summary>\n","\n","If $d_{t+1}=1$ (i.e. we just terminated) then our advantage $A_t$ on the final step of this trajectory is just $\\delta_t = r_t - V(s_t)$. This is correct because we're terminating next step, so the only reward we have yet to accumulate from our action $a_t$ is the immediate reward $r_t$.\n","\n","Working backwards from the terminal step and applying this recursive formula, we get:\n","\n","$$\n","\\begin{aligned}\n","\\hat{A}_{t-1} &= \\delta_{t-1} + (\\gamma \\lambda) \\hat{A}_{t} = \\delta_{t-1} + \\gamma \\lambda \\delta_t \\\\\n","\\hat{A}_{t-2} &= \\delta_{t-2} + (\\gamma \\lambda) \\hat{A}_{t-1} = \\delta_{t-2} + \\gamma \\lambda \\delta_{t-1} + (\\gamma\\lambda)^2 \\delta_t \\\\\n","&\\dots\n","\\end{aligned}\n","$$\n","and so on. This exactly matches the formula given above.\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"8PjVNetUJWQd"},"source":["### Exercise - implement `compute_advantages`\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 20-30 minutes on this exercise.\n","Use the hints if you're stuck; it can be quite a messy function to implement.\n","```\n","\n","Below, you should fill in `compute_advantages`. We recommend using a reversed for loop over $t$ to get it working, and using the recursive formula for GAE given above - don't worry about trying to vectorize it.\n","\n","Tip - make sure you understand what the indices are of the tensors you've been given! The tensors `rewards`, `values` and `terminated` contain $r_t$, $V(s_t)$ and $d_t$ respectively for all $t = 0, 1, ..., T-1$, and `next_value`, `next_terminated` are the values $V(s_T)$ and $d_T$ respectively (required for the calculation of the very last advantage $A_{T-1}$).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1704482900027,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"g1uXpzOuJWQd","outputId":"aedae318-8d9d-4dfd-c855-161f567977d7"},"outputs":[],"source":["@t.inference_mode()\n","def compute_advantages(\n","    next_value: Float[Tensor, \"num_envs\"],\n","    next_terminated: Bool[Tensor, \"num_envs\"],\n","    rewards: Float[Tensor, \"buffer_size num_envs\"],\n","    values: Float[Tensor, \"buffer_size num_envs\"],\n","    terminated: Bool[Tensor, \"buffer_size num_envs\"],\n","    gamma: float,\n","    gae_lambda: float,\n",") -> Float[Tensor, \"buffer_size num_envs\"]:\n","    \"\"\"\n","    Compute advantages using Generalized Advantage Estimation.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    T = values.shape[0]\n","    terminated = terminated.float()\n","    next_terminated = next_terminated.float()\n","\n","    # Get tensors of V(s_{t+1}) and d_{t+1} for all t = 0, 1, ..., T-1\n","    next_values = t.concat([values[1:], next_value[None, :]])\n","    next_terminated = t.concat([terminated[1:], next_terminated[None, :]])\n","\n","    # Compute deltas: \\delta_t = r_t + (1 - d_{t+1}) \\gamma V(s_{t+1}) - V(s_t)\n","    deltas = rewards + gamma * next_values * (1.0 - next_terminated) - values\n","\n","    # Compute advantages using the recursive formula, starting with advantages[T-1] = deltas[T-1] and working backwards\n","    advantages = t.zeros_like(deltas)\n","    advantages[-1] = deltas[-1]\n","    for s in reversed(range(T - 1)):\n","        advantages[s] = deltas[s] + gamma * gae_lambda * (1.0 - terminated[s + 1]) * advantages[s + 1]\n","\n","    return advantages\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    tests.test_compute_advantages(compute_advantages)"]},{"cell_type":"markdown","metadata":{},"source":["<details>\n","<summary>Help - I get <code>RuntimeError: Subtraction, the `-` operator, with a bool tensor is not supported</code></summary>\n","\n","This is probably because you're trying to perform an operation on a boolean tensor `terminated` which was designed for floats. You can fix this by casting the boolean tensor to a float tensor.\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"SDVMKVooJWQd"},"source":["## Replay Memory\n","\n","Our replay memory has some similarities to the replay buffer from yesterday, as well as some important differences.\n","\n","### Sampling method\n","\n","Yesterday, we continually updated our buffer and sliced off old data, and each time we called `sample` we'd take a randomly ordered subset of that data (with replacement).\n","\n","With PPO, we alternate between rollout and learning phases. In rollout, we fill our replay memory entirely. In learning, we call `get_minibatches` to return the entire contents of the replay memory, but randomly shuffled and sorted into minibatches. In this way, we update on every experience, not just random samples. In fact, we'll update on each experience more than once, since we'll repeat the process of (generate minibatches, update on all of them) `batches_per_learning_phase` times during each learning phase.\n","\n","### New variables\n","\n","We store some of the same variables as before - $(s_t, a_t, d_t)$, but with the addition of 3 new variables: the **logprobs** $\\pi(a_t\\mid s_t)$, the **advantages** $A_t$ and the **returns**. Explaining these two variables and why we need them:\n","\n","- `logprobs` are calculated from the logit outputs of our `actor.agent` network, corresponding to the actions $a_t$ which our agent actually chose.\n","    * These are necessary for calculating the clipped surrogate objective (see equation $(7)$ on page page 3 in the [PPO Algorithms paper](https://arxiv.org/pdf/1707.06347.pdf)), which as we'll see later makes sure the agent isn't rewarded for changing its policy an excessive amount.\n","- `advantages` are the terms $\\hat{A}_t$, computed using our function `compute_advantages` from earlier.\n","    - Again, these are used in the calculation of the clipped surrogate objective.\n","- `returns` are given by the formula `returns = advantages + values` - see [detail #9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Value%20Function%20Loss%20Clipping).\n","    - They are used to train the value network, in a way which is equivalent to minimizing the TD residual loss used in DQN.\n","\n","Don't worry if you don't understand all of this now, we'll get to all these variables later."]},{"cell_type":"markdown","metadata":{"id":"Xfp4XXn3tBbx"},"source":["### Exercise - implement `minibatch_indices`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","We'll start by implementing the `get_minibatch_indices` function, as described in [detail #6](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Mini%2Dbatch%20Updates). This will give us a list of length `num_minibatches = batch_size // minibatch_size` indices, each of length `minibatch_size`, and which collectively represent a permutation of the indices `[0, 1, ..., batch_size - 1]` where `batch_size = num_minibatches * minibatch_size`. To help visualize how this works to create our minibatches, we've included a diagram:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo-buffer-sampling-4.png\" width=\"600\">\n","\n","The test code below should also make it clear what your function should be returning.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1704482902780,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"lIgSa6VkJWQd","outputId":"46bb74ff-9e63-4be5-adb3-d4ee3359512a"},"outputs":[],"source":["def get_minibatch_indices(rng: Generator, batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n","    \"\"\"\n","    Return a list of length `num_minibatches`, where each element is an array of `minibatch_size` and the union of all\n","    the arrays is the set of indices [0, 1, ..., batch_size - 1] where `batch_size = num_steps_per_rollout * num_envs`.\n","    \"\"\"\n","    assert batch_size % minibatch_size == 0\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    num_minibatches = batch_size // minibatch_size\n","    indices = rng.permutation(batch_size).reshape(num_minibatches, minibatch_size)\n","    return list(indices)\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    rng = np.random.default_rng(0)\n","\n","    batch_size = 12\n","    minibatch_size = 6\n","    # num_minibatches = batch_size // minibatch_size = 2\n","\n","    indices = get_minibatch_indices(rng, batch_size, minibatch_size)\n","\n","    assert isinstance(indices, list)\n","    assert all(isinstance(x, np.ndarray) for x in indices)\n","    assert np.array(indices).shape == (2, 6)\n","    assert sorted(np.unique(indices)) == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","    print(\"All tests in `test_minibatch_indexes` passed!\")"]},{"cell_type":"markdown","metadata":{"id":"2UeWIb5aJWQe"},"source":["### `ReplayMemory` class\n","\n","Next, we've given you the `ReplayMemory` class. This follows a very similar structure to the DQN equivalent `ReplayBuffer` yesterday, with a bit of added complexity. We'll highlight the key differences below:\n","\n","- There's no `[-self.buffer_size:]` slicing like there was in the DQN buffer yesterday. That's because rather than continually adding to our buffer and removing the oldest data, we'll iterate through a process of (fill entire memory, generate a bunch of minibatches from that memory and train on them, empty the memory, repeat).\n","- The `get_minibatches` method computes the advantages and returns. This isn't really in line with the SoC (separation of concerns) principle, but this is the easiest place to compute them because we can't do it after we sample the minibatches.\n","- A single learning phase involves creating `num_minibatches = batch_size // minibatch_size` minibatches and training on each of them, and then repeating this process `batches_per_learning_phase` times. So the total number of minibaches per learning phase is `batches_per_learning_phase * num_minibatches`.\n","\n","<details>\n","<summary>Question - can you see why <code>advantages</code> can't be computed after we sample minibatches?</summary>\n","\n","The samples are not in chronological order, they're shuffled. The formula for computing advantages required the data to be in chronological order.\n","\n","</details>"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yWicey5nJWQe"},"outputs":[],"source":["@dataclass\n","class ReplayMinibatch:\n","    \"\"\"\n","    Samples from the replay memory, converted to PyTorch for use in neural network training.\n","\n","    Data is equivalent to (s_t, a_t, logpi(a_t|s_t), A_t, A_t + V(s_t), d_{t+1})\n","    \"\"\"\n","\n","    obs: Float[Tensor, \"minibatch_size *obs_shape\"]\n","    actions: Int[Tensor, \"minibatch_size *action_shape\"]\n","    logprobs: Float[Tensor, \"minibatch_size\"]\n","    advantages: Float[Tensor, \"minibatch_size\"]\n","    returns: Float[Tensor, \"minibatch_size\"]\n","    terminated: Bool[Tensor, \"minibatch_size\"]\n","\n","\n","class ReplayMemory:\n","    \"\"\"\n","    Contains buffer; has a method to sample from it to return a ReplayMinibatch object.\n","    \"\"\"\n","\n","    rng: Generator\n","    obs: Float[Arr, \"buffer_size num_envs *obs_shape\"]\n","    actions: Int[Arr, \"buffer_size num_envs *action_shape\"]\n","    logprobs: Float[Arr, \"buffer_size num_envs\"]\n","    values: Float[Arr, \"buffer_size num_envs\"]\n","    rewards: Float[Arr, \"buffer_size num_envs\"]\n","    terminated: Bool[Arr, \"buffer_size num_envs\"]\n","\n","    def __init__(\n","        self,\n","        num_envs: int,\n","        obs_shape: tuple,\n","        action_shape: tuple,\n","        batch_size: int,\n","        minibatch_size: int,\n","        batches_per_learning_phase: int,\n","        seed: int = 42,\n","    ):\n","        self.num_envs = num_envs\n","        self.obs_shape = obs_shape\n","        self.action_shape = action_shape\n","        self.batch_size = batch_size\n","        self.minibatch_size = minibatch_size\n","        self.batches_per_learning_phase = batches_per_learning_phase\n","        self.rng = np.random.default_rng(seed)\n","        self.reset()\n","\n","    def reset(self):\n","        \"\"\"\n","        Resets all stored experiences, ready for new ones to be added to memory.\n","        \"\"\"\n","        self.obs = np.empty((0, self.num_envs, *self.obs_shape), dtype=np.float32)\n","        self.actions = np.empty((0, self.num_envs, *self.action_shape), dtype=np.int32)\n","        self.logprobs = np.empty((0, self.num_envs), dtype=np.float32)\n","        self.values = np.empty((0, self.num_envs), dtype=np.float32)\n","        self.rewards = np.empty((0, self.num_envs), dtype=np.float32)\n","        self.terminated = np.empty((0, self.num_envs), dtype=bool)\n","\n","    def add(\n","        self,\n","        obs: Float[Arr, \"num_envs *obs_shape\"],\n","        actions: Int[Arr, \"num_envs *action_shape\"],\n","        logprobs: Float[Arr, \"num_envs\"],\n","        values: Float[Arr, \"num_envs\"],\n","        rewards: Float[Arr, \"num_envs\"],\n","        terminated: Bool[Arr, \"num_envs\"],\n","    ) -> None:\n","        \"\"\"\n","        Add a batch of transitions to the replay memory.\n","        \"\"\"\n","        # Check shapes & datatypes\n","        for data, expected_shape in zip(\n","            [obs, actions, logprobs, values, rewards, terminated], [self.obs_shape, self.action_shape, (), (), (), ()]\n","        ):\n","            assert isinstance(data, np.ndarray)\n","            assert data.shape == (self.num_envs, *expected_shape)\n","\n","        # Add data to buffer (not slicing off old elements)\n","        self.obs = np.concatenate((self.obs, obs[None, :]))\n","        self.actions = np.concatenate((self.actions, actions[None, :]))\n","        self.logprobs = np.concatenate((self.logprobs, logprobs[None, :]))\n","        self.values = np.concatenate((self.values, values[None, :]))\n","        self.rewards = np.concatenate((self.rewards, rewards[None, :]))\n","        self.terminated = np.concatenate((self.terminated, terminated[None, :]))\n","\n","    def get_minibatches(\n","        self, next_value: Tensor, next_terminated: Tensor, gamma: float, gae_lambda: float\n","    ) -> list[ReplayMinibatch]:\n","        \"\"\"\n","        Returns a list of minibatches. Each minibatch has size `minibatch_size`, and the union over all minibatches is\n","        `batches_per_learning_phase` copies of the entire replay memory.\n","        \"\"\"\n","        # Convert everything to tensors on the correct device\n","        obs, actions, logprobs, values, rewards, terminated = (\n","            t.tensor(x, device=device)\n","            for x in [self.obs, self.actions, self.logprobs, self.values, self.rewards, self.terminated]\n","        )\n","\n","        # Compute advantages & returns\n","        advantages = compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda)\n","        returns = advantages + values\n","\n","        # Return a list of minibatches\n","        minibatches = []\n","        for _ in range(self.batches_per_learning_phase):\n","            for indices in get_minibatch_indices(self.rng, self.batch_size, self.minibatch_size):\n","                minibatches.append(\n","                    ReplayMinibatch(\n","                        *[\n","                            data.flatten(0, 1)[indices]\n","                            for data in [obs, actions, logprobs, advantages, returns, terminated]\n","                        ]\n","                    )\n","                )\n","\n","        # Reset memory (since we only need to call this method once per learning phase)\n","        self.reset()\n","\n","        return minibatches"]},{"cell_type":"markdown","metadata":{"id":"r3elNNoNJWQe"},"source":["Like before, here's some code to generate and plot observations.\n","\n","The first plot shows the current observations $s_t$ (with dotted lines indicating a terminated episode $d_{t+1} = 1$). The solid lines indicate the transition between different environments in `envs` (because unlike yesterday, we're actually using more than one environment in our `SyncVectorEnv`). There are `batch_size = num_steps_per_rollout * num_envs = 128 * 2 = 256` observations in total, with `128` coming from each environment.\n","\n","The second plot shows a single minibatch of sampled experiences from full memory. Each minibatch has size `minibatch_size = 128`, and `minibatches` contains in total `batches_per_learning_phase * (batch_size // minibatch_size) = 2 * 2 = 4` minibatches.\n","\n","Note that we don't need to worry about terminal observations here, because we're not actually logging `next_obs` (unlike DQN, this won't be part of our loss function)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":2720,"status":"ok","timestamp":1704481712119,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"iF3g-0vZ20Im","outputId":"3146a4af-a040-4cfc-f0d2-84ea881426aa"},"outputs":[],"source":["# TAGS: main\n","\n","num_steps_per_rollout = 128\n","num_envs = 2\n","batch_size = num_steps_per_rollout * num_envs  # 256\n","\n","minibatch_size = 128\n","num_minibatches = batch_size // minibatch_size  # 2\n","\n","batches_per_learning_phase = 2\n","\n","envs = gym.vector.SyncVectorEnv([make_env(\"CartPole-v1\", i, i, \"test\") for i in range(num_envs)])\n","memory = ReplayMemory(num_envs, (4,), (), batch_size, minibatch_size, batches_per_learning_phase)\n","\n","logprobs = values = np.zeros(envs.num_envs)  # dummy values, just so we can see demo of plot\n","obs, _ = envs.reset()\n","\n","for i in range(args.num_steps_per_rollout):\n","    # Choose random action, and take a step in the environment\n","    actions = envs.action_space.sample()\n","    next_obs, rewards, terminated, truncated, infos = envs.step(actions)\n","\n","    # Add experience to memory\n","    memory.add(obs, actions, logprobs, values, rewards, terminated)\n","    obs = next_obs\n","\n","plot_cartpole_obs_and_dones(\n","    memory.obs,\n","    memory.terminated,\n","    title=\"Current obs s<sub>t</sub><br>Dotted lines indicate d<sub>t+1</sub> = 1, solid lines are environment separators\",\n","    # FILTERS: ~\n","    # filename=str(section_dir / \"2301-A.html\"),\n","    # END FILTERS\n",")\n","\n","next_value = next_done = t.zeros(envs.num_envs).to(device)  # dummy values, just so we can see demo of plot\n","minibatches = memory.get_minibatches(next_value, next_done, gamma=0.99, gae_lambda=0.95)\n","\n","plot_cartpole_obs_and_dones(\n","    minibatches[0].obs.cpu(),\n","    minibatches[0].terminated.cpu(),\n","    title=\"Current obs (sampled)<br>this is what gets fed into our model for training\",\n","    # FILTERS: ~\n","    # filename=str(section_dir / \"2301-B.html\"),\n","    # END FILTERS\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"5FyraiztJWQi"},"source":["## PPO Agent\n","\n","As the final task in this section, you should fill in the agent's `play_step` method. This is conceptually similar to what you did during DQN, but with a few key differences:\n","\n","- In DQN we selected actions based on our Q-network & an epsilon-greedy policy, but instead your actions will be generated directly from your actor network\n","- Here, you'll have to compute the extra data `logprobs` and `values`, which we didn't have to deal with in DQN\n"]},{"cell_type":"markdown","metadata":{"id":"0YPtnhzXJWQi"},"source":["### Exercise - implement `PPOAgent`\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 20-40 minutes on this exercise.\n","```\n","\n","A few tips:\n","\n","- When sampling actions (and calculating logprobs), you might find `torch.distributions.categorical.Categorical` useful. If `logits` is a 2D tensor of shape `(N, k)` containing a batch of logit vectors and `dist = Categorical(logits=logits)`, then:\n","    - `actions = dist.sample()` will give you a vector of `N` sampled actions (which will be integers in the range `[0, k)`),\n","    - `logprobs = dist.log_prob(actions)` will give you a vector of the `N` logprobs corresponding to the sampled actions\n","- Make sure to use inference mode when using `obs` to compute `logits` and `values`, since all you're doing here is getting experiences for your memory - you aren't doing gradient descent based on these values.\n","- Check the shape of your arrays when adding them to memory (the `add` method has lots of `assert` statements here to help you), and also make sure that they are arrays not tensors by calling `.cpu().numpy()` on them.\n","- Remember to update `self.next_obs` and `self.next_terminated` at the end of the function!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":827,"status":"ok","timestamp":1704482908789,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"D7VGNTAGJWQi","outputId":"0154896b-5630-4572-c8ec-e87ae61ef2aa"},"outputs":[],"source":["class PPOAgent:\n","    critic: nn.Sequential\n","    actor: nn.Sequential\n","\n","    def __init__(self, envs: gym.vector.SyncVectorEnv, actor: nn.Module, critic: nn.Module, memory: ReplayMemory):\n","        super().__init__()\n","        self.envs = envs\n","        self.actor = actor\n","        self.critic = critic\n","        self.memory = memory\n","\n","        self.step = 0  # Tracking number of steps taken (across all environments)\n","        self.next_obs = t.tensor(envs.reset()[0], device=device, dtype=t.float)  # need starting obs (in tensor form)\n","        self.next_terminated = t.zeros(envs.num_envs, device=device, dtype=t.bool)  # need starting termination=False\n","\n","    def play_step(self) -> list[dict]:\n","        \"\"\"\n","        Carries out a single interaction step between the agent and the environment, and adds results to the replay memory.\n","\n","        Returns the list of info dicts returned from `self.envs.step`.\n","        \"\"\"\n","        # Get newest observations (i.e. where we're starting from)\n","        obs = self.next_obs\n","        terminated = self.next_terminated\n","\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        # Compute logits based on newest observation, and use it to get an action distribution we sample from\n","        with t.inference_mode():\n","            logits = self.actor(obs)\n","        dist = Categorical(logits=logits)\n","        actions = dist.sample()\n","\n","        # Step environment based on the sampled action\n","        next_obs, rewards, next_terminated, next_truncated, infos = self.envs.step(actions.cpu().numpy())\n","\n","        # Calculate logprobs and values, and add this all to replay memory\n","        logprobs = dist.log_prob(actions).cpu().numpy()\n","        with t.inference_mode():\n","            values = self.critic(obs).flatten().cpu().numpy()\n","        self.memory.add(obs.cpu().numpy(), actions.cpu().numpy(), logprobs, values, rewards, terminated.cpu().numpy())\n","\n","        # Set next observation & termination state\n","        self.next_obs = t.from_numpy(next_obs).to(device, dtype=t.float)\n","        self.next_terminated = t.from_numpy(next_terminated).to(device, dtype=t.float)\n","        # SOLUTION END\n","\n","        self.step += self.envs.num_envs\n","        return infos\n","\n","    def get_minibatches(self, gamma: float, gae_lambda: float) -> list[ReplayMinibatch]:\n","        \"\"\"\n","        Gets minibatches from the replay memory, and resets the memory\n","        \"\"\"\n","        with t.inference_mode():\n","            next_value = self.critic(self.next_obs).flatten()\n","        minibatches = self.memory.get_minibatches(next_value, self.next_terminated, gamma, gae_lambda)\n","        self.memory.reset()\n","        return minibatches\n","\n","\n","if MAIN:\n","    tests.test_ppo_agent(PPOAgent)"]},{"cell_type":"markdown","metadata":{"id":"PCrDNis6JWQj"},"source":["# 2️⃣ Learning Phase\n"]},{"cell_type":"markdown","metadata":{"id":"o4MN56j3JWQj"},"source":["In the last section, we wrote a lot of setup code (including handling most of how our rollout phase will work). Next, we'll turn to the learning phase.\n","\n","In the next exercises, you'll write code to compute your total objective function. This is given by equation $(9)$ in the paper, and is the sum of three terms - we'll implement each one individually.\n","\n","Note - the convention we've used in these exercises for signs is that **your function outputs should be the expressions in equation $(9)$**, in other words you will compute $L_t^{CLIP}(\\theta)$, $c_1 L_t^{VF}(\\theta)$ and $c_2 S[\\pi_\\theta](s_t)$. We will then perform **gradient ascent** by passing `maximize=True` into our optimizers. An equally valid solution would be to just return the negative of the objective function.\n"]},{"cell_type":"markdown","metadata":{"id":"3A7PX49tJWQj"},"source":["## Objective function\n"]},{"cell_type":"markdown","metadata":{"id":"bZrghRCuJWQj"},"source":["### Clipped Surrogate Objective ([detail #8](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Clipped%20surrogate%20objective,-(ppo2/model)))\n","\n","For each minibatch, calculate $L^{CLIP}$ from equation $(7)$ in the paper. This will allow us to improve the parameters of our actor.\n","\n","Note - in the paper, don't confuse $r_{t}$ which is reward at time $t$ with $r_{t}(\\theta)$, which is the probability ratio between the current policy (output of the actor) and the old policy (stored in `mb_logprobs`).\n","\n","Pay attention to the normalization instructions in [detail #7](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Normalization%20of%20Advantages) when implementing this loss function. They add a value of `eps = 1e-8` to the denominator to avoid division by zero, you should also do this.\n","\n","You can use the `probs.log_prob` method to get the log probabilities that correspond to the actions in `mb_action`.\n","\n","Note - if you're wondering why we're using a `Categorical` type rather than just using `log_prob` directly, it's because we'll be using them to sample actions later on in our `train_ppo` function. Also, categoricals have a useful method for returning the entropy of a distribution (which will be useful for the entropy term in the loss function).\n"]},{"cell_type":"markdown","metadata":{"id":"7zTB9kfaBPeW"},"source":["### Exercise - write `calc_clipped_surrogate_objective`\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 10-25 minutes on this exercise.\n","```"]},{"cell_type":"markdown","metadata":{"id":"J2M-PI2B5duk"},"source":["Implement the function below. A few last things to note:\n","\n","- Our `mb_action` has shape `(minibatch_size, *action_shape)`, but in most of the environments we're dealing with (CartPole, and later the Breakout Atari env) the action shape is an empty tuple, which is why we have the assert statement at the start of this function.\n","- Don't forget to normalize advantages, and add epsilon to the denominator!\n","- The clip formula can be a bit finnicky (i.e. when you take signs and max/mins), we recommend breaking the computation onto a few separate lines rather than doing it all in one go!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1704482911939,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"bvCGUl1CJWQk","outputId":"6216c254-2c90-460f-ed78-45e9d12f0af2"},"outputs":[],"source":["def calc_clipped_surrogate_objective(\n","    probs: Categorical,\n","    mb_action: Int[Tensor, \"minibatch_size\"],\n","    mb_advantages: Float[Tensor, \"minibatch_size\"],\n","    mb_logprobs: Float[Tensor, \"minibatch_size\"],\n","    clip_coef: float,\n","    eps: float = 1e-8,\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"Return the clipped surrogate objective, suitable for maximisation with gradient ascent.\n","\n","    probs:\n","        a distribution containing the actor's unnormalized logits of shape (minibatch_size, num_actions)\n","    mb_action:\n","        what actions actions were taken in the sampled minibatch\n","    mb_advantages:\n","        advantages calculated from the sampled minibatch\n","    mb_logprobs:\n","        logprobs of the actions taken in the sampled minibatch (according to the old policy)\n","    clip_coef:\n","        amount of clipping, denoted by epsilon in Eq 7.\n","    eps:\n","        used to add to std dev of mb_advantages when normalizing (to avoid dividing by zero)\n","    \"\"\"\n","    assert mb_action.shape == mb_advantages.shape == mb_logprobs.shape\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    logits_diff = probs.log_prob(mb_action) - mb_logprobs\n","\n","    r_theta = t.exp(logits_diff)\n","\n","    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n","\n","    non_clipped = r_theta * mb_advantages\n","    clipped = t.clip(r_theta, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n","\n","    return t.minimum(non_clipped, clipped).mean()\n","    # SOLUTION END\n","\n","\n","if MAIN:\n","    tests.test_calc_clipped_surrogate_objective(calc_clipped_surrogate_objective)"]},{"cell_type":"markdown","metadata":{"id":"jxwojj46JWQk"},"source":["### Value Function Loss ([detail #9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Value%20Function%20Loss%20Clipping))\n","\n","The value function loss lets us improve the parameters of our critic. Today we're going to implement the simple form: this is just the mean squared difference between the following two terms:\n","\n","* The **critic's prediction**\n","    * This is $V_\\theta(s_t)$ in the paper, and `values` in the diagram earlier (colored blue).\n","* The **observed returns**\n","    * This is $V_t^\\text{targ}$ in the paper, and `returns` in the diagram earlier (colored yellow).\n","    * We defined it as `advantages + values`, where in this case `values` is the item stored in memory rather than the one computed by our network during the learning phase. We can interpret `returns` as a more accurate estimate of values, since the `advantages` term takes into account the rewards $r_{t+1}, r_{t+2}, ...$ which our agent actually accumulated.\n","\n","The PPO paper did a more complicated thing with clipping, but we're going to deviate from the paper and NOT clip, since [detail #9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Value%20Function%20Loss%20Clipping) gives evidence that it isn't beneficial. $\\,$\n"]},{"cell_type":"markdown","metadata":{"id":"1Z-oIvMFJWQk"},"source":["### Exercise - write `calc_value_function_loss`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 5-10 minutes on this exercise.\n","```\n","\n","Implement `calc_value_function_loss` which returns the term denoted $c_1 L_t^{VF}$ in equation $(9)$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1704482913837,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"cEnEw2QrJWQk","outputId":"e819fae9-f321-4f38-dd26-80904ac63cc5"},"outputs":[],"source":["def calc_value_function_loss(\n","    values: Float[Tensor, \"minibatch_size\"], mb_returns: Float[Tensor, \"minibatch_size\"], vf_coef: float\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"Compute the value function portion of the loss function.\n","\n","    values:\n","        the value function predictions for the sampled minibatch (using the updated critic network)\n","    mb_returns:\n","        the target for our updated critic network (computed as `advantages + values` from the old network)\n","    vf_coef:\n","        the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n","    \"\"\"\n","    assert values.shape == mb_returns.shape\n","\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    return vf_coef * (values - mb_returns).pow(2).mean()\n","    # SOLUTION END\n","\n","\n","# HIDE\n","if MAIN:\n","    tests.test_calc_value_function_loss(calc_value_function_loss)"]},{"cell_type":"markdown","metadata":{"id":"XwNbdkJSJWQl"},"source":["### Entropy Bonus ([detail #10](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Overall%20Loss%20and%20Entropy%20Bonus))\n","\n","The entropy bonus term is intended to incentivize exploration by increasing the entropy of the actions distribution. For a discrete probability distribution $p$, the entropy $H$ is defined as\n","$$\n","H(p) = \\sum_x p(x) \\ln \\frac{1}{p(x)}\n","$$\n","If $p(x) = 0$, then we define $0 \\ln \\frac{1}{0} := 0$ (by taking the limit as $p(x) \\to 0$).\n","You should understand what entropy of a discrete distribution means, but you don't have to implement it yourself: `probs.entropy` computes it using the above formula but in a numerically stable way, and in\n","a way that handles the case where $p(x) = 0$.\n","\n","Question: in CartPole, what are the minimum and maximum values that entropy can take? What behaviors correspond to each of these cases?\n","\n","<details>\n","<summary>Answer</summary>\n","\n","The minimum entropy is zero, under the policy \"always move left\" or \"always move right\".\n","\n","The maximum entropy is $\\ln(2) \\approx 0.693$ under the uniform random policy over the 2 actions.\n","</details>\n","\n","Separately from its role in the loss function, the entropy of our action distribution is a useful diagnostic to have: if the entropy of agent's actions is near the maximum, it's playing nearly randomly which means it isn't learning anything (assuming the optimal policy isn't random). If it is near the minimum especially early in training, then the agent might not be exploring enough.\n"]},{"cell_type":"markdown","metadata":{"id":"NKEWHrRJJWQl"},"source":["### Exercise - write `calc_entropy_bonus`\n","\n","```c\n","Difficulty: 🔴⚪⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to ~10 minutes on this exercise.\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1704482915061,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"GJNCM_5lJWQl","outputId":"ecfeb692-996c-4512-9953-1dc5080c773b"},"outputs":[],"source":["def calc_entropy_bonus(dist: Categorical, ent_coef: float):\n","    \"\"\"Return the entropy bonus term, suitable for gradient ascent.\n","\n","    dist:\n","        the probability distribution for the current policy\n","    ent_coef:\n","        the coefficient for the entropy loss, which weights its contribution to the overall objective function. Denoted by c_2 in the paper.\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    return ent_coef * dist.entropy().mean()\n","    # SOLUTION END\n","\n","\n","# HIDE\n","if MAIN:\n","    tests.test_calc_entropy_bonus(calc_entropy_bonus)"]},{"cell_type":"markdown","metadata":{"id":"qysj4hsGJWQm"},"source":["## Adam Optimizer & Scheduler (details [#3](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=The%20Adam%20Optimizer%E2%80%99s%20Epsilon%20Parameter) & [#4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Adam%20Learning%20Rate%20Annealing))\n","\n","Even though Adam is already an adaptive learning rate optimizer, empirically it's still beneficial to decay the learning rate.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JjwoQnvtJWQm"},"source":["### Exercise - implement `PPOScheduler`\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","Implement a linear decay from `initial_lr` to `end_lr` over `total_training_steps` steps. Also, make sure you read details details [#3](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=The%20Adam%20Optimizer%E2%80%99s%20Epsilon%20Parameter) and [#4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Adam%20Learning%20Rate%20Annealing) so you don't miss any of the Adam implementational details. Note, the training terminates after `num_updates`, so you don't need to worry about what the learning rate will be after this point.\n","\n","Recall from our look at optimizers in the first week: we edit hyperparameters like learning rate of an optimizer by iterating through `optimizer.param_groups` and setting the `param_group[\"lr\"]` attribute.\n","\n","We've implemented the `make_optimizer` function for you. Note that we've passed in `list(actor.parameters()) + list(critic.parameters())` to the optimizer, which is a list concatenation of the actor and critic parameters. They have to be passed in as a single iterable, rather than a list of iterables. You also have the option to use something like `itertools.chain(actor.parameters(), critic.parameters())` if you want to avoid creating new lists.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":231,"status":"ok","timestamp":1704482916842,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"PmZD54YkJWQm","outputId":"24a9b65c-c297-4bd9-a09b-13dd34a0ff37"},"outputs":[],"source":["class PPOScheduler:\n","    def __init__(self, optimizer: Optimizer, initial_lr: float, end_lr: float, total_training_steps: int):\n","        self.optimizer = optimizer\n","        self.initial_lr = initial_lr\n","        self.end_lr = end_lr\n","        self.total_training_steps = total_training_steps\n","        self.n_step_calls = 0\n","\n","    def step(self):\n","        \"\"\"Implement linear learning rate decay so that after total_training_steps calls to step, the learning rate is end_lr.\n","\n","        Do this by directly editing the learning rates inside each param group (i.e. `param_group[\"lr\"] = ...`), for each param\n","        group in `self.optimizer.param_groups`.\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        self.n_step_calls += 1\n","        frac = self.n_step_calls / self.total_training_steps\n","        assert frac <= 1\n","        for param_group in self.optimizer.param_groups:\n","            param_group[\"lr\"] = self.initial_lr + frac * (self.end_lr - self.initial_lr)\n","        # SOLUTION END\n","\n","\n","def make_optimizer(\n","    actor: nn.Module, critic: nn.Module, total_training_steps: int, initial_lr: float, end_lr: float = 0.0\n",") -> tuple[optim.Adam, PPOScheduler]:\n","    \"\"\"\n","    Return an appropriately configured Adam with its attached scheduler.\n","    \"\"\"\n","    optimizer = optim.AdamW(\n","        list(actor.parameters()) + list(critic.parameters()), lr=initial_lr, eps=1e-5, maximize=True\n","    )\n","    scheduler = PPOScheduler(optimizer, initial_lr, end_lr, total_training_steps)\n","    return optimizer, scheduler\n","\n","\n","# HIDE\n","if MAIN:\n","    tests.test_ppo_scheduler(PPOScheduler)"]},{"cell_type":"markdown","metadata":{"id":"RPl7PxPyJWQm"},"source":["# 3️⃣ Training Loop\n"]},{"cell_type":"markdown","metadata":{"id":"PTV-Ha59JWQn"},"source":["## Writing your training loop\n","\n","Finally, we can package this all together into our full training loop. The `train` function has been written for you: it just performs an alternating sequence of rollout & learning phases, a total of `args.total_phases` times each. You can see in the `__post_init__` method of our dataclass how this value was calculated by dividing the total agent steps by the batch size (which is the number of agent steps required per rollout phase).\n","\n","Your job will be to fill in the logic for the rollout & learning phases. This will involve using many of the functions you've written in the last 2 sections."]},{"cell_type":"markdown","metadata":{"id":"jEep0ZKnJWQn"},"source":["### Exercise - complete the `PPOTrainer` class\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴🔴\n","Importance: 🔵🔵🔵🔵🔵\n","\n","You should spend up to 30-60 minutes on this exercise (including logging).\n","It will be the hardest exercise today.\n","```\n","\n","You should fill in the following methods. Ignoring logging, they should do the following:\n","\n","- `rollout_phase`\n","    - Step the agent through the environment for `num_steps_per_rollout` total steps, which collects `num_steps_per_rollout * num_envs` experiences into the replay memory\n","    - This will be near identical to yesterday's `add_to_replay_buffer` method\n","- `learning_phase`\n","    - Sample from the replay memory using `agent.get_minibatches` (which returns a list of minibatches), this automatically resets the memory\n","    - Iterate over these minibatches, and for each minibatch you should backprop wrt the objective function computed from the `compute_ppo_objective` method\n","    - Note that after each `backward()` call, you should also **clip the gradients** in accordance with [detail #11](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Global%20Gradient%20Clipping%20)\n","        - You can use `nn.utils.clip_grad_norm(parameters, max_grad_norm)` for this - see [documentation page](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html). The `args` dataclass contains the max norm for clipping gradients\n","- `compute_ppo_objective`\n","    - Handles actual computation of the PPO objective function\n","    - Note that you'll need to compute `logits` and `values` from the minibatch observation `minibatch.obs`, but unlike in our previous functions **this shouldn't be done in inference mode**, since these are actually the values that propagate gradients!\n","    - Also remember to get the sign correct - our optimizer was set up for **gradient ascent**, so we should return `total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus` from this method\n","\n","Once you get this working, you should also add logging:\n","\n","- Log the data for any terminated episodes in `rollout_phase`\n","    - This should be the same as yesterday's exercise, in fact you can use the same `get_episode_data_from_infos` helper function (we've imported it for you at the top of this file)\n","- Log useful data related to your different objective function components in `compute_ppo_objective`\n","    - Some recommendations for what to log can be found in [detail #12](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Debug%20variables)\n","    \n","We recommend not focusing too much on wandb & logging initially, just like yesterday. Once again you have the probe environments to test your code, and even after that point you'll get better feedback loops by turning off wandb until you're more confident in your solution. The most important thing to log is the episode length & reward in `rollout_phase`, and if you have this appearing on your progress bar then you'll be able to get a good sense of how your agent is doing. Even without this and without wandb, videos of your runs will automatically be saved to the folder `part3_ppo/videos/run_name`, with `run_name` being the name set at initialization for your `PPOTrainer` class.\n","\n","If you get stuck at any point during this implementation, you can look at the solutions or send a message in the Slack channel for help."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"vEAxjHywJWQn"},"outputs":[],"source":["class PPOTrainer:\n","    def __init__(self, args: PPOArgs):\n","        set_global_seeds(args.seed)\n","        self.args = args\n","        self.run_name = f\"{args.env_id}__{args.wandb_project_name}__seed{args.seed}__{time.strftime('%Y%m%d-%H%M%S')}\"\n","        self.envs = gym.vector.SyncVectorEnv(\n","            [make_env(idx=idx, run_name=self.run_name, **args.__dict__) for idx in range(args.num_envs)]\n","        )\n","\n","        # Define some basic variables from our environment\n","        self.num_envs = self.envs.num_envs\n","        self.action_shape = self.envs.single_action_space.shape\n","        self.obs_shape = self.envs.single_observation_space.shape\n","\n","        # Create our replay memory\n","        self.memory = ReplayMemory(\n","            self.num_envs,\n","            self.obs_shape,\n","            self.action_shape,\n","            args.batch_size,\n","            args.minibatch_size,\n","            args.batches_per_learning_phase,\n","            args.seed,\n","        )\n","\n","        # Create our networks & optimizer\n","        self.actor, self.critic = get_actor_and_critic(self.envs, mode=args.mode)\n","        self.optimizer, self.scheduler = make_optimizer(self.actor, self.critic, args.total_training_steps, args.lr)\n","\n","        # Create our agent\n","        self.agent = PPOAgent(self.envs, self.actor, self.critic, self.memory)\n","\n","    def rollout_phase(self) -> dict | None:\n","        \"\"\"\n","        This function populates the memory with a new set of experiences, using `self.agent.play_step` to step through\n","        the environment. It also returns a dict of data which you can include in your progress bar postfix.\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        data = None\n","        t0 = time.time()\n","\n","        for step in range(self.args.num_steps_per_rollout):\n","            # Play a step, returning the infos dict (containing information for each environment)\n","            infos = self.agent.play_step()\n","\n","            # Get data from environments, and log it if some environment did actually terminate\n","            new_data = get_episode_data_from_infos(infos)\n","            if new_data is not None:\n","                data = new_data\n","                if self.args.use_wandb:\n","                    wandb.log(new_data, step=self.agent.step)\n","\n","        if self.args.use_wandb:\n","            wandb.log(\n","                {\"SPS\": (self.args.num_steps_per_rollout * self.num_envs) / (time.time() - t0)}, step=self.agent.step\n","            )\n","\n","        return data\n","        # SOLUTION END\n","\n","    def learning_phase(self) -> None:\n","        \"\"\"\n","        This function does the following:\n","\n","            - Generates minibatches from memory\n","            - Calculates the objective function, and takes an optimization step based on it\n","            - Clips the gradients (see detail #11)\n","            - Steps the learning rate scheduler\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        minibatches = self.agent.get_minibatches(self.args.gamma, self.args.gae_lambda)\n","        for minibatch in minibatches:\n","            objective_fn = self.compute_ppo_objective(minibatch)\n","            objective_fn.backward()\n","            nn.utils.clip_grad_norm_(\n","                list(self.actor.parameters()) + list(self.critic.parameters()), self.args.max_grad_norm\n","            )\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","            self.scheduler.step()\n","        # SOLUTION END\n","\n","    def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, \"\"]:\n","        \"\"\"\n","        Handles learning phase for a single minibatch. Returns objective function to be maximized.\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        logits = self.agent.actor(minibatch.obs)\n","        dist = Categorical(logits=logits)\n","        values = self.agent.critic(minibatch.obs).squeeze()\n","\n","        clipped_surrogate_objective = calc_clipped_surrogate_objective(\n","            dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef\n","        )\n","        value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)\n","        entropy_bonus = calc_entropy_bonus(dist, self.args.ent_coef)\n","\n","        total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus\n","\n","        with t.inference_mode():\n","            newlogprob = dist.log_prob(minibatch.actions)\n","            logratio = newlogprob - minibatch.logprobs\n","            ratio = logratio.exp()\n","            approx_kl = (ratio - 1 - logratio).mean().item()\n","            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n","        if self.args.use_wandb:\n","            wandb.log(\n","                dict(\n","                    total_steps=self.agent.step,\n","                    values=values.mean().item(),\n","                    lr=self.scheduler.optimizer.param_groups[0][\"lr\"],\n","                    value_loss=value_loss.item(),\n","                    clipped_surrogate_objective=clipped_surrogate_objective.item(),\n","                    entropy=entropy_bonus.item(),\n","                    approx_kl=approx_kl,\n","                    clipfrac=np.mean(clipfracs),\n","                ),\n","                step=self.agent.step,\n","            )\n","\n","        return total_objective_function\n","        # SOLUTION END\n","\n","    def train(self) -> None:\n","        if args.use_wandb:\n","            wandb.init(\n","                project=self.args.wandb_project_name,\n","                entity=self.args.wandb_entity,\n","                name=self.run_name,\n","                monitor_gym=self.args.video_log_freq is not None,\n","            )\n","            wandb.watch([self.actor, self.critic], log=\"all\", log_freq=50)\n","\n","        pbar = tqdm(range(self.args.total_phases))\n","        last_logged_time = time.time()  # so we don't update the progress bar too much\n","\n","        for phase in pbar:\n","            data = self.rollout_phase()\n","            if data is not None and time.time() - last_logged_time > 0.5:\n","                last_logged_time = time.time()\n","                pbar.set_postfix(phase=phase, **data)\n","\n","            self.learning_phase()\n","\n","        self.envs.close()\n","        if self.args.use_wandb:\n","            wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"VGRo3mpuWtAm"},"source":["<details>\n","<summary>Solution (simple, no logging)</summary>\n","\n","```python\n","def rollout_phase(self) -> dict | None:\n","    \"\"\"\n","    This function populates the memory with a new set of experiences, using `self.agent.play_step` to step through\n","    the environment. It also returns a dict of data which you can include in your progress bar postfix.\n","    \"\"\"\n","    for step in range(self.args.num_steps_per_rollout):\n","        infos = self.agent.play_step()\n","\n","def learning_phase(self) -> None:\n","    \"\"\"\n","    This function does the following:\n","\n","        - Generates minibatches from memory\n","        - Calculates the objective function, and takes an optimization step based on it\n","        - Clips the gradients (see detail #11)\n","        - Steps the learning rate scheduler\n","    \"\"\"\n","    minibatches = self.agent.get_minibatches(self.args.gamma, self.args.gae_lambda)\n","    for minibatch in minibatches:\n","        objective_fn = self.compute_ppo_objective(minibatch)\n","        objective_fn.backward()\n","        nn.utils.clip_grad_norm_(\n","            list(self.actor.parameters()) + list(self.critic.parameters()), self.args.max_grad_norm\n","        )\n","        self.optimizer.step()\n","        self.optimizer.zero_grad()\n","        self.scheduler.step()\n","\n","def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, \"\"]:\n","    \"\"\"\n","    Handles learning phase for a single minibatch. Returns objective function to be maximized.\n","    \"\"\"\n","    logits = self.agent.actor(minibatch.obs)\n","    dist = Categorical(logits=logits)\n","    values = self.agent.critic(minibatch.obs).squeeze()\n","\n","    clipped_surrogate_objective = calc_clipped_surrogate_objective(\n","        dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef\n","    )\n","    value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)\n","    entropy_bonus = calc_entropy_bonus(dist, self.args.ent_coef)\n","\n","    total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus\n","    return total_objective_function\n","```\n","\n","</details>\n","\n","<details>\n","<summary>Solution (full)</summary>\n","\n","```python\n","def rollout_phase(self) -> dict | None:\n","    \"\"\"\n","    This function populates the memory with a new set of experiences, using `self.agent.play_step` to step through\n","    the environment. It also returns a dict of data which you can include in your progress bar postfix.\n","    \"\"\"\n","    data = None\n","    t0 = time.time()\n","\n","    for step in range(self.args.num_steps_per_rollout):\n","        # Play a step, returning the infos dict (containing information for each environment)\n","        infos = self.agent.play_step()\n","\n","        # Get data from environments, and log it if some environment did actually terminate\n","        new_data = get_episode_data_from_infos(infos)\n","        if new_data is not None:\n","            data = new_data\n","            if self.args.use_wandb:\n","                wandb.log(new_data, step=self.agent.step)\n","\n","    if self.args.use_wandb:\n","        wandb.log(\n","            {\"SPS\": (self.args.num_steps_per_rollout * self.num_envs) / (time.time() - t0)}, step=self.agent.step\n","        )\n","\n","    return data\n","\n","def learning_phase(self) -> None:\n","    \"\"\"\n","    This function does the following:\n","\n","        - Generates minibatches from memory\n","        - Calculates the objective function, and takes an optimization step based on it\n","        - Clips the gradients (see detail #11)\n","        - Steps the learning rate scheduler\n","    \"\"\"\n","    minibatches = self.agent.get_minibatches(self.args.gamma, self.args.gae_lambda)\n","    for minibatch in minibatches:\n","        objective_fn = self.compute_ppo_objective(minibatch)\n","        objective_fn.backward()\n","        nn.utils.clip_grad_norm_(\n","            list(self.actor.parameters()) + list(self.critic.parameters()), self.args.max_grad_norm\n","        )\n","        self.optimizer.step()\n","        self.optimizer.zero_grad()\n","        self.scheduler.step()\n","\n","def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, \"\"]:\n","    \"\"\"\n","    Handles learning phase for a single minibatch. Returns objective function to be maximized.\n","    \"\"\"\n","    logits = self.agent.actor(minibatch.obs)\n","    dist = Categorical(logits=logits)\n","    values = self.agent.critic(minibatch.obs).squeeze()\n","\n","    clipped_surrogate_objective = calc_clipped_surrogate_objective(\n","        dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef\n","    )\n","    value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)\n","    entropy_bonus = calc_entropy_bonus(dist, self.args.ent_coef)\n","\n","    total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus\n","\n","    with t.inference_mode():\n","        newlogprob = dist.log_prob(minibatch.actions)\n","        logratio = newlogprob - minibatch.logprobs\n","        ratio = logratio.exp()\n","        approx_kl = (ratio - 1 - logratio).mean().item()\n","        clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n","    if self.args.use_wandb:\n","        wandb.log(\n","            dict(\n","                total_steps=self.agent.step,\n","                values=values.mean().item(),\n","                lr=self.scheduler.optimizer.param_groups[0][\"lr\"],\n","                value_loss=value_loss.item(),\n","                clipped_surrogate_objective=clipped_surrogate_objective.item(),\n","                entropy=entropy_bonus.item(),\n","                approx_kl=approx_kl,\n","                clipfrac=np.mean(clipfracs),\n","            ),\n","            step=self.agent.step,\n","        )\n","\n","    return total_objective_function\n","```\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"7b1Wm6yRWvOd"},"source":["Here's some code to run your model on the probe environments (and assert that they're all working fine).\n","\n","A brief recap of the probe environments, along with recommendations of where to go to debug if one of them fails (note that these won't be true 100% of the time, but should hopefully give you some useful direction):\n","\n","* **Probe 1 tests basic learning ability**. If this fails, it means the agent has failed to learn to associate a constant observation with a constant reward. You should check your loss functions and optimizers in this case.\n","* **Probe 2 tests the agent's ability to differentiate between 2 different observations (and learn their respective values)**. If this fails, it means the agent has issues with handling multiple possible observations.\n","* **Probe 3 tests the agent's ability to handle time & reward delay**. If this fails, it means the agent has problems with multi-step scenarios of discounting future rewards. You should look at how your agent step function works.\n","* **Probe 4 tests the agent's ability to learn from actions leading to different rewards**. If this fails, it means the agent has failed to change its policy for different rewards, and you should look closer at how your agent is updating its policy based on the rewards it receives & the loss function.\n","* **Probe 5 tests the agent's ability to map observations to actions**. If this fails, you should look at the code which handles multiple timesteps, as well as the code that handles the agent's map from observations to actions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33883,"status":"ok","timestamp":1704481813954,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"Q7OiP8urhLKj","outputId":"8fd83488-f12e-43a0-dc29-4cf331ab5d39"},"outputs":[],"source":["def test_probe(probe_idx: int):\n","    \"\"\"\n","    Tests a probe environment by training a network on it & verifying that the value functions are\n","    in the expected range.\n","    \"\"\"\n","    # Train our network\n","    args = PPOArgs(\n","        env_id=f\"Probe{probe_idx}-v0\",\n","        wandb_project_name=f\"test-probe-{probe_idx}\",\n","        total_timesteps=[7500, 7500, 12500, 20000, 20000][probe_idx - 1],\n","        lr=0.001,\n","        video_log_freq=None,\n","        use_wandb=False,\n","    )\n","    trainer = PPOTrainer(args)\n","    trainer.train()\n","    agent = trainer.agent\n","\n","    # Get the correct set of observations, and corresponding values we expect\n","    obs_for_probes = [[[0.0]], [[-1.0], [+1.0]], [[0.0], [1.0]], [[0.0]], [[0.0], [1.0]]]\n","    expected_value_for_probes = [[[1.0]], [[-1.0], [+1.0]], [[args.gamma], [1.0]], [[1.0]], [[1.0], [1.0]]]\n","    expected_probs_for_probes = [None, None, None, [[0.0, 1.0]], [[1.0, 0.0], [0.0, 1.0]]]\n","    tolerances = [1e-3, 1e-3, 1e-3, 2e-3, 2e-3]\n","    obs = t.tensor(obs_for_probes[probe_idx - 1]).to(device)\n","\n","    # Calculate the actual value & probs, and verify them\n","    with t.inference_mode():\n","        value = agent.critic(obs)\n","        probs = agent.actor(obs).softmax(-1)\n","    expected_value = t.tensor(expected_value_for_probes[probe_idx - 1]).to(device)\n","    t.testing.assert_close(value, expected_value, atol=tolerances[probe_idx - 1], rtol=0)\n","    expected_probs = expected_probs_for_probes[probe_idx - 1]\n","    if expected_probs is not None:\n","        t.testing.assert_close(probs, t.tensor(expected_probs).to(device), atol=tolerances[probe_idx - 1], rtol=0)\n","    print(\"Probe tests passed!\\n\")\n","\n","\n","if MAIN:\n","    for probe_idx in range(1, 6):\n","        test_probe(probe_idx)"]},{"cell_type":"markdown","metadata":{"id":"LGB6-JikU4ni"},"source":["Once you've passed the tests for all 5 probe environments, you should test your model on Cartpole.\n","\n","See an example wandb run you should be getting [here](https://api.wandb.ai/links/callum-mcdougall/fdmhh8gq)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":635,"referenced_widgets":["c6f9ad7c1539408d980e3e9663fc4efd","56e27afd07e8418d9080da521c2db4e7","4d302b7ce65b46ee82fd9a974dfd62df","b701a8deb51d40eb98bbf8ba4146d23f","1873366fc48249eda00aae161adb0ef9","a5544061299a4616856b2aab578137af","fdfabc617f3f44629458a28b6b6ea817","d8b1ba53d9a74d509cb5a3e22b9bbab8"]},"executionInfo":{"elapsed":340766,"status":"ok","timestamp":1704482154710,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"W9sjlD3ZeSgX","outputId":"3d706e48-2cee-4109-86a1-e1e0cf88106b"},"outputs":[],"source":["# TAGS: main\n","\n","args = PPOArgs(use_wandb=True, video_log_freq=50)\n","trainer = PPOTrainer(args)\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"KIeWFCjoJWQo"},"source":["\n","<details>\n","<summary>Question - if you've done this correctly (and logged everything), clipped surrogate objective will be close to zero. Should you infer from this that it's not important in the overall algorithm (compared to the components of the objective function which are larger)?</summary>\n","\n","No, this doesn't mean that it's not important.\n","\n","Clipped surrogate objective is a moving target. At each rollout phase, we generate new experiences, and the expected value of the clipped surrogate objective will be zero (because the expected value of advantages is zero). But this doesn't mean that differentiating clipped surrogate objective wrt the policy doesn't have a large gradient!\n","\n","As we make update steps in the learning phase, the policy values $\\pi(a_t \\mid s_t)$ will increase for actions which have positive advantages, and decrease for actions which have negative advantages, so the clipped surrogate objective will no longer be zero in expectation. But (thanks to the fact that we're clipping changes larger than $\\epsilon$) it will still be very small.\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{"id":"SiQL6gxhJWQp"},"source":["### Catastrophic forgetting\n","\n","Note - you might see performance very high initially and then drop off rapidly (before recovering again).\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cf2.png\" width=\"600\">\n","\n","(This plot shows episodic return, which in this case is identical to episodic length.)\n","\n","This is a well-known RL phenomena called **catastrophic forgetting**. It happens when the memory only contains good experiences, and the agent forgets how to recover from bad experiences. One way to fix this is to change your buffer to keep 10 of experiences from previous epochs, and 90% of experiences from the current epoch. Can you implement this?\n","\n","(Note - reward shaping can also help fix this problem - see next section.)\n"]},{"cell_type":"markdown","metadata":{"id":"1zicYLfRJWQp"},"source":["## Reward Shaping\n","\n","The rewards for `CartPole` encourage the agent to keep the episode running for as long as possible, which it then needs to associate with balancing the pole. But we can write a wrapper around the `CartPoleEnv` to modify the dynamics of the environment, and help the agent learn faster.\n","\n","Try to modify the reward to make the task as easy to learn as possible. Compare this against your performance on the original environment, and see if the agent learns faster with your shaped reward. If you can bound the reward on each timestep between 0 and 1, this will make comparing the results to `CartPole-v1` easier.\n","\n","<details>\n","<summary>Help - I'm not sure what I'm meant to return in this function.</summary>\n","\n","The tuple `(obs, reward, done, info)` is returned from the CartPole environment. Here, `rew` is always 1 unless the episode has terminated.\n","\n","You should change this, so that `reward` incentivises good behaviour, even if the pole hasn't fallen yet. You can use the information returned in `obs` to construct a new reward function.\n","\n","</details>\n","\n","<details>\n","<summary>Help - I'm confused about how to choose a reward function. (Try and think about this for a while before looking at this dropdown.)</summary>\n","\n","Right now, the agent always gets a reward of 1 for each timestep it is active. You should try and change this so that it gets a reward between 0 and 1, which is closer to 1 when the agent is performing well / behaving stably, and equals 0 when the agent is doing very poorly.\n","\n","The variables we have available to us are cart position, cart velocity, pole angle, and pole angular velocity, which I'll denote as $x$, $v$, $\\theta$ and $\\omega$.\n","\n","Here are a few suggestions which you can try out:\n","* $r = 1 - (\\theta / \\theta_{\\text{max}})^2$. This will have the effect of keeping the angle close to zero.\n","* $r = 1 - (x / x_{\\text{max}})^2$. This will have the effect of pushing it back towards the centre of the screen (i.e. it won't tip and fall to the side of the screen).\n","\n","You could also try using e.g. $|\\theta / \\theta_{\\text{max}}|$ rather than $(\\theta / \\theta_{\\text{max}})^2$. This would still mean reward is in the range (0, 1), but it would result in a larger penalty for very small deviations from the vertical position.\n","\n","You can also try a linear combination of two or more of these rewards!\n","</details>\n","\n","\n","<details>\n","<summary>Help - my agent's episodic return is smaller than it was in the original CartPole environment.</summary>\n","\n","This is to be expected, because your reward function is no longer always 1 when the agent is upright. Both your time-discounted reward estimates and your actual realised rewards will be less than they were in the cartpole environment.\n","\n","For a fairer test, measure the length of your episodes - hopefully your agent learns how to stay upright for the entire 500 timestep interval as fast as or faster than it did previously.\n","</details>\n","\n","Note - if you want to use the maximum possible values of `x` and `theta` in your reward function (to keep it bounded between 0 and 1) then you can. These values can be found at the [documentation page](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) (note - the actual values you'll want are given in the bullet points below the table, not in the table itself!). You can also use `self.x_threshold` and `self.theta_threshold_radians` to get these values directly (you can look at the source code for `CartPoleEnv` to see how these are calculated).\n"]},{"cell_type":"markdown","metadata":{"id":"5O0u3ySDJWQp"},"source":["### Exercise - implement reward shaping\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 15-30 minutes on this exercise.\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"hG39nPTwEJ6S"},"source":["See [this link](https://api.wandb.ai/links/callum-mcdougall/p7e739rp) for what an ideal wandb run here should look like (using the reward function in the solutions)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":600,"referenced_widgets":["79b65cec2a3346a6b3b8a845dc8c4d8b","03bb038eb947454f8318fe9f68d7dee5","bd7b53091f464a3da46a58d322d9f2f6","78d082b21b5d4bbb978d51739b0a5512","0aa320f239e643b589ea4830ce9a4eb9","4004d03ebf0542debf6eb8b46df5c70a","e7b6bb11025d4d8d9af9de89a7d4359c","87447a318ec04900b71d8116a659041c"]},"executionInfo":{"elapsed":334948,"status":"ok","timestamp":1704482489656,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"5a8YK3XMOsS0","outputId":"e64592de-25f2-46b5-8321-c8edd75b91d4"},"outputs":[],"source":["from gymnasium.envs.classic_control import CartPoleEnv\n","\n","\n","class EasyCart(CartPoleEnv):\n","    def step(self, action):\n","        obs, reward, terminated, truncated, info = super().step(action)\n","\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        x, v, theta, omega = obs\n","\n","        # First reward: angle should be close to zero\n","        reward_1 = 1 - abs(theta / 0.2095)\n","        # Second reward: position should be close to the center\n","        reward_2 = 1 - abs(x / 2.4)\n","\n","        # Combine both rewards (keep it in the [0, 1] range)\n","        reward_new = (reward_1 + reward_2) / 2\n","        # SOLUTION END\n","\n","        return obs, reward_new, terminated, truncated, info\n","\n","\n","if MAIN:\n","    gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n","    args = PPOArgs(env_id=\"EasyCart-v0\", use_wandb=True, video_log_freq=50)\n","    trainer = PPOTrainer(args)\n","    trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"N7JVs-tMJWQq"},"source":["<details>\n","<summary>Solution (one possible implementation)</summary>\n","\n","I tried out a few different simple reward functions here. One of the best ones I found used a mix of absolute value penalties for both the angle and the horizontal position (this outperformed using absolute value penalty for just one of these two). My guess as to why this is the case - penalising by horizontal position helps the agent improve its long-term strategy, and penalising by angle helps the agent improve its short-term strategy, so both combined work better than either on their own.\n","\n","```python\n","class EasyCart(CartPoleEnv):\n","    def step(self, action):\n","        obs, rew, terminated, truncated, info = super().step(action)\n","        \n","        x, v, theta, omega = obs\n","\n","        # First reward: angle should be close to zero\n","        rew_1 = 1 - abs(theta / 0.2095)\n","        # Second reward: position should be close to the center\n","        rew_2 = 1 - abs(x / 2.4)\n","\n","        # Combine both rewards (keep it in the [0, 1] range)\n","        rew_new = (rew_1 + rew_2) / 2\n","\n","        return obs, rew_new, terminated, truncated, info\n","```\n","\n","The result:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/best-episode-length.png\" width=\"600\">\n","\n","To illustrate the point about different forms of reward optimizing different kinds of behaviour - below are links to three videos generated during the WandB training, one of just position penalisation, one of just angle penalisation, and one of both. Can you guess which is which?\n","\n","* [First video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-48-08---Vmlldzo0ODI1NDcw?accessToken=uajtb4w1gaqkbrf2utonbg2b93lfdlw9eaet4qd9n6zuegkb3mif7l3sbuke8l4j)\n","* [Second video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-47-22---Vmlldzo0ODI1NDY2?accessToken=qoss34zyuaso1b5s40nehamsk7nj93ijopmscesde6mjote0i194e7l99sg2k6dg)\n","* [Third video](https://wandb.ai//callum-mcdougall/PPOCart/reports/videos-23-07-07-13-45-15---Vmlldzo0ODI1NDQ4?accessToken=n1btft5zfqx0aqk8wkuh13xtp5mn19q5ga0mpjmvjnn2nq8q62xz4hsomd0vnots)\n","\n","<details>\n","<summary>Answer</summary>\n","\n","* First video = angle penalisation\n","* Second video = both (from the same video as the loss curve above)\n","* Third video = position penalisation\n","\n","</details>\n","\n","</details>\n","\n","<br>\n","\n","Now, change the environment such that the reward incentivises the agent to spin very fast. You may change the termination conditions of the environment (i.e. return a different value for `done`) if you think this will help."]},{"cell_type":"markdown","metadata":{"id":"DoV8gJHoEhc5"},"source":["See [this link](https://api.wandb.ai/links/callum-mcdougall/86y2vtsk) for what an ideal wandb run here should look like (using the reward function in the solutions)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":339045,"status":"ok","timestamp":1704482828687,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"wazvcyKBJWQq","outputId":"513a76a1-6050-4287-bacc-5baef0bff986"},"outputs":[],"source":["class SpinCart(CartPoleEnv):\n","    def step(self, action):\n","        obs, reward, terminated, truncated, info = super().step(action)\n","\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        x, v, theta, omega = obs\n","\n","        # Allow for 360-degree rotation (but keep the cart on-screen)\n","        terminated = abs(x) > self.x_threshold\n","\n","        # Reward function incentivises fast spinning while staying still & near centre\n","        rotation_speed_reward = min(1, 0.1 * abs(omega))\n","        stability_penalty = max(1, abs(x / 2.5) + abs(v / 10))\n","        reward_new = rotation_speed_reward - 0.5 * stability_penalty\n","        # SOLUTION END\n","\n","        return (obs, reward_new, terminated, truncated, info)\n","\n","\n","if MAIN:\n","    gym.envs.registration.register(id=\"SpinCart-v0\", entry_point=SpinCart, max_episode_steps=500)\n","    args = PPOArgs(env_id=\"SpinCart-v0\", use_wandb=True, video_log_freq=50)\n","    trainer = PPOTrainer(args)\n","    trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"nyk0BsMFJWQq"},"source":["<details>\n","<summary>Solution (one possible implementation)</summary>\n","\n","```python\n","class SpinCart(gym.envs.classic_control.cartpole.CartPoleEnv):\n","    def step(self, action):\n","        obs, reward, done, info = super().step(action)\n","        \n","        x, v, theta, omega = obs\n","\n","        # Allow for 360-degree rotation (but keep the cart on-screen)\n","        done = abs(x) > self.x_threshold\n","\n","        # Reward function incentivises fast spinning while staying still & near centre\n","        rotation_speed_reward = min(1, 0.1 * abs(omega))\n","        stability_penalty = max(1, abs(x / 2.5) + abs(v / 10))\n","        reward_new = rotation_speed_reward - 0.5 * stability_penalty\n","\n","        return obs, reward_new, done, info\n","```\n","\n","</details>\n","\n","Another thing you can try is \"dancing\". It's up to you to define what qualifies as \"dancing\" - work out a sensible definition, and the reward function to incentive it."]},{"cell_type":"markdown","metadata":{"id":"B6WVI0yOIovS"},"source":["# 4️⃣ Atari"]},{"cell_type":"markdown","metadata":{"id":"3BuLnOH2IvLZ"},"source":["## Introduction\n","\n","In this section, you'll extend your PPO implementation to play Atari games.\n","\n","The `gym` library supports a variety of different Atari games - you can find them [here](https://www.gymlibrary.dev/environments/atari/) (if you get a message when you click on this link asking whether you want to switch to gymnasium, ignore this and proceed to the gym site). You can try whichever ones you want, but we recommend you stick with the easier environments like Pong, Breakout, and Space Invaders.\n","\n","The environments in this game are very different. Rather than having observations of shape `(4,)` (representing a vector of `(x, v, theta, omega)`), the raw observations are now images of shape `(210, 160, 3)`, representing pixels in the game screen. This leads to a variety of additional challenges relative to the Cartpole environment, for example:\n","\n","* We need a much larger network, because finding the optimal strategy isn't as simple as solving a basic differential equation\n","* Reward shaping is much more difficult, because our observations are low-level and don't contain easily-accessible information about the high-level abstractions in the game (finding these abstractions in the first place is part of the model's challenge!)\n","\n","The action space is also different for each environment. For example, in Breakout, the environment has 4 actions - run the code below to see this (if you get an error, try restarting the kernel and running everything again, minus the library installs)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1704482932258,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"ZGzDAUOJml_l","outputId":"534c9f23-e5a1-44a9-f11d-e0c61ee45c8a"},"outputs":[],"source":["# TAGS: main\n","\n","env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n","\n","print(env.action_space)  # Discrete(4): 4 actions to choose from\n","print(env.observation_space)  # Box(0, 255, (210, 160, 3), uint8): an RGB image of the game screen\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","\n","<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Discrete(4)\n","Box(0, 255, (210, 160, 3), uint8)</pre>"]},{"cell_type":"markdown","metadata":{"id":"jVsKfcvvmm40"},"source":["These 4 actions are \"do nothing\", \"fire the ball\", \"move right\", and \"move left\" respectively, which you can see from:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TAGS: main\n","\n","print(env.get_action_meanings())\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","\n","<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">['NOOP', 'FIRE', 'RIGHT', 'LEFT']</pre>"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","You can see more details on the game-specific [documentation page](https://ale.farama.org/environments/breakout/). On this documentation page, you can also see information like the reward for this environment. In this case, the reward comes from breaking bricks in the wall (more reward from breaking bricks higher up). This is a more challenging reward function than the one for CartPole, where a very simple strategy (move in the direction you're tipping) leads directly to a higher reward by marginally prolonging episode length.\n","\n","We can also run the code below to take some random steps in our environment and animate the results:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_frames(frames: Int[Arr, \"timesteps height width channels\"], figsize=(4, 5)):\n","    fig, ax = plt.subplots(figsize=figsize)\n","    im = ax.imshow(frames[0])\n","    plt.close()\n","\n","    def update(frame):\n","        im.set_array(frame)\n","        return [im]\n","\n","    ani = FuncAnimation(fig, update, frames=frames, interval=100)\n","    display(HTML(ani.to_jshtml()))\n","\n","\n","# FILTERS: ~\n","def save_display_frames(frames: Int[Arr, \"timesteps height width channels\"], filename: str, figsize=(4, 5)):\n","    fig, ax = plt.subplots(figsize=figsize)\n","    im = ax.imshow(frames[0])\n","    plt.close()\n","\n","    def update(frame):\n","        im.set_array(frame)\n","        return [im]\n","\n","    ani = FuncAnimation(fig, update, frames=frames, interval=100)\n","    display(HTML(ani.to_jshtml()))\n","\n","    with open(filename, \"w\") as f:\n","        f.write(ani.to_jshtml())\n","\n","\n","# END FILTERS\n","\n","if MAIN:\n","    nsteps = 150\n","\n","    frames = []\n","    obs, info = env.reset()\n","    for _ in tqdm(range(nsteps)):\n","        action = env.action_space.sample()\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        frames.append(obs)\n","\n","    display_frames(np.stack(frames))\n","\n","    # FILTERS: ~\n","    # save_display_frames(np.stack(frames), str(section_dir / \"2302.html\"))\n","    # END FILTERS\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","TAGS: html\n","\n","<div style=\"text-align: left\"><embed src=\"https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-23/2302.html\" width=\"500\" height=\"620\"></div>"]},{"cell_type":"markdown","metadata":{"id":"2X0J5oPaIs0p"},"source":["### Playing Breakout\n","\n","Just like for Cartpole and MountainCar, we're given you a Python file to play Atari games yourself. The file is called `play_breakout.py`, and running it (i.e. `python play_breakout.py`) will open up a window for you to play the game. Take note of the key instructions, which will be printed in your terminal.\n","\n","You should also be able to try out other games, by changing the relevant parts of the `play_breakout.py` file to match those games' [documentation pages](https://www.gymlibrary.dev/environments/atari/complete_list/)."]},{"cell_type":"markdown","metadata":{"id":"R-OQeAAjI5Gz"},"source":["## Implementational details of Atari\n","\n","The [37 Implementational Details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Atari%2Dspecific%20implementation%20details) post describes how to get PPO working for games like Atari. In the sections below, we'll go through these steps."]},{"cell_type":"markdown","metadata":{"id":"A9pzJ9rTI66p"},"source":["### Wrappers (details [#1-7](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=The%20Use%20of%20NoopResetEnv), and [#9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Scaling%20the%20Images%20to%20Range%20%5B0%2C%201%5D))\n","\n","All the extra details except for one are just wrappers on the environment, which implement specific behaviours. For example:\n","\n","* **Frame Skipping** - we repeat the agent's action for a number of frames (by default 4), and sum the reward over these frames. This saves time when the model's forward pass is computationally cheaper than an environment step.\n","* **Image Transformations** - we resize the image from `(210, 160)` to `(L, L)` for some smaller value `L` (in this case we'll use 84), and convert it to grayscale.\n","\n","We've written some environment wrappers for you (and imported some others from the `gymnasium` library), combining them all together into the `prepare_atari_env` function in the `part3_ppo/utils.py` file. You can have a read of this and see how it works, but since we're implementing these for you, you won't have to worry about them too much.\n","\n","The code below visualizes the results of them (with the frames stacked across rows, so we can see them all at once). You might want to have a think about how the kind of information your actor & critic networks are getting here, and how this might make the RL task easier."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TAGS: main\n","\n","env_wrapped = prepare_atari_env(env)\n","\n","frames = []\n","obs, info = env_wrapped.reset()\n","for _ in tqdm(range(nsteps)):\n","    action = env_wrapped.action_space.sample()\n","    obs, reward, terminated, truncated, info = env_wrapped.step(action)\n","    obs = einops.repeat(np.array(obs), \"frames h w -> h (frames w) 3\")  # stack frames across the row\n","    frames.append(obs)\n","\n","display_frames(np.stack(frames), figsize=(12, 3))\n","\n","# FILTERS: ~\n","# save_display_frames(np.stack(frames), str(section_dir / \"2303.html\"), figsize=(12, 3))\n","# END FILTERS\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","TAGS: html\n","\n","<div style=\"text-align: left\"><embed src=\"https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-23/2303.html\" width=\"1250\" height=\"420\"></div>"]},{"cell_type":"markdown","metadata":{"id":"Wbj03Oy4I8Vs"},"source":["### Shared CNN for actor & critic ([detail #8](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Shared%20Nature%2DCNN%20network))\n","\n","This is the most interesting one conceptually. If we have a new observation space then it naturally follows that we need a new architecture, and if we're working with images then using a convolutional neural network is reasonable. But another particularly interesting feature here is that we use a **shared architecture** for the actor and critic networks. The idea behind this is that the early layers of our model extract features from the environment (i.e. they find the high-level abstractions contained in the image), and then the actor and critic components perform **feature extraction** to turn these features into actions / value estimates respectively. This is commonly referred to as having a **policy head** and a **value head**. We'll see this idea come up later, when we perform RL on transformers."]},{"cell_type":"markdown","metadata":{"id":"zdOCiUVZI-of"},"source":["### Exercise - rewrite `get_actor_and_critic`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","The function `get_actor_and_critic` had a boolean argument `atari`, which we ignored previously, but which we'll now return to. When this argument is `False` then the function should behave exactly as it did before (i.e. the Cartpole version), but when `True` then it should return a shared CNN architecture for the actor and critic. The architecture should be as follows:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ppo_mermaid_2.svg\" width=\"350\">\n","\n","Note - when calculating the number of input features for the linear layer, you can assume that the value `L` is 4 modulo 8, i.e. we can write `L = 8m + 4` for some integer `m`. This will make the convolutions easier to track. You shouldn't hardcode the number of input features assuming an input shape of `(4, 84, 84)`; this is bad practice!\n","\n","We leave the exercise of finding the number of input features to the linear layer as a challenge for you. If you're stuck, you can find a hint in the section below (this isn't a particularly conceptually important detail).\n","\n","<details>\n","<summary>Help - I don't know what the number of inputs for the linear layer should be.</summary>\n","\n","The linear layer is fed 64 input features. By symmetry of convolutions and of original input, the shape of the linear layer's input (flattened) must have input features `64 * L_new * L_new`. Our only challenge is to find `L_new` in terms of `L`.\n","\n","There's never any padding, so for a conv with parameters `(size, stride)`, the dimensions change as `L -> 1 + (L - size) // stride` (see the [documentation page](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)). So we have:\n","\n","$$\n","\\begin{aligned}\n","8m + 4  \\quad &\\rightarrow  \\quad 1 + \\frac{(8m + 4) - 8}{4} \\quad = \\quad 2m \\\\\n"," \\\\\n","2m      \\quad &\\rightarrow  \\quad 1 + \\frac{2m - 4}{2}       \\quad = \\quad m - 1 \\\\\n"," \\\\\n","m - 1   \\quad &\\rightarrow  \\quad 1 + \\frac{(m - 1) - 3}{1}  \\quad = \\quad m - 3\n","\\end{aligned}\n","$$\n","\n","For instance, if `L = 84` then `m = 10` and `L_new = m-3 = 7`. So the linear layer is fed 64 features of shape `(64, 7, 7)`\n","\n","</details>\n","\n","Now, you can fill in the `get_actor_and_critic_atari` function below, which is called when we call `get_actor_and_critic` with `mode == \"atari\"`.\n","\n","Note that we take the observation shape as argument, not the number of observations. It should be `(4, L, L)` as indicated by the diagram. The shape `(4, L, L)` is a reflection of the fact that we're using 4 frames of history per input (which helps the model calculate things like velocity), and each of these frames is a monochrome resized square image."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2653,"status":"ok","timestamp":1704487366871,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"KyCkGE__JCk9","outputId":"26f53ed9-06a6-48b8-e80e-8a7401027517"},"outputs":[],"source":["def get_actor_and_critic_atari(obs_shape: tuple[int], num_actions: int):\n","    \"\"\"\n","    Returns (actor, critic) in the \"atari\" case, according to diagram above.\n","    \"\"\"\n","    assert obs_shape[-1] % 8 == 4\n","    # Your new code should go here\n","\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    L_after_convolutions = (obs_shape[-1] // 8) - 3\n","    in_features = 64 * L_after_convolutions * L_after_convolutions\n","\n","    hidden = nn.Sequential(\n","        layer_init(nn.Conv2d(4, 32, 8, stride=4, padding=0)),\n","        nn.ReLU(),\n","        layer_init(nn.Conv2d(32, 64, 4, stride=2, padding=0)),\n","        nn.ReLU(),\n","        layer_init(nn.Conv2d(64, 64, 3, stride=1, padding=0)),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        layer_init(nn.Linear(in_features, 512)),\n","        nn.ReLU(),\n","    )\n","\n","    actor = nn.Sequential(hidden, layer_init(nn.Linear(512, num_actions), std=0.01))\n","    critic = nn.Sequential(hidden, layer_init(nn.Linear(512, 1), std=1))\n","\n","    return actor, critic\n","    # SOLUTION END\n","\n","\n","# HIDE\n","if MAIN:\n","    tests.test_get_actor_and_critic(get_actor_and_critic, mode=\"atari\")"]},{"cell_type":"markdown","metadata":{"id":"nkghZpoCJJF6"},"source":["## Training Atari\n","\n","Now, you should be able to run an Atari training loop!\n","\n","We recommend you use the following parameters, for fidelity:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":618,"referenced_widgets":["7b55d4cc46bc4bf3aa4d4c29b393e087","1a5c4926c66c4aea9961a95f514f8b25","aa2fcc88da544199b94175576ead01a0","263ce0ad652749a7914b33fa8a4a6d34","648817b2cf1a444fb0714b36eae3a236","92632d8b197a4f11a638caddca91439b","5f0039c9da594b50980cab9406be467e","7cecde6e8a594bccaafbbb12372c6311"]},"executionInfo":{"elapsed":3417473,"status":"ok","timestamp":1704486361988,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"lWfrLBGanxIX","outputId":"065e364a-0fcc-47fa-9451-b8b64a95c60d"},"outputs":[],"source":["# TAGS: main\n","\n","args = PPOArgs(\n","    env_id=\"ALE/Breakout-v5\",\n","    wandb_project_name=\"PPOAtari\",\n","    use_wandb=True,\n","    mode=\"atari\",\n","    clip_coef=0.1,\n","    num_envs=8,\n","    video_log_freq=25,\n",")\n","trainer = PPOTrainer(args)\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"y7cYBu3RJXob"},"source":["Note that this will probably take a lot longer to train than your previous experiments, because the architecture is much larger, and finding an initial strategy is much harder than it was for CartPole. Don't worry if it starts off with pretty bad performance (on my machine the code above takes about 40 minutes to run, and I only start seeing any improvement after about the 5-10 minute mark, or approx 70k total agent steps). You can always experiment with different methods to try and boost performance early on, like an entroy bonus which is initially larger then decays (analogous to our epsilon scheduling in DQN, which would reduce the probability of exploration over time).\n","\n","Here is a video produced from a successful run, using the parameters above:\n","\n","<video width=\"320\" height=\"480\" controls>\n","<source src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/media-23/2304.mp4\" type=\"video/mp4\">\n","</video>\n","\n","and here's the corresponding plot of episodic returns (with episoic lengths following a similar pattern):\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/wandb-atari-returns.png\" width=\"550\">"]},{"cell_type":"markdown","metadata":{"id":"_R0puGukJYw5"},"source":["### A note on debugging crashed kernels\n","\n","> *This section is more relevant if you're doing these exercises on VSCode; you can skip it if you're in Colab.*\n","\n","Because the `gym` library is a bit fragile, sometimes you can get uninformative kernel errors like this:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/kernel_error.png\" width=\"600\">\n","\n","which annoyingly doesn't tell you much about the nature or location of the error. When this happens, it's often good practice to replace your code with lower-level code bit by bit, until the error message starts being informative.\n","\n","For instance, you might start with `trainer.train()`, and if this fails without an informative error message then you might try replacing this function call with the actual contents of the `train` function (which should involve the methods `trainer.rollout_phase()` and `trainer.learning_phase()`). If the problem is in `rollout_phase`, you can again replace this line with the actual contents of this method.\n","\n","If you're working in `.py` files rather than `.ipynb`, a useful tip - as well as running `Shift + Enter` to run the cell your cursor is in, if you have text highlighted (and you've turned on `Send Selection To Interactive Window` in VSCode settings) then using `Shift + Enter` will run just the code you've highlighted. This could be a single variable name, a single line, or a single block of code."]},{"cell_type":"markdown","metadata":{"id":"_8KQkx92JfFE"},"source":["# 5️⃣ Mujoco"]},{"cell_type":"markdown","metadata":{},"source":["> An important note - **mujoco environments are notoriously demanding when it comes to having exactly the right library installs and versions.** If you're having trouble at any point during these exercises, we recommend switching to the Colab notebooks (where these exercises have been thoroughly tested). You can always return to VSCode for the next set of exercises!\n"]},{"cell_type":"markdown","metadata":{"id":"za-phuppJlmP"},"source":["## Installation & Rendering\n","\n","Running the following code should install the necessary packages. If you're on a Colab / the Jupyter notebook interface then you'll already have run it during your setup code, if not then we recommend you switch to a Linux-based VM (you can see instructions for this in the Streamlit homepage)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30001,"status":"ok","timestamp":1704487112515,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"4o_v9cuoJkiX","outputId":"81f81ac1-fec4-4323-f249-2b3713dc294f"},"outputs":[],"source":["# FILTERS: ~py\n","# TAGS: master-comment\n","\n","!sudo apt-get install -y libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev software-properties-common\n","!sudo apt-get install -y patchelf\n"]},{"cell_type":"markdown","metadata":{"id":"R_4CyjSOJihD"},"source":["To test that this works, run the following. The first time you run this, it might take about 1-2 minutes, and throw up several warnings and messages. But the cell should still run without raising an exception, and all subsequent times you run it, it should be a lot faster (with no error messages)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64146,"status":"ok","timestamp":1704487182266,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"USqU0A_gJn9q","outputId":"612fa968-a40b-43a5-9e5c-af5fa2ce7a83"},"outputs":[],"source":["# TAGS: main\n","\n","env = gym.make(\"Hopper-v4\", render_mode=\"rgb_array\")\n","\n","print(env.action_space)\n","print(env.observation_space)\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","\n","<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Box(-1.0, 1.0, (3,), float32)\n","Box(-inf, inf, (11,), float64)</pre>"]},{"cell_type":"markdown","metadata":{"id":"vn4GStUaJpA6"},"source":["Previously, we've dealt with discrete action spaces (e.g. going right or left in Cartpole). But here, we have a continuous action space - the actions take the form of a vector of 3 values, each in the range `[-1.0, 1.0]`. \n","\n","<details>\n","<summary>Question - after reading the <a href=\"https://gymnasium.farama.org/environments/mujoco/hopper/\">documentation page</a>, can you see exactly what our 3 actions mean?</summary>\n","\n","They represent the **torque** applied between the three different links of the hopper. There is:\n","\n","* The **thigh rotor** (i.e. connecting the upper and middle parts of the leg),\n","* The **leg rotor** (i.e. connecting the middle and lower parts of the leg),\n","* The **foot rotor** (i.e. connecting the lower part of the leg to the foot).\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/hopper-torque.png\" width=\"400\">\n","\n","</details>\n","\n","How do we deal with a continuous action space, when it comes to choosing actions? Rather than our actor network's output being a vector of `logits` which we turn into a probability distribution via `Categorical(logits=logits)`, we instead have our actor output two vectors `mu` and `log_sigma`, which we turn into a normal distribution which is then sampled from.\n","\n","The observations take the form of a vector of 11 values describing the position, velocity, and forces applied to the joints. So unlike for Atari, we can't directly visualize the environment using its observations, instead we'll visualize it using `env.render()` which returns an array representing the environment state (thanks to the fact that we initialized the env with `render_mode=\"rgb_array\"`)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":18507,"status":"ok","timestamp":1704487245304,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"681amLLTJp8r","outputId":"e55f7837-8eb5-4979-af70-6f42a596bd3e"},"outputs":[],"source":["# TAGS: main\n","\n","nsteps = 150\n","\n","frames = []\n","obs, info = env.reset()\n","for _ in tqdm(range(nsteps)):\n","    action = env.action_space.sample()\n","    obs, reward, terminated, truncated, info = env.step(action)\n","    frames.append(env.render())  # frames can't come from obs, because unlike in Atari our observations aren't images\n","\n","display_frames(np.stack(frames))\n","\n","# FILTERS: ~\n","# save_display_frames(np.stack(frames), str(section_dir / \"2306.html\"), figsize=(12, 3))\n","# END FILTERS\n"]},{"cell_type":"markdown","metadata":{},"source":["FILTERS: soln,st\n","TAGS: html\n","\n","<div style=\"text-align: left\"><embed src=\"https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-23/2306.html\" width=\"500\" height=\"620\"></div>"]},{"cell_type":"markdown","metadata":{"id":"AW0hD2D9JyPT"},"source":["## Implementational details of MuJoCo"]},{"cell_type":"markdown","metadata":{"id":"vkhFK47XJzJx"},"source":["### Clipping, Scaling & Normalisation ([details #5-9](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Handling%20of%20action%20clipping%20to%20valid%20range%20and%20storage))\n","\n","Just like for Atari, there are a few messy implementational details which will be taken care of with gym wrappers. For example, if we generate our actions by sampling from a normal distribution, then there's some non-zero chance that our action will be outside of the allowed action space. We deal with this by clipping the actions to be within the allowed range (in this case between -1 and 1).\n","\n","See the function `prepare_mujoco_env` within `part3_ppo/utils` (and read details 5-9 on the PPO page) for more information."]},{"cell_type":"markdown","metadata":{"id":"w128a2vuJ12A"},"source":["### Actor and Critic networks ([details #1-4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Continuous%20actions%20via%20normal%20distributions))\n","\n","Our actor and critic networks are quite similar to the ones we used for cartpole. They won't have shared architecture.\n","\n","<details>\n","<summary>Question - can you see why it's less useful to have shared architecture in this case, relative to the case of Atari?</summary>\n","\n","The point of the shared architecture in Atari was that it allowed our critic and actor to perform **feature extraction**, i.e. the early part of the network (which was fed the raw pixel input) generated a high-level representation of the state, which was then fed into the actor and critic heads. But for CartPole and for MuJoCo, we have a very small observation space (4 discrete values in the case of CartPole, 11 for the Hopper in MuJoCo), so there's no feature extraction necessary.\n","\n","</details>\n","\n","The only difference will be in the actor network. There will be an `actor_mu` and `actor_log_sigma` network. The `actor_mu` will have exactly the same architecture as the CartPole actor network, and it will output a vector used as the mean of our normal distribution. The `actor_log_sigma` network will just be a bias, since the standard deviation is **state-independent** ([detail #2](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=State%2Dindependent%20log%20standard%20deviation)).\n","\n","Because of this extra complexity, we'll create a class for our actor and critic networks."]},{"cell_type":"markdown","metadata":{"id":"ymXiDbjKKWa7"},"source":["### Exercise - implement `Actor` and `Critic`\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","As discussed, the architecture of `actor_mu` is identical to your cartpole actor network, and the critic is identical. The only difference is the addition of `actor_log_sigma`, which you should initialize as an `nn.Parameter` object of shape `(1, num_actions)`.\n","\n","Your `Actor` class's forward function should return a tuple of `(mu, sigma, dist)`, where `mu` and `sigma` are the parameters of the normal distribution, and `dist` was created from these values using `torch.distributions.Normal`.\n","\n","<details>\n","<summary>Why do we use <code>log_sigma</code> rather than just outputting <code>sigma</code> ?</summary>\n","\n","We have our network output `log_sigma` rather than `sigma` because the standard deviation is always positive. If we learn the log standard deviation rather than the standard deviation, then we can treat it just like a regular learned weight.\n","</details>\n","\n","Tip - when creating your distribution, you can use the `broadcast_to` tensor method, so that your standard deviation and mean are the same shape.\n","\n","We've given you the function `get_actor_and_critic_mujoco` (which is called when you call `get_actor_and_critic` with `mode=\"mujoco\"`). All you need to do is fill in the `Actor` and `Critic` classes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1704487356834,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"zpC42URWKZAZ","outputId":"48b6262d-c4e5-4053-8183-c9d14da08475"},"outputs":[],"source":["class Critic(nn.Module):\n","    def __init__(self, num_obs):\n","        super().__init__()\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        self.critic = nn.Sequential(\n","            layer_init(nn.Linear(num_obs, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 1), std=1.0),\n","        )\n","        # SOLUTION END\n","\n","    def forward(self, obs) -> Tensor:\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        value = self.critic(obs)\n","        return value\n","        # SOLUTION END\n","\n","\n","class Actor(nn.Module):\n","    actor_mu: nn.Sequential\n","    actor_log_sigma: nn.Parameter\n","\n","    def __init__(self, num_obs, num_actions):\n","        super().__init__()\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        self.actor_mu = nn.Sequential(\n","            layer_init(nn.Linear(num_obs, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, num_actions), std=0.01),\n","        )\n","        self.actor_log_sigma = nn.Parameter(t.zeros(1, num_actions))\n","        # SOLUTION END\n","\n","    def forward(self, obs) -> tuple[Tensor, Tensor, t.distributions.Normal]:\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        mu = self.actor_mu(obs)\n","        sigma = t.exp(self.actor_log_sigma).broadcast_to(mu.shape)\n","        dist = t.distributions.Normal(mu, sigma)\n","        return mu, sigma, dist\n","        # SOLUTION END\n","\n","\n","# HIDE\n","def get_actor_and_critic_mujoco(num_obs: int, num_actions: int):\n","    \"\"\"\n","    Returns (actor, critic) in the \"classic-control\" case, according to description above.\n","    \"\"\"\n","    return Actor(num_obs, num_actions), Critic(num_obs)\n","\n","\n","if MAIN:\n","    tests.test_get_actor_and_critic(get_actor_and_critic, mode=\"mujoco\")"]},{"cell_type":"markdown","metadata":{"id":"sZZORgIpKeCR"},"source":["### Exercise - additional rewrites\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","You should spend up to 10-25 minutes on this exercise.\n","```\n","\n","There are a few more rewrites you'll need for continuous action spaces, which is why we recommend that you create a new solutions file for this part (like we've done with `solutions.py` and `solutions_cts.py`).\n","\n","You'll need to make the following changes:"]},{"cell_type":"markdown","metadata":{"id":"2bHcmORGKgKH"},"source":["#### Logprobs and entropy\n","\n","Rather than `probs = Categorical(logits=logits)` as your distribution (which you sample from & pass into your loss functions), you'll just use `dist` as your distribution. Methods like `.logprobs(action)` and `.entropy()` will work on `dist` just like they did on `probs`.\n","\n","Note that these two methods will return objects of shape `(batch_size, action_shape)` (e.g. for Hopper the last dimension will be 3). We treat the action components as independent ([detail #4](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Independent%20action%20components)), meaning **we take a product of the probabilities, so we sum the logprobs / entropies**. For example:\n","\n","$$\n","\\begin{aligned}\n","\\operatorname{prob}\\left(a_t\\right)&=\\operatorname{prob}\\left(a_t^1\\right) \\cdot \\operatorname{prob}\\left(a_t^2\\right) \\\\\n","\\log\\left(a_t\\right)&=\\log\\left(a_t^1\\right) + \\log\\left(a_t^2\\right)\n","\\end{aligned}\n","$$\n","\n","So you'll need to sum logprobs and entropy over the last dimension. The logprobs value that you add to the replay memory should be summed over (because you don't need the individual logprobs, you only need the logprob of the action as a whole).\n","\n","#### Logging\n","\n","You should log `mu` and `sigma` during the learning phase.\n","\n","Below, we've given you a template for all the things you'll need to change (with new class & function names so they don't overwrite the previous versions), however if you prefer you can just rewrite your previous classes & functions in a way indicated by the code we've given you below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmfkHUGo6eCY"},"outputs":[],"source":["class PPOAgentCts(PPOAgent):\n","    def play_step(self) -> list[dict]:\n","        \"\"\"\n","        CHANGES:\n","            - actor returns (mu, sigma, dist), with dist used to sample actions\n","            - logprobs need to be summed over action space\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        obs = self.next_obs\n","        terminated = self.next_terminated\n","\n","        with t.inference_mode():\n","            # CHANGED: actor returns (mu, sigma, dist), with dist used to sample actions\n","            mu, sigma, dist = self.actor.forward(obs)\n","        actions = dist.sample()\n","\n","        next_obs, rewards, next_terminated, next_truncated, infos = self.envs.step(actions.cpu().numpy())\n","\n","        # CHANGED: logprobs need to be summed over action space\n","        logprobs = dist.log_prob(actions).sum(-1).cpu().numpy()\n","        with t.inference_mode():\n","            values = self.critic(obs).flatten().cpu().numpy()\n","        self.memory.add(obs.cpu().numpy(), actions.cpu().numpy(), logprobs, values, rewards, terminated.cpu().numpy())\n","\n","        self.next_obs = t.from_numpy(next_obs).to(device, dtype=t.float)\n","        self.next_terminated = t.from_numpy(next_terminated).to(device, dtype=t.float)\n","\n","        self.step += self.envs.num_envs\n","        return infos\n","        # SOLUTION END\n","\n","\n","def calc_clipped_surrogate_objective_cts(\n","    dist: t.distributions.Normal,\n","    mb_action: Int[Tensor, \"minibatch_size *action_shape\"],\n","    mb_advantages: Float[Tensor, \"minibatch_size\"],\n","    mb_logprobs: Float[Tensor, \"minibatch_size\"],\n","    clip_coef: float,\n","    eps: float = 1e-8,\n",") -> Float[Tensor, \"\"]:\n","    \"\"\"\n","    CHANGES:\n","        - logprobs need to be summed over action space\n","    \"\"\"\n","    assert (mb_action.shape[0],) == mb_advantages.shape == mb_logprobs.shape\n","\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    # CHANGED: logprobs need to be summed over action space\n","    logits_diff = dist.log_prob(mb_action).sum(-1) - mb_logprobs\n","\n","    r_theta = t.exp(logits_diff)\n","\n","    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n","\n","    non_clipped = r_theta * mb_advantages\n","    clipped = t.clip(r_theta, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n","\n","    return t.minimum(non_clipped, clipped).mean()\n","    # SOLUTION END\n","\n","\n","def calc_entropy_bonus_cts(dist: t.distributions.Normal, ent_coef: float):\n","    \"\"\"\n","    CHANGES:\n","        - entropy needs to be summed over action space before taking mean\n","    \"\"\"\n","    # EXERCISE\n","    # raise NotImplementedError()\n","    # EXERCISE END\n","    # SOLUTION\n","    # CHANGED: sum over first dim before taking mean\n","    return ent_coef * dist.entropy().sum(-1).mean()\n","    # SOLUTION END\n","\n","\n","class PPOTrainerCts(PPOTrainer):\n","    def __init__(self, args: PPOArgs):\n","        super().__init__(args)\n","        self.agent = PPOAgentCts(self.envs, self.actor, self.critic, self.memory)\n","\n","    def compute_ppo_objective(self, minibatch: ReplayMinibatch) -> Float[Tensor, \"\"]:\n","        \"\"\"\n","        CHANGES:\n","            - actor returns (mu, sigma, dist), with dist used for loss functions\n","            - objective function calculated using new `_cts` functions defined above\n","            - newlogprob (for logging) needs to be summed over action space\n","            - mu and sigma should be logged\n","        \"\"\"\n","        # EXERCISE\n","        # raise NotImplementedError()\n","        # EXERCISE END\n","        # SOLUTION\n","        # CHANGED: actor returns (mu, sigma, dist), with dist used for loss functions\n","        mu, sigma, dist = self.agent.actor(minibatch.obs)\n","        values = self.agent.critic(minibatch.obs).squeeze()\n","\n","        # CHANGED: objective function calculated using new `_cts` functions defined above\n","        clipped_surrogate_objective = calc_clipped_surrogate_objective_cts(\n","            dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef\n","        )\n","        value_loss = calc_value_function_loss(values, minibatch.returns, self.args.vf_coef)\n","        entropy_bonus = calc_entropy_bonus_cts(dist, self.args.ent_coef)\n","        total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus\n","\n","        with t.inference_mode():\n","            # CHANGED: newlogprob (for logging) needs to be summed over action space\n","            newlogprob = dist.log_prob(minibatch.actions).sum(-1)\n","            logratio = newlogprob - minibatch.logprobs\n","            ratio = logratio.exp()\n","            approx_kl = (ratio - 1 - logratio).mean().item()\n","            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n","        if self.args.use_wandb:\n","            wandb.log(\n","                dict(\n","                    total_steps=self.agent.step,\n","                    values=values.mean().item(),\n","                    lr=self.scheduler.optimizer.param_groups[0][\"lr\"],\n","                    value_loss=value_loss.item(),\n","                    clipped_surrogate_objective=clipped_surrogate_objective.item(),\n","                    entropy=entropy_bonus.item(),\n","                    approx_kl=approx_kl,\n","                    clipfrac=np.mean(clipfracs),\n","                    # CHANGED: mu and sigma should be logged\n","                    mu=mu.mean().item(),\n","                    sigma=sigma.mean().item(),\n","                ),\n","                step=self.agent.step,\n","            )\n","\n","        return total_objective_function\n","        # SOLUTION END\n"]},{"cell_type":"markdown","metadata":{"id":"1grMAY8SKkz4"},"source":["## Training MuJoCo\n","\n","Now, you should be ready to run your training loop! We recommend using the following parameters, to match the original implmentation which the [37 Implementational Details](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details) post is based on (but you can experiment with different values if you like).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":626,"referenced_widgets":["099945c5356b4707bb06b0f7085a248f","6907653b745142f99af2529f09180d31","2166ad9a998f42468f829b6d301a6819","980291dcbb5d498eb2bf2fa37720f687","db55f8baf4bc4d5c98f8538fa502a4aa","f88bb3531acd4ad18dbfbe9457284a03","fb78f177d74e4f5882c2a8fb1bef62fd","6449de554fc3403c9fcd9a9236a1747c"]},"executionInfo":{"elapsed":1636620,"status":"ok","timestamp":1704489056324,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":0},"id":"AZHinEl9KmGE","outputId":"29845af4-02eb-4ca7-d1f1-f97aa53d872c"},"outputs":[],"source":["# TAGS: main\n","\n","args = PPOArgs(\n","    env_id=\"Hopper-v4\",\n","    wandb_project_name=\"PPOMuJoCo\",\n","    use_wandb=True,\n","    mode=\"mujoco\",\n","    lr=3e-4,\n","    ent_coef=0.0,\n","    num_minibatches=32,\n","    num_steps_per_rollout=2048,\n","    num_envs=1,\n","    video_log_freq=75,\n",")\n","trainer = PPOTrainerCts(args)\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"XFqc7K1PKnKY"},"source":["You should expect the reward to increase pretty fast initially and then plateau once the agent learns the solution \"kick off for a very large initial jump, and don't think about landing\". Eventually the agent gets past this plateau, and learns to land successfully without immediately falling over. Once it's at the point where it can string two jumps together, your reward should start increasing much faster.\n","\n","Here is a video produced from a successful run, using the parameters above:\n","\n","<video width=\"400\" height=\"420\" controls>\n","<source src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/media-23/2305.mp4\" type=\"video/mp4\">\n","</video>\n","\n","and here's the corresponding plot of episode lengths:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/wandb-mujoco-lengths.png\" width=\"550\">\n","\n","Although we've used `Hopper-v4` in these examples, you might also want to try `InvertedPendulum-v4` (docs [here](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/)). It's a much easier environment to solve, and it's a good way to check that your implementation is working (after all if it worked for CartPole then it should work here - in fact your inverted pendulum agent should converge to a perfect solution almost instantly, no reward shaping required). You can check out the other MuJoCo environments [here](https://gymnasium.farama.org/environments/mujoco/)."]},{"cell_type":"markdown","metadata":{"id":"67auFWjvKoZp"},"source":["# ☆ Bonus"]},{"cell_type":"markdown","metadata":{"id":"qnIsjHUQKp_E"},"source":["## Trust Region Methods\n","\n","Some versions of the PPO algorithm use a slightly different objective function. Rather than our clipped surrogate objective, they use constrained optimization (maximising the surrogate objective subject to a restriction on the [KL divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence) between the old and new policies).\n","\n","$$\n","\\begin{array}{ll}\n","\\underset{\\theta}{\\operatorname{maximize}} & \\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old}}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right] \\\\\n","\\text { subject to } & \\hat{\\mathbb{E}}_t\\left[\\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old}}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right] \\leq \\delta\n","\\end{array}\n","$$\n","\n","The intuition behind this is similar to the clipped surrogate objective. For our clipped objective, we made sure the model wasn't rewarded for deviating from its old policy beyond a certain point (which encourages small updates). Adding an explicit KL constraint accomplishes something similar, because it forces the model to closely adhere to the old policy. For more on KL-divergence and why it's a principled measure, see [this post](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence). We call these algorithms trust-region methods because they incentivise the model to stay in a **trusted region of policy space**, i.e. close to the old policy (where we can be more confident in our results).\n","\n","The theory behind TRPO actually suggests the following variant - turning the strict constraint into a penalty term, which you should find easier to implement:\n","\n","$$\n","\\underset{\\theta}{\\operatorname{maximize}} \\, \\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old}}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\mathrm{KL}\\left[\\pi_{\\theta_{\\text {old}}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right]\n","$$\n","\n","Rather than forcing the new policy to stay close to the previous policy, this adds a penalty term which incentivises this behaviour (in fact, there is a 1-1 correspondence between constrained optimization problems and the corresponding unconstrained version).\n","\n","Can you implement this? Does this approach work better than the clipped surrogate objective? What values of $\\beta$ work best?\n","\n","Tip - you can calculate KL divergence using the PyTorch [KL Divergence function](https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl). You could also try the approximate version, as described in [detail #12](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#:~:text=Debug%20variables) of the \"37 Implementational Details\" post."]},{"cell_type":"markdown","metadata":{"id":"JLEtVb7dKrkX"},"source":["## Long-term replay memory\n","\n","Above, we discussed the problem of **catastrophic forgetting** (where the agent forgets how to recover from bad behaviour, because the memory only contains good behaviour). One way to fix this is to have a long-term replay memory, for instance:\n","\n","* (simple version) You reserve e.g. 10% of your buffer for experiences generated at the start of training.\n","* (complex version) You design a custom scheduled method for removing experiences from memory, so that you always have a mix of old and new experiences.\n","\n","Can you implement one of these, and does it fix the catastrophic forgetting problem (without needing to use reward shaping)?"]},{"cell_type":"markdown","metadata":{"id":"JfyG57GsKswF"},"source":["## Vectorized Advantage Calculation\n","\n","Try optimizing away the for-loop in your advantage calculation. It's tricky (and quite messy), so an easier version of this is: find a vectorized calculation and try to explain what it does.\n","\n","<details>\n","<summary>Hint (for your own implementation)</summary>\n","\n","*(Assume `num_envs=1` for simplicity)*\n","\n","Construct a 2D boolean array from `dones`, where the `(i, j)`-th element of the array tells you whether the expression for the `i`-th advantage function should include rewards / values at timestep `j`. You can do this via careful use of `torch.cumsum`, `torch.triu`, and some rearranging.\n","</details>\n","\n","There are solutions available in `solutions.py` (commented out)."]},{"cell_type":"markdown","metadata":{"id":"MqoS9Cv-Kuzt"},"source":["## Other Discrete Environments\n","\n","Two environments (supported by gym) which you might like to try are:\n","\n","* [`Acrobot-v1`](https://www.gymlibrary.dev/environments/classic_control/acrobot/) - this is one of the [Classic Control environments](https://www.gymlibrary.dev/environments/classic_control/), and it's a bit harder to learn than cartpole.\n","* [`MountainCar-v0`](https://www.gymlibrary.dev/environments/classic_control/mountain_car/) - this is one of the [Classic Control environments](https://www.gymlibrary.dev/environments/classic_control/), and it's much harder to learn than cartpole. This is primarily because of **sparse rewards** (it's really hard to get to the top of the hill), so you'll definitely need reward shaping to get through it!\n","* [`LunarLander-v2`](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) - this is part of the [Box2d](https://www.gymlibrary.dev/environments/box2d/) environments. It's a bit harder still, because of the added environmental complexity (physics like gravity and friction, and constraints like fuel conservatino). The reward is denser (with the agent receiving rewards for moving towards the landing pad and penalties for moving away or crashing), but the increased complexity makes it overall a harder problem to solve. You might have to perform hyperparameter sweeps to find the best implementation (you can go back and look at the syntax for hyperparameter sweeps [here](https://arena-ch0-fundamentals.streamlit.app/[0.4]_Optimization)). Also, [this page](https://pylessons.com/LunarLander-v2-PPO) might be a useful reference (although the details of their implementation differs from the one we used today). You can look at the hyperparameters they used."]},{"cell_type":"markdown","metadata":{"id":"PGlTq7YPKwD7"},"source":["## Continuous Action Spaces & Reward Shaping\n","\n","The `MountainCar-v0` environment has discrete actions, but there's also a version `MountainCarContinuous-v0` with continuous action spaces. Implementing this will require a combination of the continuous action spaces you dealt with during the MuJoCo section, and the reward shaping you used during the CartPole exercises."]},{"cell_type":"markdown","metadata":{"id":"Llni5H1CLLNb"},"source":["## Choose & build your own environment (e.g. Wordle)\n","\n","You can also try choosing your own task, framing it as an RL problem, and adapting your PPO algorithm to solve it. For example, training an agent to play Wordle (or a relation like Semantle) might be a suitably difficult task. [This post](https://wandb.ai/andrewkho/wordle-solver/reports/Solving-Wordle-with-Reinforcement-Learning--VmlldzoxNTUzOTc4) gives a high level account of training an agent to play Wordle - they use DQN, but they don't go too deep into the technical details (and it's possible that PPO would work better for this task)."]},{"cell_type":"markdown","metadata":{"id":"0GrN-mO6Kw9g"},"source":["## Minigrid envs / Procgen\n","\n","There are many more exciting environments to play in, but generally they're going to require more compute and more optimization than we have time for today. If you want to try them out, some we recommend are:\n","\n","- [Minimalistic Gridworld Environments](https://github.com/Farama-Foundation/gym-minigrid) - a fast gridworld environment for experiments with sparse rewards and natural language instruction.\n","- [microRTS](https://github.com/santiontanon/microrts) - a small real-time strategy game suitable for experimentation.\n","- [Megastep](https://andyljones.com/megastep/) - RL environment that runs fully on the GPU (fast!)\n","- [Procgen](https://github.com/openai/procgen) - A family of 16 procedurally generated gym environments to measure the ability for an agent to generalize. Optimized to run quickly on the CPU.\n","    - For this one, you might want to read [Jacob Hilton's online DL tutorial](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/6-Reinforcement-Learning.md) (the RL chapter suggests implementing PPO on Procgen), and [Connor Kissane's solutions](https://github.com/ckkissane/deep_learning_curriculum/blob/master/solutions/6_Reinforcement_Learning.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"BKAy8P76KyEF"},"source":["## Multi-Agent PPO\n","\n","Multi-Agent PPO (MAPPO) is an extension of the standard PPO algorithm which trains multiple agents at once. It was first described in the paper [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955). Can you implement MAPPO?"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jd3LpCav3UXu","XcgAnZZOyBYk","Wrc4voZ6JWQc","PCrDNis6JWQj","RPl7PxPyJWQm","B6WVI0yOIovS","_8KQkx92JfFE","67auFWjvKoZp"],"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1f_RBuosHddwQrydZ7-iAnBs6lhkMMMSG","timestamp":1704215013474}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03bb038eb947454f8318fe9f68d7dee5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aa320f239e643b589ea4830ce9a4eb9","placeholder":"​","style":"IPY_MODEL_4004d03ebf0542debf6eb8b46df5c70a","value":"0.555 MB of 0.565 MB uploaded (0.000 MB deduped)\r"}},"099945c5356b4707bb06b0f7085a248f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6907653b745142f99af2529f09180d31","IPY_MODEL_2166ad9a998f42468f829b6d301a6819"],"layout":"IPY_MODEL_980291dcbb5d498eb2bf2fa37720f687"}},"0aa320f239e643b589ea4830ce9a4eb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1873366fc48249eda00aae161adb0ef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5c4926c66c4aea9961a95f514f8b25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_648817b2cf1a444fb0714b36eae3a236","placeholder":"​","style":"IPY_MODEL_92632d8b197a4f11a638caddca91439b","value":"4.115 MB of 4.125 MB uploaded (0.000 MB deduped)\r"}},"2166ad9a998f42468f829b6d301a6819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb78f177d74e4f5882c2a8fb1bef62fd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6449de554fc3403c9fcd9a9236a1747c","value":0.9997088834660515}},"263ce0ad652749a7914b33fa8a4a6d34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4004d03ebf0542debf6eb8b46df5c70a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d302b7ce65b46ee82fd9a974dfd62df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdfabc617f3f44629458a28b6b6ea817","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8b1ba53d9a74d509cb5a3e22b9bbab8","value":1}},"56e27afd07e8418d9080da521c2db4e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1873366fc48249eda00aae161adb0ef9","placeholder":"​","style":"IPY_MODEL_a5544061299a4616856b2aab578137af","value":"0.656 MB of 0.656 MB uploaded (0.000 MB deduped)\r"}},"5f0039c9da594b50980cab9406be467e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6449de554fc3403c9fcd9a9236a1747c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"648817b2cf1a444fb0714b36eae3a236":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6907653b745142f99af2529f09180d31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db55f8baf4bc4d5c98f8538fa502a4aa","placeholder":"​","style":"IPY_MODEL_f88bb3531acd4ad18dbfbe9457284a03","value":"34.944 MB of 34.954 MB uploaded (0.000 MB deduped)\r"}},"78d082b21b5d4bbb978d51739b0a5512":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79b65cec2a3346a6b3b8a845dc8c4d8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_03bb038eb947454f8318fe9f68d7dee5","IPY_MODEL_bd7b53091f464a3da46a58d322d9f2f6"],"layout":"IPY_MODEL_78d082b21b5d4bbb978d51739b0a5512"}},"7b55d4cc46bc4bf3aa4d4c29b393e087":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_1a5c4926c66c4aea9961a95f514f8b25","IPY_MODEL_aa2fcc88da544199b94175576ead01a0"],"layout":"IPY_MODEL_263ce0ad652749a7914b33fa8a4a6d34"}},"7cecde6e8a594bccaafbbb12372c6311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87447a318ec04900b71d8116a659041c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92632d8b197a4f11a638caddca91439b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"980291dcbb5d498eb2bf2fa37720f687":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5544061299a4616856b2aab578137af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa2fcc88da544199b94175576ead01a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f0039c9da594b50980cab9406be467e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cecde6e8a594bccaafbbb12372c6311","value":0.9975051050331825}},"b701a8deb51d40eb98bbf8ba4146d23f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7b53091f464a3da46a58d322d9f2f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b6bb11025d4d8d9af9de89a7d4359c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87447a318ec04900b71d8116a659041c","value":0.9824425959212358}},"c6f9ad7c1539408d980e3e9663fc4efd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_56e27afd07e8418d9080da521c2db4e7","IPY_MODEL_4d302b7ce65b46ee82fd9a974dfd62df"],"layout":"IPY_MODEL_b701a8deb51d40eb98bbf8ba4146d23f"}},"d8b1ba53d9a74d509cb5a3e22b9bbab8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db55f8baf4bc4d5c98f8538fa502a4aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b6bb11025d4d8d9af9de89a7d4359c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f88bb3531acd4ad18dbfbe9457284a03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb78f177d74e4f5882c2a8fb1bef62fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdfabc617f3f44629458a28b6b6ea817":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
