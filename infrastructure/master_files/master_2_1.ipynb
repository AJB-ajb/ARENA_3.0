{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[\n",
    "    {\"title\": \"Multi-Armed Bandit\", \"icon\": \"1-circle-fill\", \"subtitle\" : \"(40%)\"},\n",
    "    {\"title\": \"Tabular RL & Policy Improvement\", \"icon\": \"2-circle-fill\", \"subtitle\" : \"(60%)\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2.1] - Intro to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab: [exercises](https://www.youtube.com/watch?v=dQw4w9WgXcQ) | [solutions](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
    "\n",
    "ARENA [Streamlit Page](https://arena3-chapter2-rl.streamlit.app/[2.1]_Intro_to_RL)\n",
    "\n",
    "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-2noug8mpy-TRYbCnc3pzj7ITNrZIjKww), and ask any questions on the dedicated channels for this chapter of material/\n",
    "\n",
    "If you want to change to dark mode, you can do this by clicking the three horizontal lines in the top-right, then navigating to Settings → Theme.\n",
    "\n",
    "Links to other chapters: [(0) Fundamentals](https://arena3-chapter0-fundamentals.streamlit.app/), [(1) Transformers & Mech Interp](https://arena3-chapter1-transformer-interp.streamlit.app/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/bandit.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This section is designed to bring you up to speed with the basics of reinforcement learning. Before we cover the big topics like PPO and RLHF, we need to build a strong foundation by understanding what RL is and what kinds of problems it was designed to solve.\n",
    "\n",
    "In today's exercises, we'll make two important assumptions about our environment: the space of possible actions and states is discrete, and the entire environment is obvservable (in other words, the probability distribution of the outcome of every possible action at every possible state is known). As we move on in the chapter, we'll relax these assumptions, and so we'll need to start using more complex methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content & Learning Objectives\n",
    "\n",
    "### 1️⃣ Multi-Armed Bandit\n",
    "\n",
    "In Part 1 we'll study the multi-armed bandit problem, which is simple yet captures introduces many of the difficulties in RL. Many practical problems can be posed in terms of generalizations of the multi-armed bandit. For example, the Hyperband algorithm for hyperparameter optimization is based on the multi-armed bandit with an infinite number of arms.\n",
    "\n",
    "> ##### Learning Objectives\n",
    "> * Understand the anatomy of a `gym.Env`, so that you feel comfortable using them and writing your own\n",
    "> * Practice RL in the tabular (lookup table) setting before adding the complications of neural networks\n",
    "> * Understand the difficulty of optimal exploration\n",
    "> * Understand that performance is extremely variable in RL, and how to measure performance\n",
    "\n",
    "### 2️⃣ Tabular RL & Policy Improvement\n",
    "> ##### Learning Objectives\n",
    "> * Understand the tabular RL problem, for known environments.\n",
    "> * Learn how to numerically evaluate a given policy (via an iterative update formula).\n",
    "> *Understand the policy improvement theorem, and understand what circumstances can allow us to directly solve for the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERS: colab\n",
    "# TAGS: master-comment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from importlib.metadata import distributions\n",
    "from pathlib import Path\n",
    "\n",
    "# FILTERS: ~\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "# END FILTERS\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "chapter = \"chapter2_rl\"\n",
    "repo = \"ARENA_3.0\"\n",
    "branch = \"master_file\"\n",
    "\n",
    "# Install dependencies\n",
    "if \"jaxtyping\" not in [dist.metadata[\"Name\"] for dist in distributions()]:\n",
    "    %pip install einops gym==0.23.1 jaxtyping\n",
    "\n",
    "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
    "root = (\n",
    "    \"/content\"\n",
    "    if IN_COLAB\n",
    "    else \"/root\"\n",
    "    if repo not in os.getcwd()\n",
    "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
    ")\n",
    "\n",
    "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
    "    if not IN_COLAB:\n",
    "        !sudo apt-get install unzip\n",
    "        %pip install jupyter ipython --upgrade\n",
    "\n",
    "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
    "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
    "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
    "        !rm {root}/{branch}.zip\n",
    "        !rmdir {root}/{repo}-{branch}\n",
    "\n",
    "\n",
    "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
    "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
    "\n",
    "os.chdir(f\"{root}/{chapter}/exercises\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "\n",
    "Arr = np.ndarray\n",
    "max_episode_steps = 1000\n",
    "N_RUNS = 200\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter2_rl\"\n",
    "section = \"part1_intro_to_rl\"\n",
    "exercises_dir = next(p for p in Path.cwd().parents if p.name == chapter) / \"exercises\"\n",
    "section_dir = exercises_dir / section\n",
    "# FILTERS: ~colab\n",
    "if str(exercises_dir) not in sys.path:\n",
    "    sys.path.append(str(exercises_dir))\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "# END FILTERS\n",
    "\n",
    "import part1_intro_to_rl.utils as utils\n",
    "import part1_intro_to_rl.tests as tests\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "* [Sutton and Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf), Chapter 2 (25 minutes)\n",
    "  - Section 2.1: A k-armed Bandit Problem through to Section 2.7 Upper-Confidence-Bound Action Section\n",
    "  - We won't cover Gradient Bandits. Don't worry about these.\n",
    "  - Don't worry too much about all the math for the moment if you can't follow it. The earlier sections are more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to OpenAI Gym\n",
    "\n",
    "Today and for the rest of this week, we'll be using OpenAI Gym, which provides a uniform interface to many different RL environments including Atari games. Gym was released in 2016 and details of the API have changed significantly over the years. We are using version 0.23.1, so ensure that any documentation you use refers to the same version.\n",
    "\n",
    "Below, we've provided a simple environment for the multi-armed bandit described in the Sutton and Barto reading. Here, an action is an integer indicating the choice of arm to pull, and an observation is the constant integer 0, since there's nothing to observe here. Even though the agent does in some sense observe the reward, the reward is always a separate variable from the observation.\n",
    "\n",
    "We're using NumPy for this section. PyTorch provides GPU support and autograd, neither of which is of any use to environment code or the code we're writing today.\n",
    "\n",
    "Read the `MultiArmedBandit` code carefully and make sure you understand how the Gym environment API works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `info` dictionary\n",
    "\n",
    "The environment's `step` method returns four values: `obs`, `reward`, `done`, and the `info` dictionary.\n",
    "\n",
    "`info` can contain anything extra that doesn't fit into the uniform interface - it's up to the environment what to put into it. A good use of this is for debugging information that the agent isn't \"supposed\" to see. In this case, we'll return the index of the actual best arm. This would allow us to measure how often the agent chooses the best arm, but it would also allow us to build a \"cheating\" agent that peeks at this information to make its decision.\n",
    "\n",
    "Cheating agents are helpful because we know that they should obtain the maximum possible rewards; if they aren't, then there's a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `render()` method\n",
    "\n",
    "Render is only used for debugging or entertainment, and what it does is up to the environment. It might render a little popup window showing the Atari game, or it might give you a RGB image tensor, or just some ASCII text describing what's happening. In this case, we'll just make a little plot showing the distribution of rewards for each arm of the bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Action Types\n",
    "\n",
    "A `gym.Env` is a generic type: both the type of the observations and the type of the actions depends on the specifics of the environment.\n",
    "\n",
    "We're only dealing with the simplest case: a discrete set of actions which are the same in every state (and ditto for observations). In general, the actions could be continuous, or depend on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    '''\n",
    "    A class representing a multi-armed bandit environment, based on OpenAI Gym's Env class.\n",
    "\n",
    "    Attributes:\n",
    "        action_space (gym.spaces.Discrete): The space of possible actions, representing the arms of the bandit.\n",
    "        observation_space (gym.spaces.Discrete): The space of possible observations.\n",
    "        num_arms (int): The number of arms in the bandit.\n",
    "        stationary (bool): Indicates whether the reward distribution (i.e. the arm_reward_means) is stationary or not.\n",
    "        arm_reward_means (np.ndarray): The mean rewards for each arm.\n",
    "    '''\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        '''\n",
    "        Initializes the MultiArmedBandit environment.\n",
    "\n",
    "        Args:\n",
    "            num_arms (int): The number of arms for the bandit. Defaults to 10.\n",
    "            stationary (bool): Whether the bandit has a stationary reward distribution. Defaults to True.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Takes an action by choosing an arm and returns the result of the action.\n",
    "\n",
    "        Args:\n",
    "            arm (ActType): The selected arm to pull in the bandit.\n",
    "\n",
    "        Returns:\n",
    "            tuple[ObsType, float, bool, dict]: A tuple containing the observation, reward, done flag, and additional info.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, info)\n",
    "\n",
    "    def reset(self, seed: int | None=None, options=None) -> ObsType:\n",
    "        '''\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (int | None): The seed for random number generation. Defaults to None.\n",
    "            return_info (bool): If True, return additional info. Defaults to False.\n",
    "            options (dict): Additional options for environment reset. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ObsType: The initial observation.\n",
    "        '''\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        '''\n",
    "        Renders the state of the environment, in the form of a violin plot.\n",
    "        '''\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering an Environment\n",
    "\n",
    "User code normally won't use the constructor of an `Env` directly for two reasons:\n",
    "\n",
    "- Usually, we want to wrap our `Env` in one or more wrapper classes.\n",
    "- If we want to test our agent on a variety of environments, it's annoying to have to import all the `Env` classes directly.\n",
    "\n",
    "The `register` function stores information about our `Env` in a registry so that a later call to `gym.make` can look it up using the `id` string that is passed in.\n",
    "\n",
    "By convention, the `id` strings have a suffix with a version number. There can be multiple versions of the \"same\" environment with different parameters, and benchmarks should always report the version number for a fair comparison. For instance, `id=\"ArmedBanditTestbed-v0\"` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeLimit Wrapper\n",
    "\n",
    "As defined, our environment never terminates; the `done` flag is always False so the agent would keep playing forever. By setting `max_episode_steps` here, we cause our env to be wrapped in a `TimeLimit` wrapper class which terminates the episode after that number of steps. This works by having the `done` object returned by the `step` function be overridden after `max_episode_steps`, and set to `True`.\n",
    "\n",
    "The time limit is also an essential part of the problem definition: if it were larger or shorter, there would be more or less time to explore, which means that different algorithms (or at least different hyperparameters) would then have improved performance.\n",
    "\n",
    "*Note - the `gym` library is well known for being pretty janky and having annoying errors and warnings! You should generally ignore these warnings unless they're causing you problems e.g. you're failing tests.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our env inside its wrappers looks like: <TimeLimit<OrderEnforcing<MultiArmedBandit<ArmedBanditTestbed-v0>>>>\n"
     ]
    }
   ],
   "source": [
    "if MAIN:\n",
    "    gym.envs.registration.register(\n",
    "        id=\"ArmedBanditTestbed-v0\",\n",
    "        entry_point=MultiArmedBandit,\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        nondeterministic=True,\n",
    "        reward_threshold=1.0,\n",
    "        kwargs={\"num_arms\": 10, \"stationary\": True},\n",
    "    )\n",
    "\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\")\n",
    "    print(f\"Our env inside its wrappers looks like: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on (pseudo) RNGs\n",
    "\n",
    "The PRNG that `gym.Env` provides as `self.np_random` is from the [PCG family](https://www.pcg-random.org/index.html). In RL code, you often need massive quantities of pseudorandomly generated numbers, so it's important to have a generator that is both very fast and has good quality output.\n",
    "\n",
    "When you call `np.random.randint` or similar, you're using the old-school Mersenne Twister algorithm which is both slower and has inferior quality output to PCG. Since Numpy 1.17, you can use `np.random.default_rng()` to get a PCG generator and then use its `integers` method to get random integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now, you'll start defining agents to act within this class.\n",
    "\n",
    "You'll be defining your agents as subclasses of the base class `Agent` for agents in a multi-armed bandit environment, given below. It contains the following methods:\n",
    "\n",
    "* `get_action`, which returns the agent's next action. This doesn't take state as an argument, because the state of the multi armed bandit game is always the same.\n",
    "    * This isn't implemented for `Agent`, because we'll implement it separately for each agent we build on top of the base agent.\n",
    "* `observe`, which determines what the model does when it observes a particular action & reward combination.\n",
    "    * The default behaviour is to do nothing, although for some agents we'll implement more complicated behaviour.\n",
    "* `reset`, which is run before each episode starts.\n",
    "    * All that the base method does is reset the random number generator, but again future agents might need to do more, e.g. reset data gathered from the `observe` method.\n",
    "\n",
    "Additionally, the `run_episode` function will run a single episode of interaction between an agent and the environment. As discussed above, this will terminate after `max_episode_steps` for our environment (we've set this to be 1000). The `run_agent` function calls `run_episode` multiple times in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Base class for agents in a multi-armed bandit environment\n",
    "\n",
    "    (you do not need to add any implementation here)\n",
    "    '''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        self.num_arms = num_arms\n",
    "        self.reset(seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, action: ActType, reward: float, info: dict) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed: int):\n",
    "    '''\n",
    "    Runs a single episode of interaction between an agent and an environment.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment in which the agent operates.\n",
    "        agent (Agent): The agent that takes actions in the environment.\n",
    "        seed (int): The seed for random number generation to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: A tuple containing arrays of rewards\n",
    "        received in each step and a flag indicating if the chosen arm was the best.\n",
    "    '''\n",
    "    (rewards, was_best) = ([], [])\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    agent.reset(seed=seed)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        arm = agent.get_action()\n",
    "        (obs, reward, done, info) = env.step(arm)\n",
    "        agent.observe(arm, reward, info)\n",
    "        rewards.append(reward)\n",
    "        was_best.append(1 if arm == info[\"best_arm\"] else 0)\n",
    "\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    was_best = np.array(was_best, dtype=int)\n",
    "    return (rewards, was_best)\n",
    "\n",
    "\n",
    "def run_agent(env: gym.Env, agent: Agent, n_runs=200, base_seed=1):\n",
    "    all_rewards = []\n",
    "    all_was_bests = []\n",
    "    base_rng = np.random.default_rng(base_seed)\n",
    "    for n in tqdm(range(n_runs)):\n",
    "        seed = base_rng.integers(low=0, high=10_000, size=1).item()\n",
    "        (rewards, corrects) = run_episode(env, agent, seed)\n",
    "        all_rewards.append(rewards)\n",
    "        all_was_bests.append(corrects)\n",
    "    return (np.array(all_rewards), np.array(all_was_bests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement `RandomAgent`\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 10-20 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should fill in the `get_action` method from `RandomAgent`, which is a subclass of `Agent`. This agent should pick an arm at random, i.e. from the range `[0, ..., num_arms-1]`, and return it as an integer. You won't need to change the `observe` method since the random agent doesn't change its behaviour based on observations, and you won't need to change the `reset` method since there are no observations which need to be reset.\n",
    "\n",
    "This agent will be useful as a baseline to ensure the environment has no bugs. If your later agents are doing worse than random, you have a bug! Later, we'll add some smarter agents.\n",
    "\n",
    "When you've finished implementing this agent, running the cell below will:\n",
    "        \n",
    "- Verify that `RandomAgent` pulls the optimal arm with frequency roughly `1/num_arms`.\n",
    "- Verify that the average reward is very roughly zero. This is the case since the mean reward for each arm is centered on zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 142.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected correct freq: 0.1, actual: 0.099565\n",
      "Expected average reward: 0.0, actual: 0.006279\n",
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent(Agent):\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the get_action method for the RandomAgent class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        return self.rng.integers(low=0, high=self.num_arms)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"RandomAgent\"\n",
    "\n",
    "# HIDE\n",
    "if MAIN:\n",
    "    num_arms = 10\n",
    "    stationary = True\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "    agent = RandomAgent(num_arms, 0)\n",
    "    all_rewards, all_corrects = run_agent(env, agent)\n",
    "\n",
    "    print(f\"Expected correct freq: {1/10}, actual: {all_corrects.mean():.6f}\")\n",
    "    assert np.isclose(all_corrects.mean(), 1/10, atol=0.05), \"Random agent is not random enough!\"\n",
    "\n",
    "    print(f\"Expected average reward: 0.0, actual: {all_rewards.mean():.6f}\")\n",
    "    assert np.isclose(all_rewards.mean(), 0, atol=0.05), \"Random agent should be getting mean arm reward, which is zero.\"\n",
    "\n",
    "    print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement reward averaging\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 20-30 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should now complete the methods for the `RewardAveraging` agent, which applies the reward averaging algorithm as detailed in Sutton and Barto section 2.4, \"Incremental Implementation\":\n",
    "\n",
    "* You should fill in `observe` to keep track of the number of times $n$ each arm has been pushed, as well as the value $Q_n$ for each arm.\n",
    "    * Gotcha - in S & B notation, $n$ is the number of times this particular arm has been pulled, not the total number of actions taken!\n",
    "    * We recommend defining arrays `N` and `Q`, each of length `num_arms`, to keep track of all these values.\n",
    "* You should fill in `get_action` with an epsilon-greedy method: taking a random action with probability `epsilon`, and taking the best action based on the current value of $Q$ with probability `1-epsilon` (see Sutton & Barto).\n",
    "* You should fill in `reset` to call the reset method from the parent class, *and* make sure that the tracked values of $(n, Q_n)$ are set back to appropriate values at the start of each episode.\n",
    "    * Note, the `reset` method is also called before the very first run, so you don't need to define `N` and `Q` in the init method.\n",
    "    * The `Q` values should be initialized according to the optimism value of this agent.\n",
    "    * Ensure the `Q` values are stored as float (and not numpy's default, integer)\n",
    "\n",
    "<details>\n",
    "<summary>Hint - average reward formula</summary>\n",
    "\n",
    "$$Q_k = Q_{k-1} + \\frac{1}{k}[R_k - Q_{k-1}]$$\n",
    "\n",
    "Where $k$ is the number of times the action has been taken, $R_k$ is the reward from the kth time the action was taken, and $Q_{k-1}$ is the average reward from the previous times this action was taken (this notation departs slightly from the S&B notation, but may be more helpful for our implementation).\n",
    "\n",
    "**Important - $k$ is not the total number of timesteps, it's the total number of times you've taken this particular action.**\n",
    "</details>\n",
    "\n",
    "We've given you a function for plotting multiple agents' reward trajectories on the same graph, with an optional moving average parameter to make the graph smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardAveraging(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int, epsilon: float, optimism: float):\n",
    "        self.epsilon = epsilon\n",
    "        self.optimism = optimism\n",
    "        super().__init__(num_arms, seed)\n",
    "\n",
    "    def get_action(self):\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the get_action method for the RewardAveraging class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.integers(low=0, high=self.num_arms).item()\n",
    "        else:\n",
    "            return np.argmax(self.Q)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def observe(self, action, reward, info):\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the observe method for the RewardAveraging class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
    "        # END SOLUTION\n",
    "\n",
    "    def reset(self, seed: int):\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the reset method for the RewardAveraging class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        super().reset(seed)\n",
    "        self.N = np.zeros(self.num_arms)\n",
    "        self.Q = np.full(self.num_arms, self.optimism, dtype=float)\n",
    "        # END SOLUTION\n",
    "\n",
    "    def __repr__(self):\n",
    "        # For the legend, when plotting\n",
    "        return f\"RewardAveraging(eps={self.epsilon}, optimism={self.optimism})\"\n",
    "\n",
    "# HIDE\n",
    "if MAIN:\n",
    "    num_arms = 10\n",
    "    stationary = True\n",
    "    names = []\n",
    "    all_rewards = []\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "\n",
    "    for optimism in [0, 5]:\n",
    "        agent = RewardAveraging(num_arms, 0, epsilon=0.01, optimism=optimism)\n",
    "        (rewards, num_correct) = run_agent(env, agent, n_runs=N_RUNS, base_seed=1)\n",
    "        all_rewards.append(rewards)\n",
    "        names.append(str(agent))\n",
    "        print(agent)\n",
    "        print(f\" -> Frequency of correct arm: {num_correct.mean():.4f}\")\n",
    "        print(f\" -> Average reward: {rewards.mean():.4f}\")\n",
    "    utils.plot_rewards(all_rewards, names, moving_avg_window=15)\n",
    "    # FILTERS: ~\n",
    "    #fig = utils.plot_rewards_master(all_rewards, names, moving_avg_window=15)\n",
    "    #fig.show()\n",
    "    #fig.write_html(str(section_dir / \"0201.html\"))\n",
    "    # END FILTERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Question - can you interpret these results?</summary>\n",
    "\n",
    "At the very start, the more optimistic agent performs worse, because it explores more and exploits less (here you will see it at the very beginning if you zoom in). Its estimates are wildly over-optimistic, so even if it finds a good arm, its Q-value for that arm will decrease. On the other hand, if the realistic agent finds a good arm early on, it'll probably return to exploit it.\n",
    "\n",
    "However, the optimistic agent eventually outperforms the realistic agent, because its increased exploration means it's more likely to converge on the best arm.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Question - how do you think these results would change if epsilon was decreased for both agents?</summary>\n",
    "\n",
    "You should expect the optimistic agent to outperform the realistic agent even more. The smaller epsilon is, the more necessary optimism is (because without it the agent won't explore enough).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERS: soln,st\n",
    "TAGS: html,st-dropdown[Click to see the expected output]\n",
    "\n",
    "<iframe src = \"https://info-arena.github.io/ARENA_img/media/ch2_fig1.html\", width=\"680\", height=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement cheater agent\n",
    "\n",
    "```yaml\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 5-15 minutes on this exercise.\n",
    "\n",
    "It's important for you to understand why cheater agents are important for debugging.\n",
    "```\n",
    "\n",
    "The cheater agent will always choose the best arm. It's a good idea to implement, because you can compare your other agents to it to make sure they're not doing better than the cheater (if they are, you probably have a bug!).\n",
    "\n",
    "You should fill in the methods `get_action` and `observe` below. The cheater agent will always choose the best arm available to it (remember that the best arm is stored in the `info` dictionary of each observation - see the `step` method in the `MultiArmedBandit` class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheatyMcCheater(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        super().__init__(num_arms, seed)\n",
    "        self.best_arm = 0\n",
    "\n",
    "    def get_action(self):\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the get_action method for the CheatyMcCheater class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        return self.best_arm\n",
    "        # END SOLUTION\n",
    "\n",
    "    def observe(self, action: int, reward: float, info: dict):\n",
    "        # EXERCISE\n",
    "        # raise NotImplementedError(\"Implement the observe method for the CheatyMcCheater class.\")\n",
    "        # END EXERCISE\n",
    "        # SOLUTION\n",
    "        self.best_arm = info[\"best_arm\"]\n",
    "        # END SOLUTION\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Cheater\"\n",
    "    \n",
    "# HIDE\n",
    "if MAIN:\n",
    "    cheater = CheatyMcCheater(num_arms, 0)\n",
    "    reward_averaging = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=0)\n",
    "    random = RandomAgent(num_arms, 0)\n",
    "\n",
    "    names = []\n",
    "    all_rewards = []\n",
    "\n",
    "    for agent in [cheater, reward_averaging, random]:\n",
    "        (rewards, num_correct) = run_agent(env, agent, n_runs=N_RUNS, base_seed=1)\n",
    "        names.append(str(agent))\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    utils.plot_rewards(all_rewards, names, moving_avg_window=15)\n",
    "    # FILTERS: ~\n",
    "    # fig = utils.plot_rewards_master(all_rewards, names, moving_avg_window=15)\n",
    "    # fig.write_html(str(section_dir / \"0202.html\"))\n",
    "    # fig.show()\n",
    "    # END FILTERS\n",
    "\n",
    "    assert (all_rewards[0] < all_rewards[1]).mean() < 0.001, \"Cheater should be better than reward averaging\"\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERS: soln,st\n",
    "TAGS: html,st-dropdown[Click to see the expected output]\n",
    "\n",
    "<iframe src = \"https://info-arena.github.io/ARENA_img/media/ch2_fig2.html\", width=\"680\", height=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Authentic RL Experience\n",
    "\n",
    "It would be nice if we could say something like \"optimistic reward averaging is a good/bad feature that improves/decreases performance in bandit problems.\" Unfortunately, we can't justifiably claim either at this point.\n",
    "\n",
    "Usually, RL code fails silently, which makes it difficult to be confident that you don't have any bugs. I had a bug in my first attempt that made both versions appear to perform equally, and there were only 13 lines of code, and I had written similar code before.\n",
    "\n",
    "The number of hyperparameters also grows rapidly, and hyperparameters have interactions with each other. Even in this simple problem, we already have two different ways to encourage exploration (`epsilon` and `optimism`), and it's not clear whether it's better to use one or the other, or both in some combination. It's actually worse than that, because `epsilon` should probably be annealed down at some rate.\n",
    "\n",
    "Even in this single comparison, we trained 200 agents for each version. Is that a big number or a small number to estimate the effect size? Probably we should like, compute some statistics? And test with a different number of arms - maybe we need more exploration for more arms? The time needed for a rigorous evaluation is going to increase quickly.\n",
    "\n",
    "We're using 0.23.1 of `gym`, which is not the latest version. 0.24.0 and 0.24.1 according to the [release notes](https://github.com/openai/gym/releases) have \"large bugs\" and the maintainers \"highly discourage using these releases\". How confident are we in the quality of the library code we're relying on?\n",
    "\n",
    "As we continue onward to more complicated algorithms, keep an eye out for small discrepancies or minor confusions. Look for opportunities to check and cross-check everything, and be humble with claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - implement UCBA selection\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 15-30 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Once you feel good about your `RewardAveraging` implementation, you should implement `UCBActionSelection`.\n",
    "\n",
    "This should store the same moving average rewards for each action as `RewardAveraging` did, but instead of taking actions using the epsilon-greedy strategy it should use Equation 2.10 in Section 2.7 to select actions using the upper confidence bound.\n",
    "\n",
    "You should expect to see a small improvement over `RewardAveraging` using this strategy.\n",
    "\n",
    "*Tip - be careful of division-by-zero errors!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
