# %%
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Literal, Optional, SupportsFloat, TypeVar

import gymnasium as gym
import numpy as np
import pandas as pd
import torch as t
import wandb
from IPython.display import display

# from gym.wrappers import RecordVideo

Arr = np.ndarray
from gymnasium.wrappers import (
    # GrayScaleObservation,
    # ResizeObservation,
    AtariPreprocessing,
    FrameStack,
)

# from part3_ppo.atari_wrappers import (
#     ClipRewardEnv,
#     EpisodicLifeEnv,
#     FireResetEnv,  # only if "FIRE" is in env.unwrapped.get_action_meanings()
#     MaxAndSkipEnv,
#     NoopResetEnv,
# )


# %%


ObsType = TypeVar("ObsType")
ActType = TypeVar("ActType")
RenderFrame = TypeVar("RenderFrame")


class RecordVideoWandb(gym.wrappers.RecordVideo):
    """
    This is a class which wraps around gym.wrappers.RecordVideo, not only enabling saving of videos to disc but also
    logging them to Weights & Biases. This is because wandb's built-in support for logging gymnasium-recorded videos
    automatically is very bad and error-prone, surprisingly this actually works better.

    Note, we assume we're logging every n episodes, not every n steps (see `make_env` code below). This is because
    logging every n steps leads to videos not starting from the first frame of the environment being reset, which is a
    bit annoying.

    Works like this: in pbar, use `n_videos_logged=getattr(self.envs.envs[0], "n_videos_logged", 0)`

    ! Note - not using this class any more, because pinning gymnasium==0.29.0 so RecordVideo no longer has this bug
    """

    def __init__(self, *args, **kwargs):
        self.use_wandb = kwargs.pop("use_wandb", False)
        self.n_videos_logged = 0
        super().__init__(*args, **kwargs)

    def stop_recording(self):
        path = os.path.join(self.video_folder, f"{self._video_name}.mp4")  # get this because `stop_recording` resets it
        super().stop_recording()
        if self.use_wandb and (self.episode_id > 1):  # so we don't log the video created on env initialization
            wandb.log({"videos": wandb.Video(path)})  # or step=self.step_id, self.episode_id ?
        self.n_videos_logged += 1


def make_env(
    env_id: str,
    seed: int,
    idx: int,
    run_name: str,
    mode: str = "classic-control",
    capture_video_every_n_episodes: int = None,
    video_save_path: str = None,
    **kwargs,
):
    """
    Return a function that returns an environment after setting up boilerplate.
    """

    # if capture_video_every_n_steps is None:
    #     video_log_freq = {"classic-control": 100, "atari": 30, "mujoco": 50}[mode]
    #     video_log_freq_step = {"classic-control": 2_000}[mode]

    def thunk():
        env = gym.make(env_id, render_mode="rgb_array")
        env = gym.wrappers.RecordEpisodeStatistics(env)
        if idx == 0 and capture_video_every_n_episodes:
            env = gym.wrappers.RecordVideo(
                env,
                f"{video_save_path}/{run_name}",
                # use_wandb=use_wandb,
                episode_trigger=lambda episode_id: episode_id % capture_video_every_n_episodes == 0,
                disable_logger=True,
            )

        if mode == "atari":
            env = prepare_atari_env(env)
        elif mode == "mujoco":
            env = prepare_mujoco_env(env)

        env.reset(seed=seed)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
        return env

    return thunk


def prepare_atari_env(env: gym.Env):
    """Note, ALE/Breakout-v5 has default frameskip=4, see https://ale.farama.org/environments/breakout/"""
    env = gym.wrappers.AtariPreprocessing(env, frame_skip=1)
    env = gym.wrappers.FrameStack(env, num_stack=4)
    return env


def prepare_mujoco_env(env: gym.Env):
    env = gym.wrappers.ClipAction(env)
    env = gym.wrappers.NormalizeObservation(env)
    env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
    env = gym.wrappers.NormalizeReward(env)
    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
    return env


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    t.manual_seed(seed)


def window_avg(arr: Arr, window: int):
    """
    Computes sliding window average
    """
    return np.convolve(arr, np.ones(window), mode="valid") / window


def cummean(arr: Arr):
    """
    Computes the cumulative mean
    """
    return np.cumsum(arr) / np.arange(1, len(arr) + 1)


# Taken from https://stackoverflow.com/questions/42869495/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm
# See https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average
def ewma(arr: Arr, alpha: float):
    """
    Returns the exponentially weighted moving average of x.
    Parameters:
    -----------
    x : array-like
    alpha : float {0 <= alpha <= 1}
    Returns:
    --------
    ewma: numpy array
          the exponentially weighted moving average
    """
    # Coerce x to an array
    s = np.zeros_like(arr)
    s[0] = arr[0]
    for i in range(1, len(arr)):
        s[i] = alpha * arr[i] + (1 - alpha) * s[i - 1]
    return s


def sum_rewards(rewards: list[int], gamma: float = 1):
    """
    Computes the total discounted sum of rewards for an episode.
    By default, assume no discount
    Input:
        rewards [r1, r2, r3, ...] The rewards obtained during an episode
        gamma: Discount factor
    Output:
        The sum of discounted rewards
        r1 + gamma*r2 + gamma^2 r3 + ...
    """
    total_reward = 0
    for r in rewards[:0:-1]:  # reverse, excluding first
        total_reward += r
        total_reward *= gamma
    total_reward += rewards[0]
    return total_reward


@dataclass
class PPOArgs:
    # Basic / global
    seed: int = 1
    env_id: str = "CartPole-v1"
    mode: Literal["classic-control", "atari", "mujoco"] = "classic-control"

    # Wandb / logging
    use_wandb: bool = False
    capture_video: bool = True
    exp_name: str = "PPO_Implementation"
    log_dir: str = "logs"
    wandb_project_name: str = "PPOCart"
    wandb_entity: str = None

    # Duration of different phases
    total_timesteps: int = 500000
    num_envs: int = 4
    num_steps: int = 128
    num_minibatches: int = 4
    batches_per_learning_phase: int = 4

    # Optimization hyperparameters
    learning_rate: float = 0.00025
    max_grad_norm: float = 0.5

    # Computing advantage function
    gamma: float = 0.99
    gae_lambda: float = 0.95

    # Computing other loss functions
    clip_coef: float = 0.2
    ent_coef: float = 0.01
    vf_coef: float = 0.25

    def __post_init__(self):
        self.batch_size = self.num_steps * self.num_envs
        assert self.batch_size % self.num_minibatches == 0, "batch_size must be divisible by num_minibatches"
        self.minibatch_size = self.batch_size // self.num_minibatches
        self.total_phases = self.total_timesteps // self.batch_size
        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches


arg_help_strings = dict(
    seed="seed of the experiment",
    env_id="the id of the environment",
    mode="can be 'classic-control', 'atari' or 'mujoco'",
    use_wandb="if toggled, this experiment will be tracked with Weights and Biases",
    capture_video="whether to capture videos of the agent performances (check out `videos` folder)",
    exp_name="the name of this experiment",
    log_dir="the directory where the logs will be stored",
    wandb_project_name="the wandb's project name",
    wandb_entity="the entity (team) of wandb's project",
    total_timesteps="total timesteps of the experiments",
    num_envs="number of synchronized vector environments in our `envs` object (this is N in the '37 Implementational Details' post)",
    num_steps="number of steps taken in the rollout phase (this is M in the '37 Implementational Details' post)",
    num_minibatches="the number of minibatches you divide each batch up into",
    batches_per_learning_phase="how many times you loop through the data generated in each rollout phase",
    learning_rate="the learning rate of the optimizer",
    max_grad_norm="value used in gradient clipping",
    gamma="the discount factor gamma",
    gae_lambda="the discount factor used in our GAE estimation",
    clip_coef="the epsilon term used in the clipped surrogate objective function",
    ent_coef="coefficient of entropy bonus term",
    vf_coef="cofficient of value loss function",
    batch_size="N * M in the '37 Implementational Details' post (calculated from other values in PPOArgs)",
    minibatch_size="the size of a single minibatch we perform a gradient step on (calculated from other values in PPOArgs)",
    total_phases="total number of phases during training (calculated from other values in PPOArgs)",
    total_training_steps="total number of minibatches we will perform an update step on during training (calculated from other values in PPOArgs)",
)


def arg_help(args: Optional[PPOArgs], print_df=False):
    """Prints out a nicely displayed list of arguments, their default values, and what they mean."""
    if args is None:
        args = PPOArgs()
        changed_args = []
    else:
        default_args = PPOArgs()
        changed_args = [key for key in default_args.__dict__ if getattr(default_args, key) != getattr(args, key)]
    df = pd.DataFrame([arg_help_strings]).T
    df.columns = ["description"]
    df["default value"] = [repr(getattr(args, name)) for name in df.index]
    df.index.name = "arg"
    df = df[["default value", "description"]]
    if print_df:
        df.insert(1, "changed?", ["yes" if i in changed_args else "" for i in df.index])
        with pd.option_context("max_colwidth", 0, "display.width", 150, "display.colheader_justify", "left"):
            print(df)
    else:
        s = df.style.set_table_styles(
            [
                {"selector": "td", "props": "text-align: left;"},
                {"selector": "th", "props": "text-align: left;"},
            ]
        ).apply(
            lambda row: ["background-color: red" if row.name in changed_args else None]
            + [
                None,
            ]
            * (len(row) - 1),
            axis=1,
        )
        with pd.option_context("max_colwidth", 0):
            display(s)


def set_global_seeds(seed):
    """Sets random seeds in several different ways (to guarantee reproducibility)"""
    t.manual_seed(seed)
    t.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)
    t.backends.cudnn.deterministic = True
