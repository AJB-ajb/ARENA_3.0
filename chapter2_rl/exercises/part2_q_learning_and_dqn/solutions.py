# %%


import os
import re
import sys
import time
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import Literal

import einops
import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import torch as t
import wandb
from gymnasium.spaces import Box, Discrete
from IPython.display import HTML, clear_output
from jaxtyping import Bool, Float, Int
from matplotlib.animation import FuncAnimation
from torch import Tensor, nn
from tqdm import tqdm, trange

warnings.filterwarnings("ignore")

Arr = np.ndarray

# Make sure exercises are in the path
chapter = "chapter2_rl"
section = "part2_q_learning_and_dqn"
root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())
exercises_dir = root_dir / chapter / "exercises"
section_dir = exercises_dir / section
if str(exercises_dir) not in sys.path:
    sys.path.append(str(exercises_dir))

MAIN = __name__ == "__main__"

import part2_q_learning_and_dqn.tests as tests
import part2_q_learning_and_dqn.utils as utils
from part1_intro_to_rl.solutions import Environment, Norvig, Toy, find_optimal_policy
from part3_ppo.utils import make_env
from plotly_utils import cliffwalk_imshow, line, plot_cartpole_obs_and_dones

device = t.device("cuda" if t.cuda.is_available() else "cpu")

MAIN = __name__ == "__main__"

# %%

ObsType = int
ActType = int


class DiscreteEnviroGym(gym.Env):
    action_space: gym.spaces.Discrete
    observation_space: gym.spaces.Discrete
    """
    A discrete environment class for reinforcement learning, compatible with OpenAI Gym.

    This class represents a discrete environment where actions and observations are discrete.
    It is designed to interface with a provided `Environment` object which defines the
    underlying dynamics, states, and actions.

    Attributes:
        action_space (gym.spaces.Discrete): The space of possible actions.
        observation_space (gym.spaces.Discrete): The space of possible observations (states).
        env (Environment): The underlying environment with its own dynamics and properties.
    """

    def __init__(self, env: Environment):
        super().__init__()
        self.env = env
        self.observation_space = gym.spaces.Discrete(env.num_states)
        self.action_space = gym.spaces.Discrete(env.num_actions)
        self.reset()

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        """
        Execute an action and return the new state, reward, done flag, and additional info.
        The behaviour of this function depends primarily on the dynamics of the underlying
        environment.
        """
        states, rewards, probs = self.env.dynamics(self.pos, action)
        idx = self.np_random.choice(len(states), p=probs)
        new_state, reward = states[idx], rewards[idx]
        self.pos = new_state
        terminated = self.pos in self.env.terminal
        truncated = False
        info = {"env": self.env}
        return new_state, reward, terminated, truncated, info

    def reset(self, seed: int | None = None, options=None) -> tuple[ObsType, dict]:
        """
        Resets the environment to its initial state.
        """
        super().reset(seed=seed)
        self.pos = self.env.start
        return self.pos, {}

    def render(self, mode="human"):
        assert mode == "human", f"Mode {mode} not supported!"


# %%

if MAIN:
    gym.envs.registration.register(
        id="NorvigGrid-v0",
        entry_point=DiscreteEnviroGym,
        max_episode_steps=100,
        nondeterministic=True,
        kwargs={"env": Norvig(penalty=-0.04)},
    )

    gym.envs.registration.register(
        id="ToyGym-v0",
        entry_point=DiscreteEnviroGym,
        max_episode_steps=2,
        nondeterministic=False,
        kwargs={"env": Toy()},
    )

# %%


@dataclass
class Experience:
    """
    A class for storing one piece of experience during an episode run.
    """

    obs: ObsType
    act: ActType
    reward: float
    new_obs: ObsType
    new_act: ActType | None = None


@dataclass
class AgentConfig:
    """Hyperparameters for agents"""

    epsilon: float = 0.1
    lr: float = 0.05
    optimism: float = 0


defaultConfig = AgentConfig()


class Agent:
    """Base class for agents interacting with an environment (you do not need to add any implementation here)"""

    rng: np.random.Generator

    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):
        self.env = env
        self.reset(seed)
        self.config = config
        self.gamma = gamma
        self.num_actions = env.action_space.n
        self.num_states = env.observation_space.n
        self.name = type(self).__name__

    def get_action(self, obs: ObsType) -> ActType:
        raise NotImplementedError()

    def observe(self, exp: Experience) -> None:
        """
        Agent observes experience, and updates model as appropriate.
        Implementation depends on type of agent.
        """
        pass

    def reset(self, seed: int) -> tuple[ObsType, dict]:
        self.rng = np.random.default_rng(seed)
        return None, {}

    def run_episode(self, seed) -> list[int]:
        """
        Simulates one episode of interaction, agent learns as appropriate
        Inputs:
            seed : Seed for the random number generator
        Outputs:
            The rewards obtained during the episode
        """
        rewards = []
        obs, info = self.env.reset(seed=seed)
        self.reset(seed=seed)
        done = False
        while not done:
            act = self.get_action(obs)
            new_obs, reward, terminated, truncated, info = self.env.step(act)
            done = terminated or truncated
            exp = Experience(obs, act, reward, new_obs)
            self.observe(exp)
            rewards.append(reward)
            obs = new_obs
        return rewards

    def train(self, n_runs=500):
        """
        Run a batch of episodes, and return the total reward obtained per episode
        Inputs:
            n_runs : The number of episodes to simulate
        Outputs:
            The discounted sum of rewards obtained for each episode
        """
        all_rewards = []
        for seed in trange(n_runs):
            rewards = self.run_episode(seed)
            all_rewards.append(utils.sum_rewards(rewards, self.gamma))
        return all_rewards


class Random(Agent):
    def get_action(self, obs: ObsType) -> ActType:
        return self.rng.integers(0, self.num_actions)


# %%


class Cheater(Agent):
    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma=0.99, seed=0):
        super().__init__(env, config, gamma, seed)
        self.pi_opt = find_optimal_policy(self.env.unwrapped.env, self.gamma)

    def get_action(self, obs):
        return self.pi_opt[obs]


if MAIN:
    env_toy = gym.make("ToyGym-v0")
    agents_toy: list[Agent] = [Cheater(env_toy), Random(env_toy)]
    returns_dict = {}
    for agent in agents_toy:
        returns = agent.train(n_runs=100)
        returns_dict[agent.name] = utils.cummean(returns)

    line(
        list(returns_dict.values()),
        names=list(returns_dict.keys()),
        title=f"Avg. reward on {env_toy.spec.name}",
        labels={"x": "Episode", "y": "Avg. reward", "variable": "Agent"},
        template="simple_white",
        width=700,
        height=400,
    )

# %%


class EpsilonGreedy(Agent):
    """
    A class for SARSA and Q-Learning to inherit from.
    """

    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):
        super().__init__(env, config, gamma, seed)
        self.Q = np.zeros((self.num_states, self.num_actions)) + self.config.optimism

    def get_action(self, obs: ObsType) -> ActType:
        """
        Selects an action using epsilon-greedy with respect to Q-value estimates
        """
        if self.rng.random() < self.config.epsilon:
            return self.rng.integers(0, self.num_actions)
        else:
            return self.Q[obs].argmax()


class QLearning(EpsilonGreedy):
    def observe(self, exp: Experience) -> None:
        s_t, a_t, r_t_1, s_t_1 = exp.obs, exp.act, exp.reward, exp.new_obs
        self.Q[s_t, a_t] += self.config.lr * (r_t_1 + self.gamma * np.max(self.Q[s_t_1]) - self.Q[s_t, a_t])


class SARSA(EpsilonGreedy):
    def observe(self, exp: Experience):
        s_t, a_t, r_t_1, s_t_1, a_t_1 = exp.obs, exp.act, exp.reward, exp.new_obs, exp.new_act
        self.Q[s_t, a_t] += self.config.lr * (r_t_1 + self.gamma * self.Q[s_t_1, a_t_1] - self.Q[s_t, a_t])

    def run_episode(self, seed) -> list[int]:
        rewards = []
        obs, info = self.env.reset(seed=seed)
        act = self.get_action(obs)
        self.reset(seed=seed)
        done = False
        while not done:
            new_obs, reward, terminated, truncated, info = self.env.step(act)
            done = terminated or truncated
            new_act = self.get_action(new_obs)
            exp = Experience(obs, act, reward, new_obs, new_act)
            self.observe(exp)
            rewards.append(reward)
            obs = new_obs
            act = new_act
        return rewards


if MAIN:
    n_runs = 1000
    gamma = 0.99
    seed = 1
    env_norvig = gym.make("NorvigGrid-v0")
    config_norvig = AgentConfig()
    args_norvig = (env_norvig, config_norvig, gamma, seed)
    agents_norvig: list[Agent] = [
        Cheater(*args_norvig),
        QLearning(*args_norvig),
        SARSA(*args_norvig),
        Random(*args_norvig),
    ]
    returns_dict = {}
    for agent in agents_norvig:
        returns = agent.train(n_runs)
        returns_dict[agent.name] = utils.cummean(returns)

    line(
        list(returns_dict.values()),
        names=list(returns_dict.keys()),
        title=f"Avg. reward on {env_norvig.spec.name}",
        labels={"x": "Episode", "y": "Avg. reward", "variable": "Agent"},
        template="simple_white",
        width=700,
        height=400,
    )

# %%

if MAIN:
    gamma = 1
    seed = 0

    config_cliff = AgentConfig(epsilon=0.1, lr=0.1, optimism=0)
    env = gym.make("CliffWalking-v0")
    n_runs = 2500
    args_cliff = (env, config_cliff, gamma, seed)

    returns_list = []
    name_list = []
    agents = [QLearning(*args_cliff), SARSA(*args_cliff)]

    for agent in agents:
        assert isinstance(agent, (QLearning, SARSA))  # for typechecker
        returns = agent.train(n_runs)[1:]
        returns_list.append(utils.cummean(returns))
        name_list.append(agent.name)
        V = agent.Q.max(axis=-1).reshape(4, 12)
        pi = agent.Q.argmax(axis=-1).reshape(4, 12)
        cliffwalk_imshow(V, pi, title=f"CliffWalking: {agent.name} Agent", width=800, height=400)

    line(
        returns_list,
        names=name_list,
        template="simple_white",
        title="Q-Learning vs SARSA on CliffWalking-v0",
        labels={"x": "Episode", "y": "Avg. reward", "variable": "Agent"},
        width=700,
        height=400,
    )

# %%


class CliffWalking(Environment):
    def __init__(self, penalty=-1):
        self.height = 4
        self.width = 12
        self.penalty = penalty
        num_states = self.height * self.width
        num_actions = 4
        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])
        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])  # up, right, down, left
        self.dim = (self.height, self.width)

        # special states: tuples of state and reward
        # all other states get penalty
        start = 36
        terminal = np.array([47], dtype=int)
        self.cliff = np.arange(37, 47, dtype=int)
        self.goal_rewards = np.array([1.0, -1.0])

        super().__init__(num_states, num_actions, start=start, terminal=terminal)

    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:
        """
        Returns tuple of (out_states, out_rewards, out_probs) for this given (state, action) pair.
        """

        def state_index(state):
            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)
            pos = state[0] + state[1] * self.width
            assert 0 <= pos < self.num_states, print(state, pos)
            return pos

        pos = self.states[state]

        if state in self.terminal:
            return (np.array([state]), np.array([0]), np.array([1]))

        # No slipping; each action is deterministic
        out_probs = np.zeros(self.num_actions)
        out_probs[action] = 1

        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions
        out_rewards = np.zeros(self.num_actions) + self.penalty
        new_states = [pos + x for x in self.actions]

        for i, s_new in enumerate(new_states):
            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):
                out_states[i] = state
                continue

            new_state = state_index(s_new)

            # Check if would hit the cliff, if so then get -100 penalty and go back to start
            if new_state in self.cliff:
                out_states[i] = self.start
                out_rewards[i] -= 100

            else:
                out_states[i] = new_state

            for idx in range(len(self.terminal)):
                if new_state == self.terminal[idx]:
                    out_rewards[i] = self.goal_rewards[idx]

        return (out_states, out_rewards, out_probs)

    @staticmethod
    def render(Q: Arr, name: str):
        V = Q.max(axis=-1).reshape(4, 12)
        pi = Q.argmax(axis=-1).reshape(4, 12)
        cliffwalk_imshow(V, pi, title=f"CliffWalking: {name} Agent")


if MAIN:
    gym.envs.registration.register(
        id="CliffWalking-myversion",
        entry_point=DiscreteEnviroGym,
        max_episode_steps=200,
        nondeterministic=True,
        kwargs={"env": CliffWalking(penalty=-1)},
    )
    gamma = 0.99
    seed = 0
    config_cliff = AgentConfig(epsilon=0.1, lr=0.1, optimism=0)
    env = gym.make("CliffWalking-myversion")
    n_runs = 500
    args_cliff = (env, config_cliff, gamma, seed)

    agents = [Cheater(*args_cliff), QLearning(*args_cliff), SARSA(*args_cliff), Random(*args_cliff)]
    returns_list = []
    name_list = []

    for agent in agents:
        returns = agent.train(n_runs)[1:]
        returns_list.append(utils.cummean(returns))
        name_list.append(agent.name)

    line(
        returns_list,
        names=name_list,
        template="simple_white",
        title="Q-Learning vs SARSA on CliffWalking-v0",
        labels={"x": "Episode", "y": "Avg. reward", "variable": "Agent"},
        width=700,
        height=400,
    )

# %%


class QNetwork(nn.Module):
    """For consistency with your tests, please wrap your modules in a `nn.Sequential` called `layers`."""

    layers: nn.Sequential

    def __init__(self, dim_observation: int, num_actions: int, hidden_sizes: list[int] = [120, 84]):
        super().__init__()
        in_features_list = [dim_observation] + hidden_sizes
        out_features_list = hidden_sizes + [num_actions]
        layers = []
        for i, (in_features, out_features) in enumerate(zip(in_features_list, out_features_list)):
            layers.append(nn.Linear(in_features, out_features))
            if i < len(in_features_list) - 1:
                layers.append(nn.ReLU())
        self.layers = nn.Sequential(*layers)

    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)


if MAIN:
    net = QNetwork(dim_observation=4, num_actions=2)
    n_params = sum((p.nelement() for p in net.parameters()))
    assert isinstance(getattr(net, "layers", None), nn.Sequential)
    print(net)
    print(f"Total number of parameters: {n_params}")
    print("You should manually verify network is Linear-ReLU-Linear-ReLU-Linear")
    assert not isinstance(net.layers[-1], nn.ReLU)
    assert n_params == 10934

# %%


@dataclass
class ReplayBufferSamples:
    """
    Samples from the replay buffer, converted to PyTorch for use in neural network training.

    Data is equivalent to (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1}). Note - here, d_{t+1} is actually **terminated** rather
    than **done** (i.e. it records the times when we went out of bounds, not when the environment timed out).
    """

    observations: Float[Tensor, "sample_size *obs_shape"]
    actions: Float[Tensor, "sample_size *action_shape"]
    rewards: Float[Tensor, "sample_size"]
    terminated: Bool[Tensor, "sample_size"]
    next_observations: Float[Tensor, "sample_size *obs_shape"]


# HIDE END


class ReplayBuffer:
    """
    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.
    """

    rng: np.random.Generator
    observations: Float[Arr, "buffer_size *obs_shape"]
    actions: Float[Arr, "buffer_size *action_shape"]
    rewards: Float[Arr, "buffer_size"]
    terminated: Bool[Arr, "buffer_size"]
    next_observations: Float[Arr, "buffer_size *obs_shape"]

    def __init__(
        self, num_environments: int, obs_shape: tuple[int], action_shape: tuple[int], buffer_size: int, seed: int
    ):
        self.num_environments = num_environments
        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.buffer_size = buffer_size
        self.rng = np.random.default_rng(seed)

        self.observations = np.empty((0, *self.obs_shape), dtype=np.float32)
        self.actions = np.empty((0, *self.action_shape), dtype=np.int32)
        self.rewards = np.empty(0, dtype=np.float32)
        self.terminated = np.empty(0, dtype=bool)
        self.next_observations = np.empty((0, *self.obs_shape), dtype=np.float32)

    def add(
        self,
        obs: Float[Arr, "num_envs *obs_shape"],
        actions: Int[Arr, "num_envs *action_shape"],
        rewards: Float[Arr, "num_envs"],
        terminated: Bool[Arr, "num_envs"],
        next_obs: Float[Arr, "num_envs *obs_shape"],
    ) -> None:
        """
        obs:        Observation before the action (s_t)
        actions:    Action chosen by the agent (a_t)
        rewards:    Reward after the action (r_{t+1})
        terminated: If True, the episode ended and was reset automatically (d_{t+1})
        next_obs:   Observation after the action (s_{t+1})
        """
        self.observations = np.concatenate((self.observations, obs))[-self.buffer_size :]
        self.actions = np.concatenate((self.actions, actions))[-self.buffer_size :]
        self.rewards = np.concatenate((self.rewards, rewards))[-self.buffer_size :]
        self.terminated = np.concatenate((self.terminated, terminated))[-self.buffer_size :]
        self.next_observations = np.concatenate((self.next_observations, next_obs))[-self.buffer_size :]

    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:
        """
        Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device. Sampling is
        with replacement, and sample_size may be larger than the buffer size.
        """
        current_buffer_size = self.observations.shape[0]
        indices = self.rng.integers(0, current_buffer_size, sample_size)

        return ReplayBufferSamples(
            t.from_numpy(self.observations[indices]).to(device),
            t.from_numpy(self.actions[indices]).to(device),
            t.from_numpy(self.rewards[indices]).to(device),
            t.from_numpy(self.terminated[indices]).to(device),
            t.from_numpy(self.next_observations[indices]).to(device),
        )


# %%

if MAIN:
    buffer = ReplayBuffer(num_environments=1, obs_shape=(4,), action_shape=(), buffer_size=256, seed=0)
    envs = gym.vector.SyncVectorEnv([make_env("CartPole-v1", 0, 0, "test")])
    obs, infos = envs.reset()

    for i in range(256):
        # Choose random action, and take a step in the environment
        actions = envs.action_space.sample()
        next_obs, rewards, terminated, truncated, infos = envs.step(actions)

        # Get `real_next_obs` by finding all environments where we terminated & replacing `next_obs` with the actual terminal states
        true_next_obs = next_obs.copy()
        for n in range(envs.num_envs):
            if (terminated | truncated)[n]:
                true_next_obs[n] = infos["final_observation"][n]

        # Add experience to buffer, as long as we didn't just finish an episode (so obs & next_obs are from the same episode)
        buffer.add(obs, actions, rewards, terminated, true_next_obs)
        obs = next_obs

    sample = buffer.sample(256, device="cpu")

    plot_cartpole_obs_and_dones(
        buffer.observations,
        buffer.terminated,
        title="Current observations s<sub>t</sub><br>so when d<sub>t+1</sub> = 1, these are the states just before termination",
    )

    plot_cartpole_obs_and_dones(
        buffer.next_observations,
        buffer.terminated,
        title="Next observations s<sub>t+1</sub><br>so when d<sub>t+1</sub> = 1, these are the terminated states",
    )

    plot_cartpole_obs_and_dones(
        sample.observations,
        sample.terminated,
        title="Current observations (sampled)<br>this is what gets fed into our model for training",
    )

# %%


def linear_schedule(
    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int
) -> float:
    """Return the appropriate epsilon for the current step.

    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).
    In other words, we are in "explore mode" with start_e >= epsilon >= end_e for the first `exploration_fraction` fraction
    of total timesteps, and then stay at end_e for the rest of the episode.
    """
    return start_e + (end_e - start_e) * min(current_step / (exploration_fraction * total_timesteps), 1)


if MAIN:
    epsilons = [
        linear_schedule(step, start_e=1.0, end_e=0.05, exploration_fraction=0.5, total_timesteps=500)
        for step in range(500)
    ]
    line(epsilons, labels={"x": "steps", "y": "epsilon"}, title="Probability of random action", height=400, width=600)

    tests.test_linear_schedule(linear_schedule)

# %%


def epsilon_greedy_policy(
    envs: gym.vector.SyncVectorEnv,
    q_network: QNetwork,
    rng: np.random.Generator,
    obs: Float[Arr, "num_envs *obs_shape"],
    epsilon: float,
) -> Int[Arr, "num_envs *action_shape"]:
    """With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.
    Inputs:
        envs:       The family of environments to run against
        q_network:  The QNetwork used to approximate the Q-value function
        obs:        The current observation for each environment
        epsilon:    The probability of taking a random action
    Outputs:
        actions:    The sampled action for each environment.
    """
    # Convert `obs` into a tensor so we can feed it into our model
    obs = t.from_numpy(obs).to(device)

    num_actions = envs.single_action_space.n
    if rng.random() < epsilon:
        return rng.integers(0, num_actions, size=(envs.num_envs,))
    else:
        q_scores = q_network(obs)
        return q_scores.argmax(-1).detach().cpu().numpy()


if MAIN:
    tests.test_epsilon_greedy_policy(epsilon_greedy_policy)

# %%

ObsType = np.ndarray
ActType = int


class Probe1(gym.Env):
    """One action, observation of [0.0], one timestep long, +1 reward.

    We expect the agent to rapidly learn that the value of the constant [0.0] observation is +1.0. Note we're using a continuous observation space for consistency with CartPole.
    """

    action_space: Discrete
    observation_space: Box

    def __init__(self, render_mode: str = "rgb_array"):
        super().__init__()
        self.observation_space = Box(np.array([0]), np.array([0]))
        self.action_space = Discrete(1)
        self.reset()

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        return np.array([0]), 1.0, True, True, {}

    def reset(self, seed: int | None = None, options=None) -> ObsType | tuple[ObsType, dict]:
        super().reset(seed=seed)
        return np.array([0.0]), {}


if MAIN:
    gym.envs.registration.register(id="Probe1-v0", entry_point=Probe1)
    env = gym.make("Probe1-v0")
    assert env.observation_space.shape == (1,)
    assert env.action_space.shape == ()

# %%


class Probe2(gym.Env):
    """One action, observation of [-1.0] or [+1.0], one timestep long, reward equals observation.

    We expect the agent to rapidly learn the value of each observation is equal to the observation.
    """

    action_space: Discrete
    observation_space: Box

    def __init__(self, render_mode: str = "rgb_array"):
        super().__init__()
        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))
        self.action_space = Discrete(1)
        self.reset()
        self.reward = None

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        assert self.reward is not None
        return np.array([self.observation]), self.reward, True, True, {}

    def reset(self, seed: int | None = None, options=None) -> ObsType | tuple[ObsType, dict]:
        super().reset(seed=seed)
        self.reward = 1.0 if self.np_random.random() < 0.5 else -1.0
        self.observation = self.reward
        return np.array([self.reward]), {}


if MAIN:
    gym.envs.registration.register(id="Probe2-v0", entry_point=Probe2)


class Probe3(gym.Env):
    """One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.

    We expect the agent to rapidly learn the discounted value of the initial observation.
    """

    action_space: Discrete
    observation_space: Box

    def __init__(self, render_mode: str = "rgb_array"):
        super().__init__()
        self.observation_space = Box(np.array([-0.0]), np.array([+1.0]))
        self.action_space = Discrete(1)
        self.reset()

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        self.n += 1
        if self.n == 1:
            return np.array([1.0]), 0.0, False, False, {}
        elif self.n == 2:
            return np.array([0.0]), 1.0, True, True, {}
        raise ValueError(self.n)

    def reset(self, seed: int | None = None, options=None) -> ObsType | tuple[ObsType, dict]:
        super().reset(seed=seed)
        self.n = 0
        return np.array([0.0]), {}


if MAIN:
    gym.envs.registration.register(id="Probe3-v0", entry_point=Probe3)


class Probe4(gym.Env):
    """Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on the action.

    We expect the agent to learn to choose the +1.0 action.
    """

    action_space: Discrete
    observation_space: Box

    def __init__(self, render_mode: str = "rgb_array"):
        self.observation_space = Box(np.array([-0.0]), np.array([+0.0]))
        self.action_space = Discrete(2)
        self.reset()

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        reward = -1.0 if action == 0 else 1.0
        return np.array([0.0]), reward, True, True, {}

    def reset(self, seed: int | None = None, options=None) -> ObsType | tuple[ObsType, dict]:
        super().reset(seed=seed)
        return np.array([0.0]), {}


if MAIN:
    gym.envs.registration.register(id="Probe4-v0", entry_point=Probe4)


class Probe5(gym.Env):
    """Two actions, random 0/1 observation, one timestep, reward is 1 if action equals observation otherwise -1.

    We expect the agent to learn to match its action to the observation.
    """

    action_space: Discrete
    observation_space: Box

    def __init__(self, render_mode: str = "rgb_array"):
        self.observation_space = Box(np.array([-1.0]), np.array([+1.0]))
        self.action_space = Discrete(2)
        self.reset()

    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:
        reward = 1.0 if action == self.obs else -1.0
        return np.array([self.obs]), reward, True, True, {}

    def reset(self, seed: int | None = None, options=None) -> ObsType | tuple[ObsType, dict]:
        super().reset(seed=seed)
        self.obs = 1.0 if self.np_random.random() < 0.5 else 0.0
        return np.array([self.obs], dtype=float), {}


if MAIN:
    gym.envs.registration.register(id="Probe5-v0", entry_point=Probe5)

# %%


@dataclass
class DQNArgs:
    # Basic / global
    seed: int = 1
    cuda: bool = t.cuda.is_available()  # TODO - remove this, replace with `device`
    env_id: str = "CartPole-v1"
    mode: Literal["classic-control", "atari"] = "classic-control"
    num_envs: int = 1

    # Wandb / logging
    use_wandb: bool = False
    capture_video_every_n_episodes: int | None = None
    exp_name: str = "CartPoleDQN"
    wandb_entity: str | None = None

    # Duration of different phases
    buffer_size: int = 10_000
    train_frequency: int = 10
    total_timesteps: int = 500_000
    target_network_frequency: int = 100

    # Optimization hyperparameters
    batch_size: int = 128
    learning_rate: float = 0.00025
    start_e: float = 1.0
    end_e: float = 0.1

    # Misc. RL related
    gamma: float = 0.99
    exploration_fraction: float = 0.2

    def __post_init__(self):
        assert self.total_timesteps - self.buffer_size >= self.train_frequency
        self.total_training_steps = (self.total_timesteps - self.buffer_size) // self.train_frequency
        self.capture_video = self.capture_video_every_n_episodes is not None


if MAIN:
    args = DQNArgs(batch_size=256)
    # utils.arg_help(args)

# %%


class DQNAgent:
    """Base Agent class handling the interaction with the environment."""

    def __init__(
        self,
        envs: gym.vector.SyncVectorEnv,
        args: DQNArgs,
        buffer: ReplayBuffer,
        q_network: QNetwork,
        target_network: QNetwork,
        rng: np.random.Generator,
    ):
        self.envs = envs
        self.args = args
        self.buffer = buffer

        self.obs, _ = self.envs.reset()  # Need a starting observation!

        self.step = 0
        self.epsilon = args.start_e
        self.q_network = q_network
        self.target_network = target_network
        self.rng = rng

    def play_step(self) -> list[dict]:
        """
        Carries out a single interaction step between agent & environment, and adds results to the replay buffer.

        Returns `infos` (list of dictionaries containing info we will log).
        """
        self.obs = np.array(self.obs, dtype=np.float32)
        actions = self.get_actions(self.obs)
        next_obs, rewards, terminated, truncated, infos = self.envs.step(actions)

        # Get `real_next_obs` by finding all environments where we terminated & replacing `next_obs` with the actual terminal states
        true_next_obs = next_obs.copy()
        for n in range(self.args.num_envs):
            if (terminated | truncated)[n]:
                true_next_obs[n] = infos["final_observation"][n]

        self.buffer.add(self.obs, actions, rewards, terminated, true_next_obs)
        self.obs = next_obs
        self.step += 1
        return infos

    def get_actions(self, obs: np.ndarray) -> np.ndarray:
        """
        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.
        """
        self.epsilon = linear_schedule(
            self.step, self.args.start_e, self.args.end_e, self.args.exploration_fraction, self.args.total_timesteps
        )
        actions = epsilon_greedy_policy(self.envs, self.q_network, self.rng, obs, self.epsilon)
        assert actions.shape == (len(self.envs.envs),)
        return actions


if MAIN:
    tests.test_agent(DQNAgent)

# %%


class DQNTrainer:
    def __init__(self, args: DQNArgs):
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__seed{args.seed}__{time.strftime('%Y%m%d-%H%M%S')}"
        self.envs = gym.vector.SyncVectorEnv(
            [make_env(idx=0, run_name=self.run_name, video_save_path=section_dir / "videos", **args.__dict__)]
        )
        self.start_time = time.time()
        self.rng = np.random.default_rng(args.seed)

        # Get obs & action shapes (we assume we're dealing with a single discrete action)
        num_actions = self.envs.single_action_space.n
        action_shape = ()
        obs_shape = self.envs.single_observation_space.shape
        num_observations = np.array(obs_shape, dtype=int).prod()

        if self.args.mode == "classic-control":
            self.q_network = QNetwork(num_observations, num_actions).to(device)
            self.target_network = QNetwork(num_observations, num_actions).to(device)
        elif self.args.mode == "atari":
            self.q_network = AtariQNetwork(obs_shape, num_actions).to(device)  # type: ignore
            self.target_network = AtariQNetwork(obs_shape, num_actions).to(device)  # type: ignore

        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = t.optim.AdamW(self.q_network.parameters(), lr=args.learning_rate)

        self.buffer = ReplayBuffer(len(self.envs.envs), obs_shape, action_shape, args.buffer_size, args.seed)
        self.agent = DQNAgent(self.envs, self.args, self.buffer, self.q_network, self.target_network, self.rng)

    def add_to_replay_buffer(self, n: int, verbose: bool = False):
        """
        Takes n steps with the agent, adding to the replay buffer (and logging any results). Should return the last
        episode length from the infos dict if an episode has terminated at some point in these steps.

        Optional argument `verbose`: if True, we can use a progress bar (useful to check how long the initial buffer
        filling is taking).
        """
        last_episode_len = None
        for step in tqdm(range(n), disable=not verbose, desc="Adding to replay buffer"):
            infos = self.agent.play_step()
            if "final_info" in infos:
                last_episode_len = infos["final_info"][0]["episode"]["l"].item()
                if self.args.use_wandb:
                    wandb.log({"episode_len": last_episode_len}, step=self.agent.step)
        return last_episode_len

    def training_step(self, step: int) -> Float[Tensor, ""]:
        """
        Samples once from the replay buffer, and takes a single training step. The `step` argument is used to track the
        number of training steps taken.
        """
        data = self.buffer.sample(self.args.batch_size, device)
        s_t, a_t, r_t_1, d_t_1, s_t_1 = (
            data.observations,
            data.actions,
            data.rewards,
            data.terminated,
            data.next_observations,
        )

        with t.inference_mode():
            target_max = self.target_network(s_t_1).max(-1).values
        predicted_q_vals = self.q_network(s_t)[range(self.args.batch_size), a_t]

        td_error = r_t_1 + self.args.gamma * target_max * (1 - d_t_1.float()) - predicted_q_vals
        loss = td_error.pow(2).mean()
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

        if step % self.args.target_network_frequency == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        if self.args.use_wandb:
            wandb.log(
                {
                    "td_loss": loss,
                    "q_values": predicted_q_vals.mean().item(),
                    "SPS": int(self.agent.step / (time.time() - self.start_time)),
                },
                step=self.agent.step,
            )

    def train(self) -> None:
        if self.args.use_wandb:
            wandb.init(
                project=self.args.exp_name,
                entity=self.args.wandb_entity,
                name=self.run_name,
                monitor_gym=self.args.capture_video,
            )
            wandb.watch(self.q_network, log="all", log_freq=20)

        self.add_to_replay_buffer(self.args.buffer_size, verbose=True)

        pbar = tqdm(range(self.args.total_training_steps))
        last_logged_time = time.time()  # helps us not update the progress bar too much

        for step in pbar:
            last_episode_len = self.add_to_replay_buffer(self.args.train_frequency)

            if (last_episode_len is not None) and (time.time() - last_logged_time > 1):
                last_logged_time = time.time()
                pbar.set_postfix(step=self.agent.step, last_episode_len=last_episode_len)

            self.training_step(step)

        if self.args.use_wandb:
            wandb.finish()


# %%


def test_probe(probe_idx: int):
    """
    Tests a probe environment by training a network on it & verifying that the value functions are
    in the expected range.
    """
    # Train our network
    args = DQNArgs(
        env_id=f"Probe{probe_idx}-v0",
        exp_name=f"test-probe-{probe_idx}",
        total_timesteps=3000 if probe_idx <= 2 else 5000,
        learning_rate=0.001,
        buffer_size=500,
        use_wandb=False,
        target_network_frequency=20,
    )
    trainer = DQNTrainer(args)
    trainer.train()

    # Get the correct set of observations, and corresponding values we expect
    obs_for_probes = [[[0.0]], [[-1.0], [+1.0]], [[0.0], [1.0]], [[0.0]], [[0.0], [1.0]]]
    expected_value_for_probes = [
        [[1.0]],
        [[-1.0], [+1.0]],
        [[args.gamma], [1.0]],
        [[-1.0, 1.0]],
        [[1.0, -1.0], [-1.0, 1.0]],
    ]
    tolerances = [5e-4, 5e-4, 5e-4, 5e-4, 1e-3]
    obs = t.tensor(obs_for_probes[probe_idx - 1]).to(device)

    # Calculate the actual value, and verify it
    value = trainer.q_network(obs)
    expected_value = t.tensor(expected_value_for_probes[probe_idx - 1]).to(device)
    t.testing.assert_close(value, expected_value, atol=tolerances[probe_idx - 1], rtol=0)
    print("Probe tests passed!\n")


if MAIN:
    for probe_idx in range(1, 6):
        test_probe(probe_idx)

# %%

if MAIN:
    args = DQNArgs(use_wandb=True, capture_video_every_n_episodes=100)
    trainer = DQNTrainer(args)
    trainer.train()

# %%

if MAIN:
    # %pip install "gymnasium[atari, accept-rom-license, other]==0.29.0"

    env = gym.make("ALE/Breakout-v5", render_mode="rgb_array")

    print(env.action_space)
    print(env.observation_space)

    # Docs here: https://ale.farama.org/environments/breakout/

# %%


def display_frames(frames: Int[Arr, "timesteps height width channels"], figsize=(4, 5)):
    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(frames[0])
    plt.close()

    def update(frame):
        im.set_array(frame)
        return [im]

    ani = FuncAnimation(fig, update, frames=frames, interval=100)
    display(HTML(ani.to_jshtml()))


if MAIN:
    nsteps = 150

    frames = []
    obs, info = env.reset()
    for _ in tqdm(range(nsteps)):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        frames.append(obs)

    display_frames(np.stack(frames))

# %%

if MAIN:
    env_wrapped = gym.wrappers.AtariPreprocessing(env, frame_skip=1)
    env_wrapped = gym.wrappers.FrameStack(env_wrapped, num_stack=4)

    frames = []
    obs, info = env_wrapped.reset()
    for _ in tqdm(range(nsteps)):
        action = env_wrapped.action_space.sample()
        obs, reward, terminated, truncated, info = env_wrapped.step(action)
        obs = einops.repeat(np.array(obs), "frames h w -> h (frames w) 3")  # stack frames across the row
        frames.append(obs)

    display_frames(np.stack(frames), figsize=(8, 4))

# %%


def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):
    t.nn.init.orthogonal_(layer.weight, std)
    t.nn.init.constant_(layer.bias, bias_const)
    return layer


class AtariQNetwork(nn.Module):
    """
    Q-Network for Atari, following the architecture pattern in the Actor-Critic setup.
    """

    def __init__(self, obs_shape: tuple[int], num_actions: int):
        super().__init__()
        assert obs_shape[-1] % 8 == 4
        L_after_convolutions = (obs_shape[-1] // 8) - 3
        in_features = 64 * L_after_convolutions * L_after_convolutions

        self.layers = nn.Sequential(
            layer_init(nn.Conv2d(4, 32, 8, stride=4, padding=0)),
            nn.ReLU(),
            layer_init(nn.Conv2d(32, 64, 4, stride=2, padding=0)),
            nn.ReLU(),
            layer_init(nn.Conv2d(64, 64, 3, stride=1, padding=0)),
            nn.ReLU(),
            nn.Flatten(),
            layer_init(nn.Linear(in_features, 512)),
            nn.ReLU(),
            layer_init(nn.Linear(512, num_actions), std=0.01),
        )

    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)


if MAIN:
    atari_q_network = AtariQNetwork(env_wrapped.observation_space.shape, env_wrapped.action_space.n)
    print(atari_q_network)

# %%

if MAIN:
    # Note: you need `self.obs = np.array(self.obs, dtype=np.float32)`, because otherwise obs is cast as int
    # (or just convert to int type in your model fwd pass, although since you're adding to the buffer which presupposes float type that's kinda jank)

    # ! conclusion - don't do Atari today, have it as a bonus exercise. I think it's still true that you can move on early from yesterday, also this doesn't matter as much as I'm making it out to - get goinggg!

    args = DQNArgs(
        use_wandb=True,
        env_id="ALE/Breakout-v5",
        mode="atari",
        capture_video_every_n_episodes=50,
        buffer_size=1_000,
    )
    trainer = DQNTrainer(args)
    trainer.train()

    # https://wandb.ai/callum-mcdougall/CartPoleDQN/runs/jamrr834

# %%
