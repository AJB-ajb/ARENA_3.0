# %%


import time
from dataclasses import dataclass
from pathlib import Path

import gymnasium as gym
import numpy as np
import torch as t
import wandb
from jaxtyping import Bool, Float, Int
from torch import Tensor, nn
from tqdm import tqdm

Arr = np.ndarray


device = t.device("cuda" if t.cuda.is_available() else "cpu")

section_dir = Path(__file__).parent


def make_env(
    env_id: str,
    seed: int,
    idx: int,
    run_name: str,
    capture_video_every_n_episodes: int = None,
    video_save_path: str = None,
    **kwargs,
):
    """
    Return a function that returns an environment after setting up boilerplate.
    """

    def thunk():
        env = gym.make(env_id, render_mode="rgb_array")
        env = gym.wrappers.RecordEpisodeStatistics(env)
        if idx == 0 and capture_video_every_n_episodes:
            env = gym.wrappers.RecordVideo(
                env,
                f"{video_save_path}/{run_name}",
                episode_trigger=lambda episode_id: episode_id % capture_video_every_n_episodes == 0,
                disable_logger=True,
            )

        env = prepare_atari_env(env)

        env.reset(seed=seed)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
        return env

    return thunk


def prepare_atari_env(env: gym.Env):
    """Note, ALE/Breakout-v5 has default frameskip=4, see https://ale.farama.org/environments/breakout/"""
    env = gym.wrappers.AtariPreprocessing(env, frame_skip=1)
    env = gym.wrappers.FrameStack(env, num_stack=4)
    return env


def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):
    t.nn.init.orthogonal_(layer.weight, std)
    t.nn.init.constant_(layer.bias, bias_const)
    return layer


class AtariQNetwork(nn.Module):
    """
    Q-Network for Atari, following the architecture pattern in the Actor-Critic setup.
    """

    def __init__(self, obs_shape: tuple[int], num_actions: int):
        super().__init__()
        assert obs_shape[-1] % 8 == 4
        L_after_convolutions = (obs_shape[-1] // 8) - 3
        in_features = 64 * L_after_convolutions * L_after_convolutions

        self.layers = nn.Sequential(
            layer_init(nn.Conv2d(4, 32, 8, stride=4, padding=0)),
            nn.ReLU(),
            layer_init(nn.Conv2d(32, 64, 4, stride=2, padding=0)),
            nn.ReLU(),
            layer_init(nn.Conv2d(64, 64, 3, stride=1, padding=0)),
            nn.ReLU(),
            nn.Flatten(),
            layer_init(nn.Linear(in_features, 512)),
            nn.ReLU(),
            layer_init(nn.Linear(512, num_actions), std=0.01),
        )

    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)


@dataclass
class ReplayBufferSamples:
    """
    Samples from the replay buffer, converted to PyTorch for use in neural network training.

    Data is equivalent to (s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1}). Note - here, d_{t+1} is actually **terminated** rather
    than **done** (i.e. it records the times when we went out of bounds, not when the environment timed out).
    """

    obs: Float[Tensor, "sample_size *obs_shape"]
    actions: Float[Tensor, "sample_size *action_shape"]
    rewards: Float[Tensor, "sample_size"]
    terminated: Bool[Tensor, "sample_size"]
    next_obs: Float[Tensor, "sample_size *obs_shape"]


class ReplayBuffer:
    """
    Contains buffer; has a method to sample from it to return a ReplayBufferSamples object.
    """

    rng: np.random.Generator
    obs: Float[Arr, "buffer_size *obs_shape"]
    actions: Float[Arr, "buffer_size *action_shape"]
    rewards: Float[Arr, "buffer_size"]
    terminated: Bool[Arr, "buffer_size"]
    next_obs: Float[Arr, "buffer_size *obs_shape"]

    def __init__(
        self, num_environments: int, obs_shape: tuple[int], action_shape: tuple[int], buffer_size: int, seed: int
    ):
        self.num_environments = num_environments
        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.buffer_size = buffer_size
        self.rng = np.random.default_rng(seed)

        self.obs = np.empty((0, *self.obs_shape), dtype=np.float32)
        self.actions = np.empty((0, *self.action_shape), dtype=np.int32)
        self.rewards = np.empty(0, dtype=np.float32)
        self.terminated = np.empty(0, dtype=bool)
        self.next_obs = np.empty((0, *self.obs_shape), dtype=np.float32)

    def add(
        self,
        obs: Float[Arr, "num_envs *obs_shape"],
        actions: Int[Arr, "num_envs *action_shape"],
        rewards: Float[Arr, "num_envs"],
        terminated: Bool[Arr, "num_envs"],
        next_obs: Float[Arr, "num_envs *obs_shape"],
    ) -> None:
        """
        Add a batch of transitions to the replay buffer.
        """
        self.obs = np.concatenate((self.obs, obs))[-self.buffer_size :]
        self.actions = np.concatenate((self.actions, actions))[-self.buffer_size :]
        self.rewards = np.concatenate((self.rewards, rewards))[-self.buffer_size :]
        self.terminated = np.concatenate((self.terminated, terminated))[-self.buffer_size :]
        self.next_obs = np.concatenate((self.next_obs, next_obs))[-self.buffer_size :]

    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:
        """
        Sample a batch of transitions from the buffer, with replacement.
        """
        indices = self.rng.integers(0, self.buffer_size, sample_size)

        return ReplayBufferSamples(
            *[
                t.tensor(x[indices], device=device)
                for x in [self.obs, self.actions, self.rewards, self.terminated, self.next_obs]
            ]
        )


def linear_schedule(
    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int
) -> float:
    """Return the appropriate epsilon for the current step.

    Epsilon should be start_e at step 0 and decrease linearly to end_e at step (exploration_fraction * total_timesteps).
    In other words, we are in "explore mode" with start_e >= epsilon >= end_e for the first `exploration_fraction` fraction
    of total timesteps, and then stay at end_e for the rest of the episode.
    """
    return start_e + (end_e - start_e) * min(current_step / (exploration_fraction * total_timesteps), 1)


def epsilon_greedy_policy(
    envs: gym.vector.SyncVectorEnv,
    q_network: AtariQNetwork,
    rng: np.random.Generator,
    obs: Float[Arr, "num_envs *obs_shape"],
    epsilon: float,
) -> Int[Arr, "num_envs *action_shape"]:
    """With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.
    Inputs:
        envs:       The family of environments to run against
        q_network:  The AtariQNetwork used to approximate the Q-value function
        obs:        The current observation for each environment
        epsilon:    The probability of taking a random action
    Outputs:
        actions:    The sampled action for each environment.
    """
    # Convert `obs` into a tensor so we can feed it into our model
    obs = t.from_numpy(obs).to(device)

    num_actions = envs.single_action_space.n
    if rng.random() < epsilon:
        return rng.integers(0, num_actions, size=(envs.num_envs,))
    else:
        q_scores = q_network(obs)
        return q_scores.argmax(-1).detach().cpu().numpy()


@dataclass
class DQNArgs:
    # Basic / global
    seed: int = 1
    cuda: bool = t.cuda.is_available()
    env_id: str = "ALE/Breakout-v5"
    num_envs: int = 4

    # Wandb / logging
    use_wandb: bool = True
    capture_video_every_n_episodes: int | None = 20
    exp_name: str = "AtariDQN"
    wandb_entity: str | None = None

    # Duration of different phases
    buffer_size: int = 5_000
    train_frequency: int = 10
    total_timesteps: int = 100_000
    target_network_frequency: int = 100

    # Optimization hyperparameters
    batch_size: int = 32
    learning_rate: float = 2.5e-4
    start_e: float = 1.0
    end_e: float = 0.1

    # Misc. RL related
    gamma: float = 0.99
    exploration_fraction: float = 0.1

    def __post_init__(self):
        assert self.total_timesteps - self.buffer_size >= self.train_frequency
        self.total_training_steps = (self.total_timesteps - self.buffer_size) // self.train_frequency
        self.capture_video = self.capture_video_every_n_episodes is not None


class DQNAgent:
    """Base Agent class handling the interaction with the environment."""

    def __init__(
        self,
        envs: gym.vector.SyncVectorEnv,
        args: DQNArgs,
        buffer: ReplayBuffer,
        q_network: AtariQNetwork,
        target_network: AtariQNetwork,
        rng: np.random.Generator,
    ):
        self.envs = envs
        self.args = args
        self.buffer = buffer

        self.obs, _ = self.envs.reset()  # Need a starting observation!

        self.step = 0
        self.epsilon = args.start_e
        self.q_network = q_network
        self.target_network = target_network
        self.rng = rng

    def play_step(self) -> list[dict]:
        """
        Carries out a single interaction step between agent & environment, and adds results to the replay buffer.

        Returns `infos` (list of dictionaries containing info we will log).
        """
        self.obs = np.array(self.obs, dtype=np.float32)
        actions = self.get_actions(self.obs)
        next_obs, rewards, terminated, truncated, infos = self.envs.step(actions)

        # Get `real_next_obs` by finding all environments where we terminated & replacing `next_obs` with the actual terminal states
        true_next_obs = next_obs.copy()
        for n in range(self.envs.num_envs):
            if (terminated | truncated)[n]:
                true_next_obs[n] = infos["final_observation"][n]

        self.buffer.add(self.obs, actions, rewards, terminated, true_next_obs)
        self.obs = next_obs
        self.step += 1
        return infos

    def get_actions(self, obs: np.ndarray) -> np.ndarray:
        """
        Samples actions according to the epsilon-greedy policy using the linear schedule for epsilon.
        """
        self.epsilon = linear_schedule(
            self.step, self.args.start_e, self.args.end_e, self.args.exploration_fraction, self.args.total_timesteps
        )
        actions = epsilon_greedy_policy(self.envs, self.q_network, self.rng, obs, self.epsilon)
        assert actions.shape == (len(self.envs.envs),)
        return actions


class DQNTrainer:
    def __init__(self, args: DQNArgs):
        self.args = args
        self.run_name = f"{args.env_id}__{args.exp_name}__seed{args.seed}__{time.strftime('%Y%m%d-%H%M%S')}"
        self.envs = gym.vector.SyncVectorEnv(
            [
                make_env(idx=idx, run_name=self.run_name, video_save_path=section_dir / "videos", **args.__dict__)
                for idx in range(args.num_envs)
            ]
        )
        self.start_time = time.time()
        self.rng = np.random.default_rng(args.seed)

        # Get obs & action shapes (we assume we're dealing with a single discrete action)
        num_actions = self.envs.single_action_space.n
        action_shape = ()
        obs_shape = self.envs.single_observation_space.shape

        self.q_network = AtariQNetwork(obs_shape, num_actions).to(device)  # type: ignore
        self.target_network = AtariQNetwork(obs_shape, num_actions).to(device)  # type: ignore

        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = t.optim.AdamW(self.q_network.parameters(), lr=args.learning_rate)

        self.buffer = ReplayBuffer(len(self.envs.envs), obs_shape, action_shape, args.buffer_size, args.seed)
        self.agent = DQNAgent(self.envs, self.args, self.buffer, self.q_network, self.target_network, self.rng)

    def add_to_replay_buffer(self, n: int, verbose: bool = False):
        """
        Takes n steps with the agent, adding to the replay buffer (and logging any results). Should return the last
        episode length from the infos dict if an episode has terminated at some point in these steps.

        Optional argument `verbose`: if True, we can use a progress bar (useful to check how long the initial buffer
        filling is taking).
        """
        last_episode_len = None
        for step in tqdm(range(n), disable=not verbose, desc="Adding to replay buffer"):
            infos = self.agent.play_step()

            # Get length of last episode, by taking it from the first of our environments that has terminated (if any)
            if "final_info" in infos and any(info is not None for info in infos["final_info"]):
                final_info = next(info for info in infos["final_info"] if info is not None)
                last_episode_len = final_info["episode"]["l"].item()
                if self.args.use_wandb:
                    wandb.log({"episode_len": last_episode_len}, step=self.agent.step)

        return last_episode_len

    def training_step(self, step: int) -> Float[Tensor, ""]:
        """
        Samples once from the replay buffer, and takes a single training step. The `step` argument is used to track the
        number of training steps taken.
        """
        data = self.buffer.sample(self.args.batch_size, device)  # s_t, a_t, r_{t+1}, d_{t+1}, s_{t+1}

        with t.inference_mode():
            target_max = self.target_network(data.next_obs).max(-1).values
        predicted_q_vals = self.q_network(data.obs)[range(len(data.actions)), data.actions]

        td_error = data.rewards + self.args.gamma * target_max * (1 - data.terminated.float()) - predicted_q_vals
        loss = td_error.pow(2).mean()
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

        if step % self.args.target_network_frequency == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        if self.args.use_wandb:
            wandb.log(
                {
                    "td_loss": loss,
                    "q_values": predicted_q_vals.mean().item(),
                    "SPS": int(self.agent.step / (time.time() - self.start_time)),
                    "epsilon": self.agent.epsilon,
                },
                step=self.agent.step,
            )
        # SOLUTION END

    def train(self) -> None:
        if self.args.use_wandb:
            wandb.init(
                project=self.args.exp_name,
                entity=self.args.wandb_entity,
                name=self.run_name,
                monitor_gym=self.args.capture_video,
            )
            wandb.watch(self.q_network, log="all", log_freq=20)

        self.add_to_replay_buffer(self.args.buffer_size // self.args.num_envs, verbose=True)

        pbar = tqdm(range(self.args.total_training_steps))
        last_logged_time = time.time()  # helps us not update the progress bar too much

        for step in pbar:
            last_episode_len = self.add_to_replay_buffer(self.args.train_frequency // self.args.num_envs)

            if (last_episode_len is not None) and (time.time() - last_logged_time > 1):
                last_logged_time = time.time()
                pbar.set_postfix(step=self.agent.step, last_episode_len=last_episode_len)

            self.training_step(step)

        if self.args.use_wandb:
            wandb.finish()


# wandb.finish()

if __name__ == "__main__":
    args = DQNArgs()
    trainer = DQNTrainer(args)
    trainer.train()
