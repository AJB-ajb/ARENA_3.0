import json
import plotly.io as pio
import re
import streamlit as st
import sys
import os
from pathlib import Path

from streamlit.components.v1 import html as st_html

# __file__ is instructions/pages/sec5_gans_and_vaes/sec52_vaes.py
# instructions_dir is instructions/
instructions_dir = Path(__file__).parent.parent.parent.resolve()
if str(instructions_dir) not in sys.path: sys.path.append(str(instructions_dir))
os.chdir(instructions_dir)


# def read_from_html(filename):
#     with open(instructions_dir / "pages" / "sec5_gans_and_vaes" / f"{filename}.html") as f:
#         html = f.read()
#     call_arg_str = re.findall(r'Plotly\.newPlot\((.*)\)', html)[0]
#     call_args = json.loads(f'[{call_arg_str}]')
#     try:
#         plotly_json = {'data': call_args[1], 'layout': call_args[2]}
#         fig = pio.from_json(json.dumps(plotly_json))
#     except:
#         del call_args[2]["template"]["data"]["scatter"][0]["fillpattern"]
#         plotly_json = {'data': call_args[1], 'layout': call_args[2]}
#         fig = pio.from_json(json.dumps(plotly_json))
#     return fig


# for k in ["vae_interp", "autoencoder_interpolation_2", "autoencoder_interpolation"]:
#     if k not in st.session_state:
#         st.session_state[k] = read_from_html(k)


def section():
    st.sidebar.markdown(
r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class="contents-el" href="#reading">Reading</a></li>
    <li class='margtop'><a class="contents-el" href="#autoencoders">Autoencoders</a></li>
    <li class='margtop'><ul class="contents">
        <li><a class="contents-el" href="#encoder">Encoder</a></li>
        <li><a class="contents-el" href="#decoder">Decoder</a></li>
        <li><a class="contents-el" href="#decoder"><b>Exercise</b> - implement autoencoder</a></li>
        <li><a class="contents-el" href="#training-loop">Training loop</a></li>
        <li><a class="contents-el" href="#generating-images-from-an-encoder">Generating images from an encoder</a></li>
        <li><a class="contents-el" href="#exercise-plot-your-embeddings-optional">Exercise - plot your embeddings (optional)</a></li>
    </li></ul>
    <li class='margtop'><a class="contents-el" href="#variational-autoencoders">Variational Autoencoders</a></li>
    <li class='margtop'><ul class="contents">
        <li><a class="contents-el" href="#reparameterisation-trick">Reparameterisation trick</a></li>
    </li></ul>
    <li class='margtop'><a class="contents-el" href="#building-a-vae">Building a VAE</a></li>
    <li class='margtop'><ul class="contents">
        <li><a class="contents-el" href="#probabilistic-encoder">Probabilistic encoder</a></li>
        <li><a class="contents-el" href="#new-loss-function">New loss function</a></li>
        <li><a class="contents-el" href="#training-loop">Training loop</a></li>
    </li></ul>
    <li class='margtop'><a class="contents-el" href="#a-deeper-dive-into-the-maths-of-vaes">A deeper dive into the maths of VAEs</a></li>
    <li class='margtop'><a class="contents-el" href="#bonus-exercises">Bonus exercises</a></li>
    <li class='margtop'><ul class="contents">
        <li><a class="contents-el" href="#beta-vaes">Beta VAEs</a></li>
        <li><a class="contents-el" href="#celeba-dataset">CelebA dataset</a></li>
        <li><a class="contents-el" href="#hierarchical-vaes">Hierarchical VAEs</a></li>
        <li><a class="contents-el" href="#denoising-and-sparse-autoencoders">Denoising and sparse autoencoders</a></li>
    </li></ul>
</ul>
""", unsafe_allow_html=True)

    st.markdown(
r"""
# Autoencoders & VAEs

> ### Learning Objectives
>
> * Understand the basic architecture of autoencoders and VAEs
> * Learn about the reparameterization trick for VAEs
> * Implement your own autoencoder
> * Implement your own VAE, and use it to generate realistic MNIST images
> * (optional) Dive deeper into the mathematical underpinnings of VAEs, and learn about the ELBO loss function

## Reading

* [From Autoencoder to Beta-VAE](https://lilianweng.github.io/posts/2018-08-12-vae/)
    * This is a very good introduction to the basic mechanism of autoencoders and VAEs. 
    * Read up to the section on the reparameterization trick (inclusive); the rest is optimal.
    * Don't worry if you don't follow all the maths; we'll go through some of it below.
* [Six (and a half) intuitions for KL divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence)
    * As the architectures we look at in this chapter get more heavily mathematical, it will become important to have good intuitions for the basics of information theory. In particular, KL divergence will show up a **lot**!

## Autoencoders

We'll start by looking at **Autoencoders**, which are much conceptually simpler than VAEs. These are simply systems which learn a compressed representation of the input, and then reconstruct it. There are two parts to this:

* The **encoder** learns to compress the output into a latent space which is lower-dimensional than the original image.
* The **decoder** learns to uncompress the encoder's output back into a faithful representation of the original image.

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/ae-diagram-dark.png" width="700">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/ae-diagram-light.png" width="700">
</div>
                
Our loss function is simply some metric of the distance between the input and the reconstructed input, e.g. the $l_2$ loss.

You'll start by writing your own autoencoder. This should be a lot like the GANs exercise in the previous section. We've given some guidance on architecture below.

*Note - for the rest of this section (not including the bonus), we'll assume we're working with the MNIST dataset rather than Celeb-A.*

### Encoder

Your encoder should consist of two convolutional layers, a flatten, then two linear layers. After each convolution or linear layer (except for the last one) you should have an activation function (using ReLU is fine, although feel free to experiment with others). The convolutions should have kernel size 4, stride 2, padding 1 (recall from yesterday that this exactly halves the width and height). The number of output channels can be 16 and 32 respectively.

After the convolutional layers, you flatten then apply linear layers. Your flattened size will be $32 \times 7 \times 7$. Your first linear layer should have `out_features=128`; the second is up to you (we recommend playing around with values between 2 and 10).

### Decoder

Again, your decoder should be a mirror image of your encoder. Reverse the order of all the layers, and replace the convolutions with transposed convolutions. Your transposed convolutions should have kernel size 4, stride 2, and padding 1 (this results in a doubling of the height and width of the image, so you can use the same shapes for the linear layers as for the encoder, but in reverse).

### Exercise - implement autoencoder

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 10-15 minutes on this exercise.
```

```python
class Autoencoder(nn.Module):

    def __init__(self, latent_dim_size: int, hidden_dim_size: int):
        super().__init__()
        pass

    def forward(self, x: t.Tensor) -> t.Tensor:
        pass
        

tests.test_autoencoder(Autoencoder)
```

<details>
<summary>Solution</summary>

```python
class Autoencoder(nn.Module):

    def __init__(self, latent_dim_size: int, hidden_dim_size: int):
        super().__init__()
        self.latent_dim_size = latent_dim_size
        self.hidden_dim_size = hidden_dim_size
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(7 * 7 * 32, hidden_dim_size),
            nn.ReLU(),
            nn.Linear(hidden_dim_size, latent_dim_size)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim_size, hidden_dim_size),
            nn.ReLU(),
            nn.Linear(hidden_dim_size, 7 * 7 * 32),
            nn.ReLU(),
            Rearrange("b (c h w) -> b c w h", c=32, h=7, w=7),
            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),
        )

    def forward(self, x: t.Tensor) -> t.Tensor:
        z = self.encoder(x)
        x_prime = self.decoder(z)
        return x_prime
```
</details>

Once you've done this, you should write a training loop which works with [MSE loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) between the original and reconstructed data. The standard Adam optimiser with default parameters should suffice.

Much like for the generator, you will find it helpful to display output while your model is training. There are actually two ways you can do this:

1. Take an MNIST image and feed it into your autoencoder - your result should look similar to the input.
2. Take a random vector in the latent space, and run just the decoder on it in order to produce an output (this is what we did for our GAN).

The problem with (2) (which we'll discuss more when we get to VAEs) is that your latent space might not actually be meaningful. In other words, it's unclear exactly how to sample from it to get output which will look like an MNIST image. For that reason, you're recommended to do (1) instead. You should also do standard things like printing out the loss as your model trains (or logging the result to Weights and Biases).

The code below creates a tensor `HOLDOUT_DATA` of shape `(10, 1, 28, 28)`, consisting of one of each kind of digit (we have the 1 dimension because your autoencoder is expecting input with channel dimension). You can use this to produce & display output in Weights and Biases.

```python
testset = get_dataset("MNIST", train=False)
HOLDOUT_DATA = dict()
for data, target in DataLoader(testset, batch_size=1):
    if target.item() not in HOLDOUT_DATA:
        HOLDOUT_DATA[target.item()] = data.squeeze()
        if len(HOLDOUT_DATA) == 10: break
HOLDOUT_DATA = t.stack([HOLDOUT_DATA[i] for i in range(10)]).to(dtype=t.float, device=device).unsqueeze(1)
```

### Exercise - write an autoencoder training loop

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 15-25 minutes on this exercise. It should recycle a lot of previous code.
```

Again, we've given you a template for the trainer class.

```python
@dataclass
class AutoencoderArgs():
    latent_dim_size: int = 5
    hidden_dim_size: int = 128
    batch_size: int = 512
    epochs: int = 10
    lr: float = 1e-3
    betas: Tuple[float] = (0.5, 0.999)
    seconds_between_eval: int = 5
    wandb_project: Optional[str] = 'day5-ae-mnist'
    wandb_name: Optional[str] = None


class AutoencoderTrainer:
    def __init__(self, args: AutoencoderArgs):
        self.args = args
        self.trainset = get_dataset("MNIST")
        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True)
        self.model = Autoencoder(
            latent_dim_size = args.latent_dim_size,
            hidden_dim_size = args.hidden_dim_size,
        ).to(device)
        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)

    def training_step(self, img: t.Tensor, label: t.Tensor):
        '''
        Performs a training step on the batch of images in `img`. Returns the loss.
        '''
        pass

    def train(self) -> None:
        '''
        Performs a full training run, logging to wandb.
        '''
        pass


args = AutoencoderArgs()
trainer = AutoencoderTrainer(args)
trainer.train()
```

<details>
<summary>Solution (one possible implementation)</summary>

```python
class AutoencoderTrainer:
    def __init__(self, args: AutoencoderArgs):
        self.args = args
        self.trainset = get_dataset("MNIST")
        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True)
        self.model = Autoencoder(
            latent_dim_size = args.latent_dim_size,
            hidden_dim_size = args.hidden_dim_size,
        ).to(device)
        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)

    def training_step(self, img: t.Tensor, label: t.Tensor):
        '''
        Performs a training step on the batch of images in `img`. Returns the loss.
        '''
        img = img.to(device)
        img_reconstructed = self.model(img)
        loss = nn.MSELoss()(img, img_reconstructed)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        return loss

    def train(self) -> None:
        '''
        Performs a full training run, logging to wandb.
        '''
        step = 0
        last_log_time = time.time()
        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)
        wandb.watch(self.model)

        for epoch in range(self.args.epochs):

            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)))

            for i, (img, label) in enumerate(progress_bar): # remember that label is not used
                
                loss = self.training_step(img, label)
                wandb.log(dict(loss=loss), step=step)
                step += img.shape[0]

                # Update progress bar
                progress_bar.set_description(f"{epoch=}, {loss=:.4f}, examples_seen={step}")

                # Evaluate model on the same holdout data
                if time.time() - last_log_time > self.args.seconds_between_eval:
                    last_log_time = time.time()
                    with t.inference_mode():
                        arrays = self.model(HOLDOUT_DATA).cpu().numpy()
                    wandb.log({"images": [wandb.Image(arr) for arr in arrays]}, step=step)

        wandb.finish()


args = AutoencoderArgs()
trainer = AutoencoderTrainer(args)
trainer.train()
```
</details>

After ten epochs, you should be able to get output of the following calibre:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/autoencoder_2.png" width="700">

This is a pretty faithful representation. Note how it's mixing up features for some of the numbers - for instance, the 5 seems to have been partly reproduced as a 9. But overall, it seems pretty accurate!

## Generating images from an encoder

We'll now return to the issue we mentioned briefly earlier - how to generate output? This was easy for our GAN; the only way we ever produced output was by putting random noise into the generator. But how should we interpret the latent space between our encoder and decoder?

We can try and plot the outputs produced by the decoder over a range, e.g. using code like this (the details might vary slightly for your model depending on how you defined your layers):

```python
# Choose number of interpolation points, and interpolation range
n_points = 11
interpolation_range = (-3, 3)

# Constructing latent dim data by making two of the dimensions vary independently between 0 and 1
latent_dim_data = t.zeros((n_points, n_points, args.latent_dim_size), device=device)
x = t.linspace(*interpolation_range, n_points)
latent_dim_data[:, :, 0] = x.unsqueeze(0)
latent_dim_data[:, :, 1] = x.unsqueeze(1)
# Rearranging so we have a single batch dimension
latent_dim_data = einops.rearrange(latent_dim_data, "b1 b2 latent_dim -> (b1 b2) latent_dim")

# Getting model output, and normalising & truncating it in the range [0, 1]
output = trainer.model.decoder(latent_dim_data).detach().cpu().numpy()
output_truncated = np.clip((output * 0.3081) + 0.1307, 0, 1)
output_single_image = einops.rearrange(output_truncated, "(b1 b2) 1 height width -> (b1 height) (b2 width)", b1=n_points)

# Plotting results
fig = px.imshow(output_single_image, color_continuous_scale="greys_r")
fig.update_layout(
    title_text="Decoder output from varying first two latent space dims", title_x=0.5,
    coloraxis_showscale=False, 
    xaxis=dict(title_text="x0", tickmode="array", tickvals=list(range(14, 14+28*n_points, 28)), ticktext=[f"{i:.2f}" for i in x]),
    yaxis=dict(title_text="x1", tickmode="array", tickvals=list(range(14, 14+28*n_points, 28)), ticktext=[f"{i:.2f}" for i in x])
)
fig.show()
```

This generates images from a vector in the latent space which has all elements set to zero except the first two. The output should look like:
""", unsafe_allow_html=True)

    # st.plotly_chart(st.session_state["autoencoder_interpolation_2"], use_container_width=True)
    with open(instructions_dir / "pages/sec5_gans_and_vaes/autoencoder_interpolation_2.html") as f:
        st_html(f.read(), height=480)

    st.markdown(
r"""
This is ... pretty underwhelming actually. Although some of these shapes seem legible (e.g. the bottom-right look like 9s, and some of the right images look like 7s), much of the space doesn't look like any recognisable number.

Why is this? Well unfortunately, the model has no reason to treat the latent space in any meaningful way. It might be the case that almost all the images are embedded into a particular subspace of the latent space, and so the encoder only gets trained on inputs in this subspace. You can explore this idea further in the exercise below.

### Exercise - plot your embeddings (optional)

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µâšªâšªâšª

You should spend up to 10-20 minutes on this exercise.
```

The output above generated images from embeddings in the latent space, using `model.decoder`. Now, you should try and do the opposite - feed MNIST data into your encoder, and plot its embedding projected along the first two dimensions of the latent space.

Before you do this, think about what you expect to see in this plot, based on the comments above. A few questions you might want to ask yourself:

* Do you expect the entire space to be utilised, or will the density of points be uneven?
* You can color-code the points of your scatter graph according to their true label. Do you expect same-colored points to be clustered together?

Note - you might see very different results depending on how many dimensions there are in your latent space.

```python
# YOUR PLOTTING CODE HERE
```

You can use the dropdown to see some example plotting code, produced with the Plotly library.

> Note that this code was generated with ChatGPT - using it to quickly generate good visualisations according to some description you feed it is often a very good use of time.
""", unsafe_allow_html=True)
    
    with st.expander("Click here to get code for plotting your embeddings, and what you should see."):
        st.markdown(
r"""
```python
def make_scatter_plot(model, trainset, n_examples=5000):
    trainloader = DataLoader(trainset, batch_size=64)
    df_list = []
    device = next(model.parameters()).device
    for img, label in trainloader:
        output = model.encoder(img.to(device)).detach().cpu().numpy()
        for label_single, output_single in zip(label, output):
            df_list.append({
                "x0": output_single[0],
                "x1": output_single[1],
                "label": str(label_single.item()),
            })
        if (n_examples is not None) and (len(df_list) >= n_examples):
            break
    df = pd.DataFrame(df_list).sort_values("label")
    fig = px.scatter(df, x="x0", y="x1", color="label", template="ggplot2")
    fig.update_layout(height=1000, width=1000, title="Scatter plot of latent space dims")

    output_on_data_to_plot = model.encoder(HOLDOUT_DATA.to(device)).detach().cpu().numpy()
    data_translated = (HOLDOUT_DATA.cpu().numpy() * 0.3081) + 0.1307
    data_translated = (255 * data_translated).astype(np.uint8).squeeze()
    for i in range(10):
        img = Image.fromarray(data_translated[i]).convert("L")
        from IPython.display import display
        x = output_on_data_to_plot[i][0]
        y = output_on_data_to_plot[i][1]
        fig.add_layout_image(
            source=img,
            xref="x",
            yref="y",
            x=x,
            y=y,
            xanchor="right",
            yanchor="top",
            sizex=2,
            sizey=2,
    )
    fig.show()

    
make_scatter_plot(trainer.model, trainer.trainset)
```
""", unsafe_allow_html=True)
        with open(instructions_dir / "pages/sec5_gans_and_vaes/autoencoder_scatter.html") as f:
            st_html(f.read(), height=700)
        
    st.markdown(
r"""
## Variational Autoencoders

Variational autoencoders try and solve the problem posed by autoencoders: how to actually make the latent space meaningful, such that you can generate output by feeding a $N(0, 1)$ random vector into your model's decoder?

The key perspective shift is this: **rather than mapping the input into a fixed vector, we map it into a distribution**. The way we learn a distribution is very similar to the way we learn our fixed inputs for the autoencoder, i.e. we have a bunch of linear or convolutional layers, our input is the original image, and our output is the tuple of parameters $(\mu(\boldsymbol{x}), \Sigma(\boldsymbol{x}))$ (as a trivial example, our VAE learning a distribution $\mu(\boldsymbol{x})=z(\boldsymbol{x})$, $\Sigma(\boldsymbol{x})=0$ is equivalent to our autoencoder learning the function $z(\boldsymbol{x})$ as its encoder).

From this [TowardsDataScience](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) article:

> Due to overfitting, the latent space of an autoencoder can be extremely irregular (close points in latent space can give very *different* decoded data, some point of the latent space can give *meaningless* content once decoded) and, so, we canâ€™t really define a *generative* process that simply consists to sample a point from the *latent space* and make it go through the decoder to get new data. *Variational autoencoders* (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a *distribution over the latent space* instead of a single point and by adding in the loss function a *regularisation* term over that returned distribution in order to ensure a better *organisation* of the latent space.

Or, in fewer words:

> **A variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.**

At first, this idea of mapping to a distribution sounds like a crazy hack - why on earth does it work? This diagram should help convey some of the intuitions:

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-scatter-dark.png" width="800">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-scatter-light.png" width="800">
</div>

With our encoder, there was nothing incentivising us to make full and meaningful use of the latent space. It's hypothetically possible that our network was mapping all the inputs to some very small subspace and reconstructing them with perfect fidelity. This wouldn't have required numbers with different features to be far apart from each other in the latent space, because even if they are close together no information is lost. See the first image above.

But with our variational autoencoder, each MNIST image produces a **sample** from the latent space, with a certain mean and variance. This means that, when two numbers look very different, their latent vectors are forced apart - if the means were close together then the decoder wouldn't be able to reconstruct them.

Another nice property of using random latent vectors is that the entire latent space will be meaningful. For instance, there is no reason why we should expect the linear interpolation between two points in the latent space to have meaningful decodings. The decoder output *will* change continuously as we continuously vary the latent vector, but that's about all we can say about it. However, if we use a variational autoencoder, we don't have this problem. The output of a linear interpolation between the cluster of $2$s and cluster of $7$s will be ***"a symbol which pattern-matches to the family of MNIST digits, but has equal probability to be interpreted as a $2$ or a $7$"***, and this is indeed what we find.

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-scatter-2-dark.png" width="700">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-scatter-2-light.png" width="700">
</div>

### Reparameterisation trick

One question that might have occurred to you - how can we perform backward passes through our network? We know how to differentiate with respect to the inputs to a function, but how can we differentiate wrt the parameters of a probability distribution from which we sample our vector? The solution is to convert our random sampling into a function, by introducing an extra parameter $\epsilon$. We sample $\epsilon$ from the standard normal distribution, and then express $\boldsymbol{z}$ as a deterministic function of $\mu$, $\sigma$ and $\epsilon$:

$$
z = \mu + \sigma \odot \epsilon
$$

where $\odot$ is a notation meaning pointwise product, i.e. $z_i = \mu_i + \sigma_i \epsilon_i$. Intuitively, we can think of this as follows: when there is randomness in the process that generates the output, there is also randomness in the derivative of the output wrt the input, so **we can get a value for the derivative by sampling from this random distribution**. If we average over enough samples, this will give us a valid gradient for training.

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-reparam-dark.png" width="900">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-reparam-light.png" width="900">
</div>

Note that if we have $\sigma_\theta(\boldsymbol{x})=0$ for all $\boldsymbol{x}$, the VAE reduces to an autoencoder (since the latent vector $z = \mu_\theta(\boldsymbol{x})$ is once again a deterministic function of $\boldsymbol{x}$). This is why it's important to add a KL-divergence term to the loss function, to make sure this doesn't happen. It's also why, if you print out the average value of $\sigma(\boldsymbol{x})$ while training, you'll probably see it stay below 1 (it's being pulled towards 1 by the KL-divergence loss, **and** pulled towards 0 by the reconstruction loss).

---

Before you move on to implementation details, there are a few questions below designed to test your understanding. They are based on material from this section, as well as the [KL divergence LessWrong post](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence). You might also find [this post](https://lilianweng.github.io/posts/2018-08-12-vae/#vae-variational-autoencoder) on VAEs from the readings helpful.

<details>
<summary>State in your own words why we need the reparameterization trick in order to train our network.</summary>

One sentence summary:

We can't backpropagate through random processes like $z_i \sim N(\mu_i(\boldsymbol{x}), \sigma_i(\boldsymbol{x})^2)$, but if we instead write $\boldsymbol{z}$ as a deterministic function of $\mu_i(\boldsymbol{x})$ and $\sigma_i(\boldsymbol{x})$ (plus some auxiliary random variable $\epsilon$) then we can differentiate our loss function wrt the inputs, and train our network.

Longer summary:

Our encoder works by generating parameters and then using those parameters to sample latent vectors $\boldsymbol{z}$ (i.e. a **stochastic process**). Our decoder is deterministic; it just maps our latent vectors $\boldsymbol{z}$ to fixed outputs $x'$. The stochastic part is the problem; we can't backpropagate gradients through random functions. However, instead of just writing $\boldsymbol{z} \sim N(\mu_\theta(\boldsymbol{x}), \sigma_\theta(\boldsymbol{x})^2I)$, we can write $\boldsymbol{z}$ as a deterministic function of its inputs: $z = g(\theta, x, \epsilon)$, where $\theta$ are the parameters of the distribution, $\boldsymbol{x}$ is the input, and $\epsilon$ is a randomly sampled value. We can then backpropagate through the network.
</details>

<details>
<summary>Summarize in one sentence what concept we're capturing when we measure the KL divergence D(P||Q) between two distributions.</summary>

Any of the following would work - $D(P||Q)$ is...

* How much information is lost if the distribution $Q$ is used to represent $P$.
* The quality of $Q$ as a probabilistic model for $P$ (where lower means $Q$ is a better model).
* How close $P$ and $Q$ are, with $P$ as the actual ground truth.
* How much evidence you should expect to get for hypothesis $P$ over $Q$, when $P$ is the actual ground truth.
</details>

## Building a VAE

For your final exercise of today, you'll build a VAE and run it to produce the same kind of output you did in the previous section. Luckily, this won't require much tweaking from your encoder architecture. The decoder can stay unchanged; there are just two big changes you'll need to make:

### Probabilistic encoder

Rather than your encode outputting a latent vector $\boldsymbol{z}$, it should output a mean $\mu$ and standard deviation $\sigma$; both vectors of dimension `latent_dim_size`. We then sample our latent vector $\boldsymbol{z}$ using $z_i = \mu_i + \sigma_i \cdot \epsilon_i$. 

Note that this is equivalent to $z = \mu + \Sigma \epsilon$ as shown in the diagram above, but where we assume $\Sigma$ is a diagonal matrix (i.e. the auxiliary random variables $\epsilon$ which we're sampling are independent). This is the most common approach taken in situations like these.

How exactly should this work in your model's architecture? Very simply - take the final linear layer in your embedding (which should have had `in_channels=128` and `out_channels=latent_dim_size`) and replace it with two linear layers with the same input and output sizes, one to produce `mu` and one to produce `sigma`. Then use these to produce `z` using the method above, and have that be the output of your encoder. (If you prefer, you can just have one linear layer with `out_channels=2*latent_dim_size` then use e.g. `torch.split` on the output to get `mu` and `sigma`.) One extra subtlety - your output `sigma` should be the standard deviation of your distribution, meaning it should always be positive. The easiest way to enforce this is to return `mu` and `logsigma`, then calculate `sigma = t.exp(logsigma)`.

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-guide-dark.png" width="700">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-guide-light.png" width="700">
</div>

You should also return the parameters `mu` and `logsigma` in your VAE's forward function - the reasons for this will become clear below.

### Exercise - build your VAE

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 10-15 minutes on this exercise.
```

Build your VAE. It should be identical to the autoencoder you built above, except for the changes made at the end of the encoder (outputting mean and std rather than a single latent vector; this latent vector needs to get generated via the reparameterisation trick).

As a suggested implementation, we've defined a `sample_latent_vector` method. This takes an input `x`, passes it through the encoder, and returns the latent vector `z` as well as the mean and log standard deviation of the distribution. This is useful because we'll need to use this method in our loss function, but also because it's useful to be able to generate samples from our model for evaluation purposes.

```python
class VAE(nn.Module):
    encoder: nn.Module
    decoder: nn.Module

    def __init__(self, latent_dim_size: int, hidden_dim_size: int):
        super().__init__()
        pass

    def sample_latent_vector(self, x: t.Tensor) -> Tuple[t.Tensor, t.Tensor, t.Tensor]:
        '''
        Passes `x` through the encoder. Returns the mean and log std dev of the latent vector,
        as well as the latent vector itself. This function can be used in `forward`, but also
        used on its own to generate samples for evaluation.
        '''
        ...
        return (z, mu, logsigma)

    def forward(self, x: t.Tensor) -> Tuple[t.Tensor, t.Tensor, t.Tensor]:
        '''
        Passes `x` through the encoder and decoder. Returns the reconstructed input, as well
        as mu and logsigma.
        '''
        ...
        return (x_prime, mu, logsigma)
```

Code to test your model performs a forward pass:

```python
trainset = get_dataset("MNIST")
model = VAE(latent_dim_size=5, hidden_dim_size=100).to(device)

print(torchinfo.summary(model, input_data=trainset[0][0].unsqueeze(0).to(device)))
```

You can also do the previous thing (compare your architecture to the solution), but this might be less informative if your model doesn't implement the 2-variables approach in the same way as the solution does.

<details>
<summary>Solution</summary>

```python
class VAE(nn.Module):
    encoder: nn.Module
    decoder: nn.Module

    def __init__(self, latent_dim_size: int, hidden_dim_size: int):
        super().__init__()
        self.latent_dim_size = latent_dim_size
        self.hidden_dim_size = hidden_dim_size
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(7 * 7 * 32, hidden_dim_size),
            nn.ReLU(),
            nn.Linear(hidden_dim_size, latent_dim_size*2),
            Rearrange("b (n latent_dim) -> n b latent_dim", n=2) # makes it easier to separate mu and sigma
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim_size, hidden_dim_size),
            nn.ReLU(),
            nn.Linear(hidden_dim_size, 7 * 7 * 32),
            nn.ReLU(),
            Rearrange("b (c h w) -> b c w h", c=32, h=7, w=7),
            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),
        )

    def sample_latent_vector(self, x: t.Tensor) -> Tuple[t.Tensor, t.Tensor, t.Tensor]:
        '''
        Passes `x` through the encoder. Returns the mean and log std dev of the latent vector,
        as well as the latent vector itself. This function can be used in `forward`, but also
        used on its own to generate samples for evaluation.
        '''
        mu, logsigma = self.encoder(\boldsymbol{x})
        sigma = t.exp(logsigma)
        z = mu + sigma * t.randn_like(mu)
        return z, mu, logsigma

    def forward(self, x: t.Tensor) -> Tuple[t.Tensor, t.Tensor, t.Tensor]:
        '''
        Passes `x` through the encoder and decoder. Returns the reconstructed input, as well
        as mu and logsigma.
        '''
        z, mu, logsigma = self.sample_latent_vector(\boldsymbol{x})
        x_prime = self.decoder(z)
        return x_prime, mu, logsigma
```
</details>

### New loss function

We're no longer calculating loss simply as the reconstruction error between $\boldsymbol{x}$ and our decoder's output $x'$. Recall from the **Loss Function: ELBO** section of the VAE paper that we have loss function:
$$
\begin{aligned}
L_{\mathrm{VAE}}(\theta, \phi) &=-\log p_\theta(\mathbf{x})+D_{\mathrm{KL}}\left(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p_\theta(\mathbf{z} \mid \mathbf{x})\right) \\
&=-\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} \mid \mathbf{x})} \log p_\theta(\mathbf{x} \mid \mathbf{z})+D_{\mathrm{KL}}\left(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p_\theta(\mathbf{z})\right) \\
\theta^*, \phi^* &=\arg \min _{\theta, \phi} L_{\mathrm{VAE}}
\end{aligned}
$$

Let's look at both of the two terms on the second line, and see what they actually mean.

**The first term** is playing the role of reconstruction loss. This might not be obvious at first, but notice that minimising it is equivalent to *maximising the probability of the decoder reconstructing $\boldsymbol{x}$ from $\boldsymbol{z}$, given that $\boldsymbol{z}$ was itself the encoding of input $\boldsymbol{x}$*. In fact, we can just swap this term out for any reasonable reconstruction loss (for instance, the $l_2$ loss that we used in the last section).

**The second term** is the KL divergence between $q_{\phi}(z|x)$ (the distribution of latent vectors produced by your VAE when given inputs $\boldsymbol{x}$) and $p_{\theta}(z)$ (the true generative distribution of latent vectors $\boldsymbol{z}$). Note that both of these distributions are known to us - the first is normal with mean $\mu(\boldsymbol{x})$ and variance $\sigma(\boldsymbol{x})^2$, and the second is just the standard normal distribution with mean 0, variance 1 (see [Figure 6](https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae:~:text=.-,Fig.%206.,-The%20graphical%20model) in the blog post). The KL divergence of these two distributions has a closed form expression, which is given by:
$$
D_{KL}(N(\mu, \sigma^2) || N(0, 1)) = \sigma^2 + \mu^2 - \log{\sigma} - \frac{1}{2}
$$
This is why it was important to output `mu` and `logsigma` in our forward functions, so we could compute this expression! (It's easier to use `logsigma` than `sigma` when evaluating the expression above, for stability reasons).

We won't ask you to derive this formula, because it requires understanding of **differential entropy**. However, it is worth doing some sanity checks, e.g. plot some graphs and convince yourself that this expression is larger as $N(\mu, \sigma^2)$ is further away from the standard normal distribution.

One can interpret this as the penalty term to make the latent space meaningful. If all the latent vectors $\boldsymbol{z}$ you generate have each component $z_i$ normally distributed with mean 0, variance 1 (and we know they're independent because our $\epsilon_i$ we used to generate them are independent), then there will be no gaps in your latent space where you produce weird-looking output (like we saw in our autoencoder plots from the previous section). You can try training your VAE without this term, and it might do okay at reproducing $\boldsymbol{x}$, but it will perform much worse when it comes time to use it for generation. Again, you can quantify this by encoding some input data and projecting it onto the first two dimensions. When you include this term you should expect to see a nice regular cloud surrounding the origin, but without this term you might see some irregular patterns or blind spots:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae_latent_space.png" width="700">

Once you've computed both of these loss functions, you should add them together and perform gradient descent on them.

### Beta-VAEs

The Beta-VAE is a very simple extension of the VAE, with a different loss function: we multiply the KL Divergence term by a constant $\beta$. This helps us balance the two different loss terms. For instance, I found using $\beta = 0.1$ gave better results than the default $\beta = 1$.

### Exercise - write your VAE training loop

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 15-25 minutes on this exercise.
```

You should write and run your training loop below. Again, this will involve a lot of recycled code (although there's enough different between this and the autoencoder training that we recommend you define a new class, rather than using inheritance).

```python
@dataclass
class VAEArgs(AutoencoderArgs):
    beta_kl: float = 0.1


class VAETrainer:
    def __init__(self, args: VAEArgs):
        pass

    def training_step(self, img: t.Tensor, label: t.Tensor):
        '''
        Performs a training step on the batch of images in `img`. Returns the loss.
        '''
        pass

    def train(self) -> None:
        '''
        Performs a full training run, logging to wandb.
        '''
        pass


args = VAEArgs(latent_dim_size = 10, hidden_dim_size = 100)
trainer = VAETrainer(args)
trainer.train()
```

<details>
<summary>Help - my KL divergence is close to zero, and my reconstruction loss isn't decreasing.</summary>

This is likely because your $\beta$ is too large. In this case, your model prioritizes having its latent vectors' distribution equal to the standard normal distribution.

Your model still wants to reduce the reconstruction loss, if it can find a way to do this without changing the distribution of the latent vectors. But this might not be possible, if your model has settled into a local minimum.

---

In general, when you're optimizing two different loss functions, it's important to test out different values for their weighted average. Getting the balance wrong can lead to local minima where the model prioritizes reducing one of your loss functions, and basically ignores the other one.

Weights and biases hyperparameter searches are a good tool for this.
</details>

<details>
<summary>Solution</summary>

```python
class VAETrainer:
    def __init__(self, args: VAEArgs):
        self.args = args
        self.trainset = get_dataset("MNIST")
        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True)
        self.model = VAE(
            latent_dim_size = args.latent_dim_size,
            hidden_dim_size = args.hidden_dim_size,
        ).to(device)
        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)

    def training_step(self, img: t.Tensor, label: t.Tensor):
        '''
        Performs a training step on the batch of images in `img`. Returns the loss.
        '''
        img = img.to(device)
        img_reconstructed, mu, logsigma = self.model(img)
        reconstruction_loss = nn.MSELoss()(img, img_reconstructed)
        kl_div_loss = (0.5 * (mu ** 2 + t.exp(2 * logsigma) - 1) - logsigma).mean() * args.beta_kl
        loss = reconstruction_loss + kl_div_loss
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.step += img.shape[0]
        wandb.log(dict(
            reconstruction_loss = reconstruction_loss.item(),
            kl_div_loss = kl_div_loss.item(),
            mean = mu.mean(),
            std = t.exp(logsigma).mean(),
            total_loss = loss.item(),
        ), step=self.step)
        return loss

    def train(self) -> None:
        '''
        Performs a full training run, logging to wandb.
        '''
        self.step = 0
        last_log_time = time.time()
        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)
        wandb.watch(self.model)

        for epoch in range(self.args.epochs):

            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)))

            for i, (img, label) in enumerate(progress_bar): # remember that label is not used
                
                loss = self.training_step(img, label)
                
                # Update progress bar
                progress_bar.set_description(f"{epoch=}, {loss=:.4f}, examples_seen={self.step}")

                # Evaluate model on the same holdout data
                if time.time() - last_log_time > self.args.seconds_between_eval:
                    last_log_time = time.time()
                    with t.inference_mode():
                        arrays, mu, logsigma = self.model(HOLDOUT_DATA)
                        arrays = arrays.cpu().numpy()
                    wandb.log({"images": [wandb.Image(arr) for arr in arrays]}, step=self.step)

        wandb.finish()


args = VAEArgs(latent_dim_size=5, hidden_dim_size=100)
trainer = VAETrainer(args)
trainer.train()
```
</details>

Once you've got your VAE working, you should go back through the exercises from your encoder (i.e. the ones where you produced plots). How different are they this time? Are your outputs any more or less faithful?

Here are some results from my VAE:
""", unsafe_allow_html=True)

    with open(instructions_dir / "pages/sec5_gans_and_vaes/vae_interpolation.html") as f:
        st_html(f.read(), height=480)

    st.markdown(
r"""
Note how we have a smooth continuum over all parts of the generation space! The top-left is clearly a 7, the top-right is clearly an 8, the bottom-right is a 6, and the bottom-left seems to be merging into a 0. There are some unidentifiable shapes, but these are mostly just linear interpolations between two shapes which *are* identifiable. It certainly looks much better than our autoencoder's generative output!
</details>

Note - don't be disheartened if your *reconstructions of the original MNIST images* don't look as faithful for your VAE than they did for your encoder. Remember the goal of these architectures isn't to reconstruct images faithfully, it's to generate images from samples in the latent dimension. This is the basis on which you should compare your models to each other.

## A deeper dive into the maths of VAEs

If you're happy with the loss function as described in the section above, then you can move on from here. If you'd like to take a deeper dive into the mathematical justifications of this loss function, you can read the following content. I'd consider it pretty essential in laying the groundwork for understanding diffusion models, and most kinds of generative image models (which we might dip into later in this course).

Firstly, let's flip the model we currently have on its head. Rather than having some method of sampling images $\boldsymbol{x}$ from our image set, then having a function mapping images to latent vectors $\boldsymbol{z}$, we will start with the decoder $p_\theta$ which:
* first generates the latent vector $\boldsymbol{z}$ from distribution $p(\boldsymbol{z})$ (which we assume to be the standard normal distribution $N(0, I)$),
* then generates $\boldsymbol{x}$ from the conditional distribution $p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$.

It may help to imagine $\boldsymbol{z}$ as being some kind of efficient encoding of all the information in our image distribution (e.g. if our images were celebrity faces, we could imagine the components of the vector $\boldsymbol{z}$ might correspond to features like gender, hair color, etc) - and $p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$ is some kind of probabilitistic way of reconstructing faces from this information. Note the distinction here - we're assuming our decoder is a probability distribution, rather than a deterministic function like we've used above (we'll return to this point below).

We can recover the probability distribution of $\boldsymbol{x}$ by integrating over all possible values of the latent vector $\boldsymbol{z}$:
$$
\begin{aligned}
p(\boldsymbol{x})&=\int_z p_\theta(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{z}) \\
&= \mathbb{E}_{\boldsymbol{z} \sim p(\boldsymbol{z})}\big[p_\theta(\boldsymbol{x} \mid \boldsymbol{z})\big]
\end{aligned}
$$
We can interpret this as the probability that our decoder $p_\theta(\cdot \mid \boldsymbol{z})$ produces the image $\boldsymbol{x}$ when we feed it some noise sampled from the standard normal distribution. So all we have to do is maximize the expected value of this expression over our sample of real images $x_i$, then we're training our decoder to produce images like the ones in our set of real images, right?

Unfortunately, it's not that easy. Evaluating this integral would be computationally intractible, because we would have to sample over all possible values for the latent vectors $\boldsymbol{z}$:
$$
\theta^*=\underset{\theta}{\operatorname{argmax}}\; \mathbb{E}_{x \sim \hat{p}(\boldsymbol{x}),\, \boldsymbol{z} \sim p(\boldsymbol{z})}\big[\log p_\theta(\boldsymbol{x} \mid \boldsymbol{z})\big]
$$
where $\hat{p}(\boldsymbol{x})$ denotes our distribution over samples of $\boldsymbol{x}$. This will take an exponentially long time to evaluate, because *we're having to sample $\boldsymbol{x}$ and $\boldsymbol{z}$ separately* - we'll only feasibly be able to capture a very small fraction of the latent space of $\boldsymbol{z}$.

*(Note, we've written $\log{p_\theta}$ here because it's usually easier to think about maximizing the log probability than the actual probability.)*

Imagine now that we had a function $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$, which is high when **the latent vector $\boldsymbol{z}$ is likely to have been produced by $\boldsymbol{x}$**. This function would be really helpful, because for each possible value of $\boldsymbol{x}$ we would have a better idea of where to sample $\boldsymbol{z}$ from. We can represent this situation with the following **graphical model**:

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-graphical-dark.png" width="500">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-graphical-light.png" width="500">
</div>

The seemingly intractible optimization problem above is replaced with a much easier one:
$$
\begin{aligned}
p(\boldsymbol{x}) &=\int q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \frac{p_\theta(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{z})}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})} \\
\theta^*&=\underset{\theta}{\operatorname{argmax}}\; \mathbb{E}_{\boldsymbol{x} \sim \hat{p}(\boldsymbol{x}), \boldsymbol{z} \sim q_\phi(\boldsymbol{z}\mid \boldsymbol{x})}\left[\frac{p_\theta(\boldsymbol{x} \mid \boldsymbol{z})p(\boldsymbol{z})}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\right]
\end{aligned}
$$
Note that this is exactly the same expression as before; we've just rearranged it by introducing the $q_\phi$ term, so that the distribution of $\boldsymbol{z}$ we're taking expectation over is now conditioned on $\boldsymbol{x}$.

Why is this problem easier? Because in order to estimate the quantity above, we don't need to sample a huge number of latent vectors $\boldsymbol{z}$ for each possible value of $\boldsymbol{x}$. The probability distribution $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ already concentrates most of our probability mass for where $\boldsymbol{z}$ should be, so we can sample according to this instead.

We now introduce an important quantity, called the **ELBO**, or **evidence lower-bound**. It is defined as follows (note that it is a function of $\boldsymbol{x}$, i.e. $\boldsymbol{x}$ is not a free variable):
$$
\mathbb{E}_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p_\theta(\boldsymbol{x} \mid \boldsymbol{z})p(\boldsymbol{z})}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\right]
$$

This is called the ELBO because it's a lower bound for the quantity $p(\boldsymbol{x})$, which we call the **evidence**. The proof for this being a lower bound comes from **Jensen's inequality**, which states that $\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])$ for any convex function $f$ (and $f(\boldsymbol{x})=-\log(\boldsymbol{x})$ is convex). In fact, we can prove the following identity holds:
$$
\log{p(\boldsymbol{x})}=\mathbb{E}_{\mathbb{z} \sim q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p(\boldsymbol{z}) p_\theta(\boldsymbol{x} \mid \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\right]+D_{\mathrm{KL}}\left(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p_\theta(\boldsymbol{z} \mid \boldsymbol{x})\right)
$$
So the evidence minus the ELBO is equal to the KL divergence between the distribution $q_\phi$ and the **posterior distribution** $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$ (the order of $\boldsymbol{z}$ and $\boldsymbol{x}$ have been swapped). KL divergence is always non-negative, hence the lower bound.

---

Finally, this brings us to VAEs. With VAEs, we treat $p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$ as our decoder, $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ as our encoder, and we train them jointly to minimise the ELBO (or rather the expected value of the ELBO over our dataset $\boldsymbol{x} \sim \hat{p}(\boldsymbol{x}))$. Using the previous identity, we see that maximizing the ELBO is equivalent to maximizing the following:
$$
\log{p(\boldsymbol{x})}-D_{\mathrm{KL}}\left(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p_\theta(\boldsymbol{z} \mid \boldsymbol{x})\right)
$$
In other words, we are maximizing the log-likelihood, and ***at the same time*** penalising any difference between our approximate posterior $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ and our true posterior $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$.

---

We can rewrite the ELBO in a different way:
$$
\begin{aligned}
\mathbb{E}_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\right] & =\mathbb{E}_{\mathbb{z} \sim q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z})\right]+\mathbb{E}_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\right] \\
& =\underbrace{\mathbb{E}_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z})\right]}_{\text {reconstruction loss}}-\underbrace{D_{\mathrm{KL}}\left(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \| p(\boldsymbol{z})\right)}_{\text {regularisation term}}
\end{aligned}
$$
which is now starting to look a lot more like (the negative of) the loss function we used earlier! The **regularisation term** is recognised as exactly equal to the KL divergence term in our loss function (because our encoder $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ learns a normal distribution with mean $\mu(\boldsymbol{x})$ and variance $\sigma(\boldsymbol{x})^2$, and our latent vector $\boldsymbol{z}$ has a standard normal distribution). It might not be immediately obvious why the first term serves as reconstruction loss, but in fact that is what it's doing. We can describe this term as ***"the expected log-likelihood of our decoder reconstructing $\boldsymbol{x}$ from latent vector $\boldsymbol{z}$, given that $\boldsymbol{z}$ was itself a latent vector produced by our encoder on input $\boldsymbol{x}$".*** If our model perfectly preserves $\boldsymbol{x}$ in the series of maps $\boldsymbol{x} \xrightarrow{\text{encoder}} \boldsymbol{z} \xrightarrow{\text{decoder}} \boldsymbol{x}$ then this reconstruction loss term would be zero (because the probability would be 1, and the logprob would be 0).

The decoder used in our VAE isn't actually probabilistic $p_\theta(\cdot \mid \boldsymbol{z})$, it's deterministic (i.e. it's a map from latent vector $\boldsymbol{z}$ to reconstructed input $\boldsymbol{x}'$). But we can pretend that the decoder output is actually the mean of a probability distribution, and we're choosing this mean as the value of our reconstruction $\boldsymbol{x}'$. The reconstruction loss term in the formula above will be smallest when this mean is close to the original value $\boldsymbol{x}$ (because then $p_\theta(\cdot \mid \boldsymbol{z})$ will be a probability distribution centered around $\boldsymbol{x}$). And it turns out that we can just replace this reconstruction loss with something that fulfils basically the same purpose (the $L_2$ penalty) - although we sometimes need to adjust these two terms (see $\beta$-VAEs above).

And that's the math of VAEs in a nutshell!

## Bonus exercises

### Beta-VAEs

Read the section on [Beta-VAEs](https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae), if you haven't already. Can you choose a better value for $\beta$?

To decide on an appropriate $\beta$, you can look at the distribution of your latent vector. For instance, if your latent vector looks very different to the standard normal distribution when it's projected onto one of its components (e.g. maybe that component is very sharply spiked around some particular value), this is a sign that you need to use a larger parameter $\beta$. You can also just use hyperparameter searches to find an optimal $\beta$. See [the paper](https://openreview.net/pdf?id=Sy2fzU9gl) which introduced Beta-VAEs for more ideas.

### CelebA dataset

Try to build an autoencoder for the CelebA dataset. You shouldn't need to change the architecture much from your MNIST VAE. You should find the training much easier than with your GAN (as discussed yesterday, GANs are notoriously unstable when it comes to training). Can you get better results than you did for your GAN?

### Hierarchical VAEs

Hierarchical VAEs are ones which stack multiple layers of parameter-learning and latent-vector-sampling, rather than just doing this once. Read the section of [this paper](https://arxiv.org/pdf/2208.11970.pdf) for a more thorough description.

<div class="img-dark">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-and-hvae-dark.png" width="900">
</div>
<div class="img-light">
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gan_images/vae-and-hvae-light.png" width="900">
</div>

(Note - the KL divergence loss used in HVAEs can sometimes be more complicated than the one presented in this diagram, if you want to implement conditional dependencies between the different layers. However, this isn't necessary for the basic HVAE architecture.)

Try to implement your own hierarchical VAE.

Note - when you get to the material on **diffusion models** later in the course, you might want to return here, because understanding HVAEs can be a useful step to understanding diffusion models. In fact diffusion models can almost be thought of as a special type of HVAE.

### Denoising and sparse autoencoders

The reading material on VAEs talks about [denoising](https://lilianweng.github.io/posts/2018-08-12-vae/#denoising-autoencoder) and [sparse](https://lilianweng.github.io/posts/2018-08-12-vae/#sparse-autoencoder) autoencoders. Try changing the architecture of your autoencoder (not your VAE) to test out one of these two techniques. Do does your decoder output change? How about your encoder scatter plot?

***Note - sparse autoencoders will play an important role in some later sections of this course (when we study superposition in mechanistic interpretability).***

If you're mathematically confident and feeling like a challenge, you can also try to implement [contractive autoencoders](https://lilianweng.github.io/posts/2018-08-12-vae/#contractive-autoencoder)!


""", unsafe_allow_html=True)

