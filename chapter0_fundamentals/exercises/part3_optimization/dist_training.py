"""
STRUCTURE OF THIS SECTION:

- Explain high-level ideas behind distributed training:
    - Different kind of parallelisation (data, model, etc) - take this from my ML notes directly
    - PyTorch's distributed training API
    - This section has more of a focus on practical skills - it should be possible to build up all the code here from scratch using low-level components (and we'll give you enough that you could do that if you wanted), but that's not the main point here

- How to run these exercises
    - It can't be run in a notebook, because dist training typically requires spawning multiple processes and Jupyter notebooks run in a single interactive process. They're not designed for this kind of use-case
    - You have 2 options for how to do these exercises:
        - (1) Write code in a Python file, and run from terminal i.e. `python dist_training.py` (or even setting up your own CLI using argparse: https://realpython.com/command-line-interfaces-python-argparse/)
        - (2) Write code in a Python file (optionally cell-separated), execute the code like you normally would, but lines where you actually call `mp.spawn` must be run in your interactive window, not as part of a cell (see video below)
    - In either case, any code that actually runs e.g. tests should be wrapped in `if __name__ == "__main__":`, so that it's only executed when you run the script directly (not by each )


SUMMARY
- This code is a simplified example that demonstrates distributed communication between multiple processes but doesn't actually utilize multiple GPUs
- `mp.spawn()`
    - This launches multiple worker processes, each with a unique rank (basically simulating a distributed env)
    - `world_size` is the total number of workers
    - Our function (in this case `send_receive`) is called as fn(rank, *args), where args is the tuple passed to `mp.spawn()`
        - (rank is also sometimes called process index)
    - To be more precise, this will create a new Python interpreter process for each worker (we call this a "child process"). Each child process won't run the code from start to finish, but will instead only execute the code from the function that `mp.spawn` calls (in our case that's the `send_receive` function)
- `setup()`
    - initializes each process with a common address and port, and a communication backend. It also gives each process a unique rank, so they know who is sending & receiving data
- `send_receive()`
    - this code will be run by each worker, but with different `rank` values. So each worker is either sending or receiving data based on this
- `dist.send` and `dist.recv`
    - are the basic primitives for sending and receiving tensors between processes, i.e. point-to-point communication. There are other primitives for collective communication, like `dist.all_reduce` which we'll see later
    - Note, each `recv` from a given `src` will wait until it receives a `send` from that `src` to continue, and also each `send` to a given `dst` will wait until it receives a `recv` from that `dst` to continue. We say these are **blocking operations**.
- `cleanup()`
    - is called at the end of the function to destroy the process group and release resources
- So this is a demonstration of inter-process communication, but not yet distributed training across multiple GPUs


MASTER_ADDR:
- Specifies the IP address or hostname of the machine that will act as the central coordinator (the "master" node) for setting up and managing the distributed environment
- Even if all processes are on the same machine, this address must be defined so they can communicate
    - In this case, it acts as an internal communication channel


MASTER_PORT:
- Specifies the port number that the master node will use for communication
- Again this is simple in our case because we're running on a single machine, so all we need is for the port to be consistent & not in use by other services on the machine


rank & world_size:
- rank: the ID of the worker, i.e. in range [0, WORLD_SIZE-1]
- world_size: the total number of workers
- Note that this is different from the total number of GPUs, which we can get from `torch.cuda.device_count()`
    - In fact, number of GPUs isn't even specified in the code below! We've not moved any of our data to GPUs, we're just using the distributed API to sync data across multiple processes


"""

import os
import time
from typing import Literal

import torch
import torch.distributed as dist
import torch.multiprocessing as mp

MAIN = __name__ == "__main__"
WORLD_SIZE = torch.cuda.device_count()


def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12345"
    dist.init_process_group("gloo", rank=rank, world_size=world_size)


def cleanup():
    dist.destroy_process_group()


def send_receive(rank, world_size):
    setup(rank, world_size)

    if rank == 0:
        # Send tensor to rank 1
        sending_tensor = torch.zeros(1)
        print(f"{rank=}, sending {sending_tensor=}")
        dist.send(tensor=sending_tensor, dst=1)
    elif rank == 1:
        # Receive tensor from rank 0
        received_tensor = torch.ones(1)
        print(f"{rank=}, creating {received_tensor=}")
        dist.recv(received_tensor, src=0)  # this line overwrites the tensor's data with our `sending_tensor`
        print(f"{rank=}, received {received_tensor=}")

    cleanup()


# if MAIN:
#     world_size = 2; mp.spawn(send_receive, args=(world_size,), nprocs=world_size, join=True)

"""
Now, we're trying with actual GPUs! We're adding lines to the code above, so that:

- Each process is assigned a specific GPU based on its rank, so rank 0 gets "cuda:0" and rank 1 gets "cuda:1"
- Each tensor is created directly on the GPU
- The rank 0 process sends the tensor to the rank 1 process
    - Note: exactly how this works is that the data in the tensor sent by `dist.send` is essentially copied into the 
      tensor that's passed into the `dist.recv` function
    - This tensor needs to be on the CPU before being sent or received, so we call `.cpu()` on them
        - Have to be careful between views and copies here, for example `dist.recv(tensor.cpu())` will fail because
          this gives you a copy, and so you're not modifying the original tensor `tensor`
        - But by contrast it's fine to `dist.send(tensor.cpu())` because we're sending the tensor's data, not the literal
          tensor object itself

- Important points to emphasize:
    - Worker rank and GPU index are not the same thing (although in this example they coincide)
    - Be careful of datatypes when sending & receiving (e.g. bad to try and recieve into a tensor with a different dtype)
"""

assert torch.cuda.is_available()
assert torch.cuda.device_count() > 1, "This example requires at least 2 GPUs per machine"


def send_receive_gpus(rank, world_size):
    setup(rank, world_size)

    # Set the device to a specific GPU based on rank
    device = torch.device(f"cuda:{rank}")

    if rank == 0:
        # Create a tensor, send it to rank 1
        sending_tensor = torch.zeros(1, device=device)
        print(f"{rank=}, {device=}, sending {sending_tensor=}")
        dist.send(sending_tensor.cpu(), dst=1)  # Send tensor to CPU before sending
    elif rank == 1:
        # Receive tensor from rank 0 (it needs to be on the CPU before receiving)
        received_tensor = torch.ones(1, device="cpu")
        print(f"{rank=}, {device=}, creating {received_tensor=}")
        dist.recv(received_tensor, src=0)  # this line overwrites the tensor's data with our `sending_tensor`
        received_tensor = received_tensor.to(device)
        print(f"{rank=}, {device=}, received {received_tensor=}")

    cleanup()


# if MAIN:
#     world_size = 2; mp.spawn(send_receive_gpus, args=(world_size,), nprocs=world_size, join=True)


"""
Setting Up DistributedDataParallel

When training a model across multiple GPUs, we need to make sure that the gradients sychronize. Let's see how that works!

We're adding to our previous code in the following ways:

- Rather than creating a tensor in both processes, we're creating a model in both processes
    - This model has a single parameter, our loss fn is the squared difference between the input and the parameter, so GD will just push the parameter to the mean of the input
    - Calling `loss.backward()` is what causes the gradients to be sychronized across the GPUs

Some more notes:
- `model.module` refers to the original model wrapped by DDP
- Breaking down in more detail what happens in `loss.backward()`:
    - Local Gradient Calculation: Each rank calculates the gradient of param with respect to loss.
    - Gradient Synchronization: DDP performs an all-reduce operation on param.grad across all ranks, averaging the gradients.
    - to emphasize: loss.backward() is the only line that requires inter-rank synchronization, while all other lines execute independently on each rank

At the end, our grad is 3 (which is the average of 4 and 2, our grads from the outputs of -2 and -1 resp) so we step to get 1.7 from 2.0
"""

from torch.nn.parallel import DistributedDataParallel as DDP


class SimpleModel(torch.nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.param = torch.nn.Parameter(torch.tensor([2.0]))

    def forward(self, x: torch.Tensor):
        return x - self.param


def run(rank, world_size):
    setup(rank, world_size)

    # Set the device for the current rank
    device = torch.device(f"cuda:{rank}")

    # Initialize the model and wrap it in DDP
    model = SimpleModel().to(device)
    model = DDP(model, device_ids=[rank])

    # Define a simple optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    # Perform a single forward and backward pass
    optimizer.zero_grad()
    input = torch.tensor([rank], device=device)
    output = model(input)

    # Define the loss as the sum of squared values of the parameter
    loss = output.pow(2).sum()
    loss.backward()  # Gradients are synchronized here by DDP

    # Print the gradient for each rank to show synchronization
    print(f"Rank {rank}, input: {input}, output: {output}, param grad: {model.module.param.grad}")

    optimizer.step()  # Update the parameter

    # Print the updated parameter to confirm changes
    print(f"Rank {rank}, new param: {model.module.param.data}")

    cleanup()


# if MAIN:
#     world_size = 2  # Two processes
#     mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)


"""
Exercise (optional) - write your own all reduce function!

There exist things like `dist.all_reduce` which is more like how `loss.backward()` works under the hood, but we're going to write our own using just the basic primitives `dist.send` and `dist.recv`.

Also a note which should be earlier - kill process by `sudo apt-get install lsof`, `lsof -i :12345` then `kill -9 <pid>`
If this still fails, just change the port in `os.environ["MASTER_PORT"]` and try again

We'll implement our all_reduce in the simplest possible way
    (1) Every >0 rank sends its gradient to rank 0, which receives them and averages them
    (2) Rank 0 sends the summed gradient back to all other ranks

Important gotcha - remember that we need to be modifying tensors inplace in `all_reduce`! For example, in step (2) when
we're receiving the averaged gradient from rank 0, don't do something like this:
        tensor = torch.zeros_like(tensor, device="cpu")
        dist.recv(tensor, src=0)
or:
    dist.recv(tensor.cpu(), src=0)
because neither will modify `tensor` inplace! We instead recommend receiving into a new tensor and then copying that
new tensor to `tensor`, using e.g. the inplace modification method `tensor.copy_(...)`.

! EDIT - they're actually gather and broadcast instead, so I've written them that way.

Also note, you need to explain how dist.all_reduce(tensor, op=dist.ReduceOp.SUM) works, cause we'll use that later!

# ! Second edit - apparently you don't even need to send them via the CPU, so I guess edit earlier shit.
"""


# def gather(tensor, rank, world_size, dst=0):
#     """Gather gradients to rank 0 and sum them."""
#     if rank != dst:
#         dist.send(tensor.cpu(), dst=dst)  #
#     else:
#         for other_rank in range(world_size):
#             if other_rank != dst:
#                 received_tensor = torch.zeros_like(tensor, device="cpu")
#                 dist.recv(received_tensor, src=other_rank)
#                 tensor += received_tensor.to(tensor.device)


def gather(tensor, rank, world_size, dst=0):
    """Gather gradients to rank 0 and sum them."""
    if rank != dst:
        dist.send(tensor, dst=dst)
    else:
        for other_rank in range(world_size):
            if other_rank != dst:
                received_tensor = torch.zeros_like(tensor)
                dist.recv(received_tensor, src=other_rank)
                tensor += received_tensor
    dist.barrier()


# TODO - add print statements (verbose argument = True)


def broadcast(tensor, rank, world_size, src=0):
    """Broadcast averaged gradients from rank 0 to all other ranks."""
    if rank == src:
        for other_rank in range(world_size):
            if other_rank != src:
                dist.send(tensor, dst=other_rank)
    else:
        received_tensor = torch.zeros_like(tensor)
        dist.recv(received_tensor, src=src)
        tensor.copy_(received_tensor)
    dist.barrier()


def all_reduce(tensor, rank, world_size, op: Literal["sum", "mean"] = "sum"):
    """Allreduce the tensor across all ranks, using 0 as the initial gathering rank."""
    gather(tensor, rank, world_size)
    if op == "mean":
        tensor /= world_size
    broadcast(tensor, rank, world_size)


def run_all_reduce(rank, world_size):
    setup(rank, world_size)

    device = torch.device(f"cuda:{rank}")

    # Initialize the model and _DON'T_ wrap it in DDP
    model = SimpleModel().to(device)

    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    optimizer.zero_grad()
    input = torch.tensor([rank], dtype=torch.float32, device=device)
    output = model(input)

    loss = output.pow(2).sum()
    loss.backward()  # Gradients are _NOT_ synchronized here, they're still different on each rank

    # Custom all_reduce to synchronize gradients
    print(f"Rank {rank}, before all_reduce, Gradient of param: {model.param.grad}")
    all_reduce(model.param.grad, rank, world_size)  # Synchronize the gradient
    print(f"Rank {rank}, after all_reduce, Synchronized Gradient of param: {model.param.grad}")

    optimizer.step()
    print(f"Rank {rank}, new param: {model.param.data}")

    cleanup()


import importlib

import dist_training_tests

importlib.reload(dist_training_tests)

from dist_training_tests import test_all_reduce, test_broadcast, test_gather

# if MAIN:
#     world_size = 2; mp.spawn(run_all_reduce, args=(world_size,), nprocs=world_size, join=True)
#     dist_training_tests.test_gather(gather)
#     dist_training_tests.test_broadcast(broadcast)
#     dist_training_tests.test_all_reduce(all_reduce)

"""
Bonus - ring operation! 

Link to smth. Also you'll need to use `dist.isend` and `dist.irecv` instead of `dist.send` and `dist.recv`, because
the ring operations need to be non-blocking
"""


def ring_all_reduce(tensor, rank, world_size, op: Literal["sum", "mean"] = "sum"):
    """Ring all_reduce implementation using non-blocking send/recv to avoid deadlock."""
    # Clone the tensor as the "send_chunk" for initial accumulation
    send_chunk = tensor.clone()

    # Step 1: Reduce-Scatter phase
    for _ in range(world_size - 1):
        # Compute the ranks involved in this round of sending/receiving
        send_to = (rank + 1) % world_size
        recv_from = (rank - 1 + world_size) % world_size

        # Prepare a buffer for the received chunk
        recv_chunk = torch.zeros_like(send_chunk)

        # Non-blocking send and receive
        send_req = dist.isend(send_chunk, dst=send_to)
        recv_req = dist.irecv(recv_chunk, src=recv_from)
        send_req.wait()
        recv_req.wait()

        # Accumulate the received chunk into the tensor
        tensor += recv_chunk

        # Update send_chunk for the next iteration
        send_chunk = recv_chunk

    # Step 2: All-Gather phase
    send_chunk = tensor.clone()
    for _ in range(world_size - 1):
        # Compute the ranks involved in this round of sending/receiving
        send_to = (rank + 1) % world_size
        recv_from = (rank - 1 + world_size) % world_size

        # Prepare a buffer for the received chunk
        recv_chunk = torch.zeros_like(send_chunk)

        # Non-blocking send and receive, and wait for completion
        send_req = dist.isend(send_chunk, dst=send_to)
        recv_req = dist.irecv(recv_chunk, src=recv_from)
        send_req.wait()
        recv_req.wait()

        # Update the tensor with received data
        tensor.copy_(recv_chunk)

        # Update send_chunk for the next iteration
        send_chunk = recv_chunk

    # Step 3: Average the final result
    if op == "mean":
        tensor /= world_size


# if MAIN:
#     dist_training_tests.test_all_reduce(ring_all_reduce)

"""
Finally, we'll leave it as an exercise to you - rewrite the train-ResNet-from-scratch code below to use the distributed training API we've just seen! You can try with 1 or 2 GPUs and compare speeds (maybe even use a wandb sweep?!)

Gonna only comment on the things that are notably different from the non-distributed version. First, the non-distributed version:
"""

import sys
from dataclasses import dataclass
from pathlib import Path

exercise_dir = Path(__file__).parent.parent.resolve()
if str(exercise_dir) not in sys.path:
    sys.path.insert(0, str(exercise_dir))

import torch as t
import torch.nn.functional as F
import wandb
from part2_cnns.solutions import IMAGENET_TRANSFORM, Linear, ResNet34
from torch import Tensor
from torch.utils.data import DataLoader, DistributedSampler, Subset
from torchvision import datasets, models
from tqdm.notebook import tqdm


def get_base_resnet_for_feature_extraction(n_classes: int, my_model: bool = True):
    """Works like the other function, but using torch default modules cause that's less hassle atm (BatchNorm yuck)."""
    if my_model:
        resnet = ResNet34()
        resnet.out_layers[-1] = Linear(resnet.out_features_per_group[-1], n_classes)
    else:
        resnet = models.resnet34()
        resnet.fc = Linear(resnet.fc.in_features, n_classes)
    return resnet


def get_cifar(trainset_size: int = 50_000, testset_size: int = 10_000) -> tuple[Subset, Subset]:
    cifar_trainset = datasets.CIFAR10(root="./data", train=True, download=True, transform=IMAGENET_TRANSFORM)
    cifar_testset = datasets.CIFAR10(root="./data", train=False, download=True, transform=IMAGENET_TRANSFORM)

    assert trainset_size <= len(cifar_trainset) and testset_size <= len(cifar_testset)
    cifar_trainset = Subset(cifar_trainset, indices=range(0, trainset_size))
    cifar_testset = Subset(cifar_testset, indices=range(0, testset_size))

    return cifar_trainset, cifar_testset


@dataclass
class ResNetTrainingArgs:  # TODO - rename "Training" to "Finetuning" in prev exercises?
    n_classes: int = 10
    batch_size: int = 128
    epochs: int = 3
    learning_rate: float = 1e-3
    wandb_project: str | None = "day3-resnet-dist-training"
    wandb_name: str | None = None


class ResNetTrainer:
    def __init__(self, args: ResNetTrainingArgs):
        self.args = args
        self.device = torch.device("cuda")
        self.model = get_base_resnet_for_feature_extraction(args.n_classes).to(self.device)
        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.learning_rate)
        self.trainset, self.testset = get_cifar()
        self.train_loader = DataLoader(self.trainset, batch_size=self.args.batch_size, num_workers=8)
        self.val_loader = DataLoader(self.testset, batch_size=self.args.batch_size, num_workers=8)
        wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)

    def training_step(self, imgs: Tensor, labels: Tensor) -> Tensor:
        imgs, labels = imgs.to(self.device), labels.to(self.device)
        logits = self.model(imgs)
        loss = F.cross_entropy(logits, labels)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        wandb.log({"loss": loss.item()}, step=self.step)
        self.step += imgs.shape[0]
        return loss

    @t.inference_mode()
    def validation_step(self, imgs: Tensor, labels: Tensor) -> Tensor:
        imgs, labels = imgs.to(self.device), labels.to(self.device)
        logits = self.model(imgs)
        return (logits.argmax(dim=1) == labels).sum()

    def evaluate(self) -> Tensor:
        # TODO - change this to be a more legible version, even if it's longer!
        self.model.eval()
        accuracy = sum(self.validation_step(imgs, labels) for imgs, labels in self.val_loader) / len(self.testset)
        wandb.log({"accuracy": accuracy.item()}, step=self.step)
        return accuracy

    def train(self):
        self.step = 0
        accuracy = self.evaluate()

        for epoch in range(self.args.epochs):
            with tqdm(total=len(self.train_loader)) as pbar:
                self.model.train()

                t0 = time.time()
                for imgs, labels in self.train_loader:
                    loss = self.training_step(imgs, labels)
                    pbar.update()
                    pbar.set_description(f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}")
                dur = time.time() - t0
                wandb.log({"time_for_epoch": dur}, step=self.step)

                accuracy = self.evaluate()
                pbar.set_description(f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}, {accuracy=:.2f}, {dur=:.2f}")

        wandb.finish()
        t.save(self.model.state_dict(), "resnet.pth")


# def train_resnet_from_scratch():
#     args = ResNetTrainingArgs()
#     trainer = ResNetTrainer(args)
#     trainer.train()


# train_resnet_from_scratch()
# # time taken? A100 = 1:37s per epoch, 3090 =

"""
# ! (2) The optionally distributed version, using only low-level shit (easier to debug I guess)
"""


@dataclass
class ResNetDistTrainingArgs:  # TODO - rename "Training" to "Finetuning" in prev exercises?
    world_size: int

    n_classes: int = 10
    batch_size: int = 128  # so each GPU gets 64, and we have an effective batch size of 128
    epochs: int = 2
    learning_rate: float = 1e-3
    wandb_project: str | None = "day3-resnet-dist-training"
    wandb_name: str | None = None


class ResNetDistTrainer:
    # ! (1) Different init method, which also takes in rank & world size (use it to set our device)
    def __init__(self, args: ResNetDistTrainingArgs, rank: int):
        self.args = args
        self.rank = rank
        self.device = torch.device(f"cuda:{rank}")

        # ! (2) initialize our model, and broadcast its weights from process 0 to all others
        self.model = get_base_resnet_for_feature_extraction(args.n_classes).to(self.device)
        t0 = time.time()
        if self.args.world_size > 1:
            print(f"Broadcasting weights (rank={self.rank}) ... ")
            for name, param in self.model.named_parameters():
                # if self.rank == 0:
                #     print(f"Broadcasting {name} ... ", end="")
                # broadcast(param, self.rank, self.args.world_size, src=0)
                # with t.no_grad():
                dist.broadcast(param.data, src=0)
                # if self.rank == 0:
                #     print("done!")
            print(f"broadcasted in in {time.time() - t0:.2f}s")
        dist.barrier()

        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.learning_rate)
        print(f"Created optimizer on rank {self.rank}")

        # ! (3) Get dataloaders, using DistributedSampler
        self.trainset, self.testset = get_cifar()
        print(f"Created trainset and testset on rank {self.rank}")
        self.train_sampler = (
            DistributedSampler(self.trainset, num_replicas=args.world_size, rank=self.rank)
            if args.world_size > 1
            else None
        )
        self.train_loader = DataLoader(
            self.trainset, batch_size=self.args.batch_size, sampler=self.train_sampler, num_workers=8
        )
        self.val_loader = DataLoader(self.testset, batch_size=self.args.batch_size, shuffle=False, num_workers=8)
        print(f"Created train_loader and val_loader on rank {self.rank}")

        # ! (4) Initialize / log to wandb, only on the main process
        if self.rank == 0:
            wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)
            print(f"Initialized wandb on rank {self.rank}")

    def training_step(self, imgs: Tensor, labels: Tensor) -> Tensor:
        imgs, labels = imgs.to(self.device), labels.to(self.device)
        logits = self.model(imgs)
        loss = F.cross_entropy(logits, labels)
        loss.backward()
        if self.args.world_size > 1:
            for param in self.model.parameters():
                dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)  # only SUM is supported, so you need to avg
                param.grad /= self.args.world_size

                # all_reduce(param.grad, self.rank, self.args.world_size, op="mean")
        dist.barrier()
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.step += imgs.shape[0] * self.args.world_size  # taking step to be the total number of examples seen
        if self.rank == 0:
            wandb.log({"loss": loss.item()}, step=self.step)
        return loss

    @t.no_grad()
    def evaluate(self) -> float:
        """Run evaluation and return (optionally log) accuracy averaged across all processes"""
        assert self.rank == 0, "Shouldn't be evaluating on any process other than 0"
        self.model.eval()

        total_correct, total_samples = 0, 0
        for imgs, labels in self.val_loader:
            imgs, labels = imgs.to(self.device), labels.to(self.device)
            logits = self.model(imgs)
            total_correct += (logits.argmax(dim=1) == labels).sum().item()
            total_samples += len(imgs)
            print(f"Evaluating: progress {total_samples}/{len(self.val_loader.dataset)}", end="\r")

        accuracy = total_correct / total_samples
        wandb.log({"accuracy": accuracy}, step=self.step)

        return accuracy

    def train(self):
        self.step = 0
        if self.rank == 0:
            accuracy = self.evaluate()
        N = len(self.train_loader)

        for epoch in range(self.args.epochs):
            t0 = time.time()

            # ! (5) Set epoch
            if self.args.world_size > 1:
                self.train_sampler.set_epoch(epoch)

            self.model.train()

            for i, (imgs, labels) in enumerate(self.train_loader):
                loss = self.training_step(imgs, labels)
                if self.rank == 0:
                    print(f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}, {self.step=:06}, {i}/{N}", end="\r")

            if self.rank == 0:
                accuracy = self.evaluate()
                dur = time.time() - t0
                wandb.log({"time_for_epoch": dur}, step=self.step)
                print(
                    f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}, {accuracy=:.2f}, {self.step=:06}, {i}/{N}, {dur=:.2f}"
                )

        if self.rank == 0:
            wandb.finish()
            t.save(self.model.state_dict(), f"resnet_{self.rank}.pth")


def dist_train_resnet_from_scratch(rank, world_size):
    setup(rank, world_size)
    args = ResNetDistTrainingArgs(world_size=world_size)
    trainer = ResNetDistTrainer(args, rank)
    trainer.train()
    cleanup()


# ws=1; mp.spawn(dist_train_resnet_from_scratch, args=(ws,), nprocs=ws, join=True)
# ws=WORLD_SIZE; assert ws==4; mp.spawn(dist_train_resnet_from_scratch, args=(ws,), nprocs=ws, join=True)


"""
And now, we present the distributed version which should hopefully be much faster!

#             self.train_sampler.set_epoch(epoch)
This line is used to set the epoch for the DistributedSampler. It ensures that the data is shuffled differently at each epoch, which is important for distributed training to ensure that each process gets a different subset of data in each epoch. This helps in achieving better convergence and avoiding overfitting to a specific data order.

Help - I get `RuntimeError: Inplace update to inference tensor outside InferenceMode is not allowed...`
I think this happens in evaluation, when you're summing up the correct predictions and the total samples. Try and do this in a non-inplace way

    inference_mode() is a stricter version of no_grad() that enables additional optimizations by guaranteeing that tensors created within it will never need gradients or be modified in-place
    dist.all_reduce performs an in-place operation - it modifies the input tensor directly by accumulating values from all processes. This in-place modification violates inference mode's guarantees
    Because inference mode is meant to provide performance optimizations by guaranteeing tensors won't be modified, PyTorch enforces this strictly - it won't allow you to modify an inference mode tensor even if you're outside the inference mode context
    solution: either use `t.no_grad()` instead, or wrap the all_reduce operation in a `with t.inference_mode(False):` block

    (meme - zero grad being the gun just domes the guy, like linux vs windows shutdown?)

Some more notes:

- batch_size in the DataLoader represents the per-GPU batch size; the effective batch size is batch_size * world_size
- the data shuffling works by giving each GPU a different subset of the data
- Calling `set_epoch` is necessary to make shuffling work properly across multiple epochs
- Make sure you aggregate accuracy, otherwise you might underestimate it by half!
- We recommend you print output rather than using tqdm, because tqdm can be a bit buggy especially in distributed settings




(put this somewhere earlier in the material)

The general recommendation is:

For SGD: Use linear scaling
For adaptive optimizers: Keep original rate or increase by less than 2x




btw we recommend using at least 4 GPUs for this exercise, because with just 2 the speed gain from dist might not significantly
outweight the speed cost of all the synchronization operations. Using 4+ is ideal!

do we break up the eval step across batches? in general no because this section isn't a bottleneck, and so the amount you're saving from parallelizing the computation might be outweighed by the overhead of synchronization. you can try it though (e.g. using `dist.all_reduce` or your implementation of all_reduce above, to get the total number of correct predictions across all processes and then aggregate them).

"""


# @dataclass
# class ResNetDistTrainingArgs:  # TODO - rename "Training" to "Finetuning" in prev exercises?
#     world_size: int

#     n_classes: int = 10
#     batch_size: int = 128  # so each GPU gets 64, and we have an effective batch size of 128
#     epochs: int = 3
#     learning_rate: float = 1e-3
#     wandb_project: str | None = "day3-resnet-dist-training"
#     wandb_name: str | None = None


# class ResNetDistTrainer:
#     # ! (1) Different init method, which also takes in rank & world size (use it to set our device)
#     def __init__(self, args: ResNetDistTrainingArgs, rank: int):
#         self.args = args
#         self.rank = rank
#         self.device = torch.device(f"cuda:{rank}")

#         # ! (2) use DDP for our model
#         model = get_base_resnet_for_feature_extraction(args.n_classes).to(self.device)
#         self.model = DDP(model, device_ids=[self.rank])

#         self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.learning_rate)

#         # ! (3) Get dataloaders, using DistributedSampler
#         self.trainset, self.testset = get_cifar()
#         self.train_sampler = DistributedSampler(self.trainset, num_replicas=args.world_size, rank=rank)
#         self.train_loader = DataLoader(
#             self.trainset, batch_size=self.args.batch_size, sampler=self.train_sampler, num_workers=8
#         )
#         self.val_loader = DataLoader(self.testset, batch_size=self.args.batch_size, shuffle=False, num_workers=8)

#         # ! (4) Initialize / log to wandb, only on the main process
#         if rank == 0:
#             wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)

#     def training_step(self, imgs: Tensor, labels: Tensor) -> Tensor:
#         imgs, labels = imgs.to(self.device), labels.to(self.device)
#         logits = self.model(imgs)
#         loss = F.cross_entropy(logits, labels)
#         loss.backward()
#         self.optimizer.step()
#         self.optimizer.zero_grad()
#         self.step += imgs.shape[0] * self.args.world_size  # taking step to be the total number of examples seen
#         if self.rank == 0:
#             wandb.log({"loss": loss.item()}, step=self.step)
#         return loss

#     @t.no_grad()
#     def evaluate(self) -> float:
#         """Run evaluation and return (optionally log) accuracy averaged across all processes"""
#         assert self.rank == 0
#         self.model.eval()

#         total_correct, total_samples = 0, 0
#         for imgs, labels in self.val_loader:
#             imgs, labels = imgs.to(self.device), labels.to(self.device)
#             logits = self.model(imgs)
#             total_correct += (logits.argmax(dim=1) == labels).sum().item()
#             total_samples += len(imgs)

#         accuracy = total_correct / total_samples
#         wandb.log({"accuracy": accuracy}, step=self.step)

#         return accuracy

#     def train(self):
#         self.step = 0
#         if self.rank == 0:
#             accuracy = self.evaluate()
#         N = len(self.train_loader)

#         for epoch in range(self.args.epochs):
#             t0 = time.time()

#             # ! (5) Set epoch
#             self.train_sampler.set_epoch(epoch)

#             self.model.train()

#             for i, (imgs, labels) in enumerate(self.train_loader):
#                 loss = self.training_step(imgs, labels)
#                 if self.rank == 0:
#                     print(f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}, {self.step=:06}, {i}/{N}", end="\r")

#             if self.rank == 0:
#                 accuracy = self.evaluate()
#                 dur = time.time() - t0
#                 wandb.log({"time_for_epoch": dur}, step=self.step)
#                 print(f"Epoch {epoch+1}/{self.args.epochs}, {loss=:.3f}, {accuracy=:.2f}, {self.step=:06}, {dur=:.2f}")

#         if self.rank == 0:
#             wandb.finish()
#             t.save(self.model.state_dict(), f"resnet_{self.rank}.pth")


# def dist_train_resnet_from_scratch(rank, world_size):
#     setup(rank, world_size)
#     args = ResNetDistTrainingArgs(world_size=world_size)
#     trainer = ResNetDistTrainer(args, rank)
#     trainer.train()
#     cleanup()


# if MAIN:
#     train_resnet_from_scratch(); mp.spawn(dist_train_resnet_from_scratch, args=(WORLD_SIZE,), nprocs=WORLD_SIZE, join=True)

# TODO - maybe remove the validation accuracy splitting thing, idk if that's actually taking a long time?

"""
Bonus stuff here - https://pytorch.org/tutorials/intermediate/ddp_series_minGPT.html
"""


"""
Bonus - your model, with BatchNorm?
"""
