import os

import torch
import torch.distributed as dist
import torch.multiprocessing as mp

PORT = "12355"


def setup(rank: int, world_size: int, port: str = PORT):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = str(port)
    dist.init_process_group("gloo", rank=rank, world_size=world_size)


def cleanup():
    dist.destroy_process_group()


def run_gather(rank: int, world_size: int, gather, port: str = PORT):
    setup(rank, world_size, port)

    # Create a tensor for each rank with its rank as the value
    tensor = torch.tensor([float(rank)], dtype=torch.float32)

    # Run gather operation (all tensors are gathered on rank 0)
    gather(tensor, rank, world_size, dst=0)

    # Check and print results on rank 0
    if rank == 0:
        print(f"Rank {rank} gathered tensor: expected 0+1+2=3, got {tensor}")
        torch.testing.assert_close(tensor, torch.full_like(tensor, 3.0))

    cleanup()


def test_gather(gather, port: str = PORT):
    world_size = 3  # Number of processes (simulated ranks)
    mp.spawn(run_gather, args=(world_size, gather, port), nprocs=world_size, join=True)
    print("All tests in `test_gather` passed!")


def run_broadcast(rank: int, world_size: int, broadcast, port: str = PORT):
    setup(rank, world_size, port)

    # Create a tensor for each rank with its rank as the value
    tensor = torch.tensor([float(rank)], dtype=torch.float32)

    # Run broadcast operation (tensor is broadcasted from rank 0 to all ranks)
    broadcast(tensor, rank, world_size, src=0)

    # Check and print results on all ranks
    print(f"Rank {rank} broadcasted tensor: expected 0 (from rank 0), got {tensor}")
    torch.testing.assert_close(tensor, torch.full_like(tensor, 0.0))

    cleanup()


def test_broadcast(broadcast, port: str = PORT):
    world_size = 3  # Number of processes (simulated ranks)
    mp.spawn(run_broadcast, args=(world_size, broadcast, port), nprocs=world_size, join=True)
    print("All tests in `test_broadcast` passed!")


def run_all_reduce(rank: int, world_size: int, all_reduce, port: str = PORT):
    setup(rank, world_size, port)

    for op in ["sum", "mean"]:
        # Create a tensor for each rank with its rank as the value
        tensor = torch.tensor([float(rank + i) for i in range(world_size)], dtype=torch.float32)
        # print(tensor.tolist())

        # Run all_reduce operation (all tensors are averaged)
        all_reduce(tensor, rank, world_size, op=op)

        # Check and print results on all ranks
        expected = sum([torch.tensor([float(r + i) for i in range(world_size)]) for r in range(world_size)])
        if op == "mean":
            expected /= world_size
        print(f"Rank {rank} gathered tensor, {op=}, expected result {expected}, got {tensor}")
        torch.testing.assert_close(tensor, expected)

    cleanup()


def test_all_reduce(all_reduce, port: str = PORT):
    world_size = 3  # Number of processes (simulated ranks)
    mp.spawn(run_all_reduce, args=(world_size, all_reduce, port), nprocs=world_size, join=True)
    print("All tests in `test_all_reduce` passed!")
