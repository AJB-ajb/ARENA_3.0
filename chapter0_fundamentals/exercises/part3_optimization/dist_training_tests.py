import os

import torch
import torch.distributed as dist
import torch.multiprocessing as mp

PORT = "12355"


def setup(rank: int, world_size: int, port: str = PORT):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = str(port)
    dist.init_process_group("gloo", rank=rank, world_size=world_size)


def cleanup():
    dist.destroy_process_group()


def run_gather(rank: int, world_size: int, gather, port: str = PORT):
    setup(rank, world_size, port)

    # Create a tensor for each rank with its rank as the value
    tensor = torch.tensor([float(rank)], dtype=torch.float32)

    # Run gather operation (all tensors are gathered on rank 0)
    gather(tensor, rank, world_size, dst=0)

    # Check and print results on rank 0
    if rank == 0:
        print(f"Rank {rank} gathered tensor: expected (0+1+2+3)/4=1.5, got {tensor}")
        torch.testing.assert_close(tensor, torch.full_like(tensor, 1.5))

    cleanup()


def test_gather(gather, port: str = PORT):
    world_size = 4  # Number of processes (simulated ranks)
    mp.spawn(run_gather, args=(world_size, gather, port), nprocs=world_size, join=True)
    print("All tests in `test_gather` passed!")


def run_broadcast(rank: int, world_size: int, broadcast, port: str = PORT):
    setup(rank, world_size, port)

    # Create a tensor for each rank with its rank as the value
    tensor = torch.tensor([float(rank)], dtype=torch.float32)

    # Run broadcast operation (tensor is broadcasted from rank 0 to all ranks)
    broadcast(tensor, rank, world_size, src=0)

    # Check and print results on all ranks
    print(f"Rank {rank} broadcasted tensor: expected 0 (from rank 0), got {tensor}")
    torch.testing.assert_close(tensor, torch.full_like(tensor, 0.0))

    cleanup()


def test_broadcast(broadcast, port: str = PORT):
    world_size = 4  # Number of processes (simulated ranks)
    mp.spawn(run_broadcast, args=(world_size, broadcast, port), nprocs=world_size, join=True)
    print("All tests in `test_broadcast` passed!")


def run_allreduce(rank: int, world_size: int, allreduce, port: str = PORT):
    setup(rank, world_size, port)

    # Create a tensor for each rank with its rank as the value
    tensor = torch.tensor([float(rank + i) for i in range(world_size)], dtype=torch.float32)
    # print(tensor.tolist())

    # Run allreduce operation (all tensors are averaged)
    allreduce(tensor, rank, world_size)

    # Check and print results on all ranks
    expected = torch.tensor([1.5, 2.5, 3.5, 4.5])
    print(
        f"Rank {rank} gathered tensor: expected average of [0,1,2,3], [1,2,3,4], [2,3,4,5], [3,4,5,6] = [1.5, 2.5, 3.5, 4.5], got {tensor}"
    )
    torch.testing.assert_close(tensor, expected)

    cleanup()


def test_allreduce(allreduce, port: str = PORT):
    world_size = 4  # Number of processes (simulated ranks)
    mp.spawn(run_allreduce, args=(world_size, allreduce, port), nprocs=world_size, join=True)
    print("All tests in `test_allreduce` passed!")
