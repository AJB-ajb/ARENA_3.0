import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#introduction'>Introduction</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-neuronmodel'><b>Exercise</b> - implement <code>NeuronModel</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#computation-in-superposition'>Computation in superposition</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-neuroncomputationmodel'><b>Exercise</b> - implement <code>NeuronComputationModel</code></a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#bonus-the-asymmetric-superposition-motif'>Bonus - the asymmetric superposition motif</a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-replicate-the-asymmetric-superposition-results'><b>Exercise</b> - replicate the asymmetric superposition results</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#summary-what-have-we-learned'>Summary - what have we learned?</a></li>
</ul></li>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# TMS: Superposition in a Privileged Basis

## Introduction

So far, we've explored superposition in a model without a privileged basis. We can rotate the hidden activations arbitrarily and, as long as we rotate all the weights, have the exact same model behavior. That is, for any ReLU output model with weights
$W$, we could take an arbitrary orthogonal matrix $O$ and consider the model $W' = OW$. Since $(OW)^T(OW) = W^T W$, the result would be an identical model!

Models without a privileged basis are elegant, and can be an interesting analogue for certain neural network representations which don't have a privileged basis â€“ word embeddings, or the transformer residual stream. But we'd also (and perhaps primarily) like to understand neural network representations where there are neurons which do impose a privileged basis, such as transformer MLP layers or conv net neurons.

Our goal in this section is to explore the simplest toy model which gives us a privileged basis. There are at least two ways we could do this: we could add an activation function or apply $L_1$ regularization to the hidden layer. We'll focus on adding an activation function, since the representation we are most interested in understanding is hidden layers with neurons, such as the transformer MLP layer.

This gives us the following "ReLU hidden layer" model. It's the simplest one we can use which is still likely to give us a privileged basis; we just take our previous setup and apply ReLU to the hidden layer.

$$
\begin{aligned}
h & =\operatorname{ReLU}(W x) \\
x^{\prime} & =\operatorname{ReLU}\left(W^T h+b\right)
\end{aligned}
$$

### Exercise - implement `NeuronModel`

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to ~10 minutes on this exercise.
```

In this section, you'll replicate the [first set of results](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss:~:text=model%20and%20a-,ReLU%20hidden%20layer%20model,-%3A) in the Anthropic paper on studying superposition in a privileged basis. To do this, you'll need a new `NeuronModel` class. It can inherit most methods from the `Model` class, but you'll need to redefine the `forward` method to include an intermediate ReLU.

```python
class NeuronModel(Model):
    def __init__(
        self,
        cfg: Config,
        feature_probability: Optional[Tensor] = None,
        importance: Optional[Tensor] = None,
        device=device
    ):
        super().__init__(cfg, feature_probability, importance, device)

    def forward(
        self,
        features: Float[Tensor, "... instances features"]
    ) -> Float[Tensor, "... instances features"]:
        pass


tests.test_neuron_model(NeuronModel)
```

<details>
<summary>Solution</summary>

```python
class NeuronModel(Model):
    def __init__(
        self,
        cfg: Config,
        feature_probability: Optional[Tensor] = None,
        importance: Optional[Tensor] = None,
        device=device
    ):
        super().__init__(cfg, feature_probability, importance, device)

    def forward(
        self,
        features: Float[Tensor, "... instances features"]
    ) -> Float[Tensor, "... instances features"]:
        activations = F.relu(einops.einsum(
           features, self.W,
           "... instances features, instances hidden features -> ... instances hidden"
        ))
        out = F.relu(einops.einsum(
            activations, self.W,
            "... instances hidden, instances hidden features -> ... instances features"
        ) + self.b_final)
        return out
```

</details>

Once you've passed these tests, you can run the cells below to train the model in the same way as before. We use just one instance, with zero sparsity and uniform importance.

We also visualize the matrix $W$. With the argument `neuron_plot=True`, we make it so that the right-hand visualisation is of $W$ rather than $W^T W$ - we can get away with this now because (unlike before) the individual elements of $W$ *are* meaningful. We're working with a **privileged basis**, and $W$ connects features to basis-aligned neurons.

You might find small deviations from the paper's results. But the most important thing to pay attention to is how **there's a shift from monosemantic to polysemantic neurons as sparsity increases**. Monosemantic neurons do exist in some regimes! Polysemantic neurons exist in others. And they can both exist in the same model! Moreover, while it's not quite clear how to formalize this, it looks a great deal like there's a neuron-level phase change, mirroring the feature phase changes we saw earlier.

In the plots you make below, you should see:

* Total monosemanticity at 5 features & 5 neurons
* With more features than neurons, some of the neurons become polysemantic (but some remain monosemantic)

```python
n_features = 10
n_hidden = 5

importance = einops.rearrange(0.75 ** t.arange(1, 1+n_features), "feats -> () feats")
feature_probability = einops.rearrange(t.tensor([0.75, 0.35, 0.15, 0.1, 0.06, 0.02, 0.01]), "instances -> instances ()")

cfg = Config(
    n_instances = len(feature_probability.squeeze()),
    n_features = n_features,
    n_hidden = n_hidden,
)

model = NeuronModel(
    cfg = cfg,
    device = device,
    importance = importance,
    feature_probability = feature_probability,
)
model.optimize(steps=10_000)

plot_features_in_Nd(
    model.W,
    height = 600,
    width = 1000,
    title = "Neuron model: n_features = 10, d_hidden = 5, I<sub>i</sub> = 0.75<sup>i</sup>",
    subplot_titles = [f"1 - S = {i:.2f}" for i in feature_probability.squeeze()],
    neuron_plot = True,
)
```

Try playing around with different settings (sparsity, importance). What kind of results do you get?

You can also try and go further, replicating results later in the paper (e.g. the neuron weight bar plots further on in the paper).

(Note - the argument `show_wtw = False` in the `visualise_Nd_superposition` function means that the right-hand heatmap we see isn't $W^T W$, but just $W$. If you set `show_wtw = False` in the previous section you'd have seen something with no visible pattern because the features had no reason to be monosemantic, but here you should be able to see some monosemantic neurons.)

## Computation in superposition

The example above was interesting, but in some ways it was also limited. The key problem here is that **the model doesn't benefit from the ReLU hidden layer**. Adding a ReLU does encourage the model to have a privileged basis, but since the model is trying to reconstruct the input (i.e. the identity, which is a linear function) it doesn't actually need to use the ReLU, and it will try anything it can to circumvent it - including learning biases which shift all the neurons into a positive regime where they behave linearly. This is a mark against using this toy model to study superposition.

To extend this point: we don't want to study boring linear functions like the identity, we want to study **how models perform (nonlinear) computation in superposition**. The MLP layer in a transformer isn't just a way to represent information faithfully and recover it; it's a way to perform computation on that information. So for this next section, we'll train a model to perform some non-linear computation. Specifically, we'll train our model to **compute the absolute value of inputs $x$**.

Our data $x$ are now sampled from the range $[-1, 1]$ rather than $[0, 1]$ (otherwise calculating the absolute value would be equivalent to reconstructing the input). This is about as simple as a nonlinear function can get, since $abs(x)$ is equivalent to $\operatorname{ReLU}(x) + \operatorname{ReLU}(-x)$. But since it's nonlinear, we can be sure that the model has to use the hidden layer ReLU.

### Exercise - implement `NeuronComputationModel`

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 20-30 minutes on this exercise.
```

You should fill in the `NeuronComputationModel` class below. Specifically, you'll need to fill in the `forward`, `generate_batch` and `calculate_loss` methods. Some guidance:

* The model has a ReLU hidden layer in its forward function (as described above & in the paper). This will require rewriting the `forward` method, but you can keep the same `__init__` method as for the `Model` class (since the weights are the same).
    * We've given you the `__init__` method already. It runs the `__init__` method of the `Model` class, but then deletes the `W` matrix and replaces it with `W1` and `W2`.
* The model's data is different - see the discussion above. Your `generate_batch` function should be rewritten - it will be the same as the first version of this function you wrote (i.e. without correlations) except for one difference: the value is sampled uniformly from the range $[-1, 1]$ rather than $[0, 1]$.
* The model's loss function is different. Rather than computing the (importance-weighted) $L_2$ error between the input $x$ and output $x'$, we're computing the $L_2$ error between $\operatorname{abs}(x)$ and $x'$. This should just require changing one line. The `optimize` function can stay the same, but it will now be optimizing this new loss function.

```python
class NeuronComputationModel(Model):
    W1: Float[Tensor, "n_instances n_hidden n_features"]
    W2: Float[Tensor, "n_instances n_features n_hidden"]
    b_final: Float[Tensor, "n_instances n_features"]

    def __init__(
        self,
        cfg: Config,
        feature_probability: Optional[Tensor] = None,
        importance: Optional[Tensor] = None,
        device=device
    ):
        super().__init__(cfg, feature_probability, importance, device)

        del self.W
        self.W1 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))))
        self.W2 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_features, cfg.n_hidden))))
        self.to(device)


    def forward(
        self,
        features: Float[Tensor, "... instances features"]
    ) -> Float[Tensor, "... instances features"]:

        pass


    def generate_batch(self, batch_size) -> Tensor:

        pass


    def calculate_loss(
        self,
        out: Float[Tensor, "batch instances features"],
        batch: Float[Tensor, "batch instances features"],
    ) -> Float[Tensor, ""]:

        pass


tests.test_neuron_computation_model(NeuronComputationModel)
```

<details>
<summary>Solution</summary>

```python
class NeuronComputationModel(Model):
    W1: Float[Tensor, "n_instances n_hidden n_features"]
    W2: Float[Tensor, "n_instances n_features n_hidden"]
    b_final: Float[Tensor, "n_instances n_features"]

    def __init__(
        self,
        cfg: Config,
        feature_probability: Optional[Tensor] = None,
        importance: Optional[Tensor] = None,
        device=device
    ):
        super().__init__(cfg, feature_probability, importance, device)

        del self.W
        self.W1 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))))
        self.W2 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_features, cfg.n_hidden))))
        self.to(device)


    def forward(
        self,
        features: Float[Tensor, "... instances features"]
    ) -> Float[Tensor, "... instances features"]:
        activations = F.relu(einops.einsum(
           features, self.W1,
           "... instances features, instances hidden features -> ... instances hidden"
        ))
        out = F.relu(einops.einsum(
            activations, self.W2,
            "... instances hidden, instances features hidden -> ... instances features"
        ) + self.b_final)
        return out


    def generate_batch(self, batch_size) -> Tensor:
        feat = 2 * t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W1.device) - 1
        feat_seeds = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W1.device)
        feat_is_present = feat_seeds <= self.feature_probability
        batch = t.where(feat_is_present, feat, 0.0)
        return batch


    def calculate_loss(
        self,
        out: Float[Tensor, "batch instances features"],
        batch: Float[Tensor, "batch instances features"],
    ) -> Float[Tensor, ""]:
        error = self.importance * ((batch.abs() - out) ** 2)
        loss = einops.reduce(error, 'batch instances features -> instances', 'mean').sum()
        return loss
```

</details>

Once you've passed these tests, you can run the code below to make the same visualisation as above.

You should see similar patterns: with very low sparsity most/all neurons are monosemantic, but more polysemantic neurons appear as sparsity increases (until all neurons are polysemantic). Another interesting observation: in the monosemantic (or mostly monosemantic) cases, for any given feature there will be some neurons which have positive exposures to that feature and others with negative exposure. This is because some neurons are representing the value $\operatorname{ReLU}(x_i)$ and others are representing the value of $\operatorname{ReLU}(-x_i)$ (as discussed above, we require both of these to compute the absolute value).

```python
n_features = 100
n_hidden = 40

importance = einops.rearrange(0.8 ** t.arange(1, 1+n_features), "feats -> () feats")
feature_probability = einops.rearrange(t.tensor([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]), "instances -> instances ()")

cfg = Config(
    n_instances = len(feature_probability.squeeze()),
    n_features = n_features,
    n_hidden = n_hidden,
)

model = NeuronComputationModel(
    cfg = cfg,
    device = device,
    importance = importance,
    feature_probability = feature_probability,
)
model.optimize(steps=10_000)

plot_features_in_Nd(
    model.W1,
    height = 800,
    width = 1600,
    title = f"Neuron computation model: n_features = {n_features}, d_hidden = {n_hidden}, I<sub>i</sub> = 0.75<sup>i</sup>",
    subplot_titles = [f"1 - S = {i:.3f}" for i in feature_probability.squeeze()],
    neuron_plot = True,
)
```

To further confirm that this is happening, we can color the values in the bar chart discretely by feature, rather than continuously by the polysemanticity of that feature. We'll use a feature probability of 50% for this visualisation, which is high enough to make sure each neuron is monosemantic. You should find that the input weights $W_1$ form pairs of antipodal neurons (i.e. ones with positive / negative exposures to that feature direction), but both of these neurons have positive output weights $W_2$ for that feature.

```python
n_features = 10
n_hidden = 10

importance = einops.rearrange(0.8 ** t.arange(1, 1+n_features), "feats -> () feats")

cfg = Config(
    n_instances = 5,
    n_features = n_features,
    n_hidden = n_hidden,
)

model = NeuronComputationModel(
    cfg = cfg,
    device = device,
    importance = importance,
    feature_probability = 0.5,
)
model.optimize(steps=10_000)

plot_features_in_Nd_discrete(
    W1 = model.W1,
    W2 = model.W2,
    height = 600,
    width = 1200,
    title = f"Neuron computation model (colored discretely, by feature)",
    legend_names = [f"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}" for i in range(n_features)],
)
```

## Bonus - the asymmetric superposition motif

In the [corresponding section](https://transformer-circuits.pub/2022/toy_model/index.html#computation-asymmetric-motif) of Anthropic's paper, they discuss a particular quirk of this toy model in detail. Their section explains it in more detail than we will here (including some visual explanations), but we'll provide a relatively brief explanation here.

> When we increase sparsity in our model & start to get superposed features, we don't always have monosemantic neurons which each calculate either $\operatorname{ReLU}(x_i)$ or $\operatorname{ReLU}(x_i)$ for some feature $i$. Instead, we sometimes have **asymmetric superposition, where a single neuron detects two different features $i$ and $j$, and stores these features with different magnitudes (assume the $W_1$ vector for feature $i$ is much larger). The $W_2$ vectors have flipped magnitudes (i.e. the vector for $j$ is much larger). When $i$ is present and $j$ is not, there's no problem, because the output for feature $i$ is `large * small` (correct size) and for $j$ is `small * small` (near zero). But when $j$ is present and $i$ is not, the output for feature $j$ is `small * large` (correct size) and for $i$ is `large * large` (much larger than it should be). In particular, this is bad when the sign of output for $i$ is positive. The model fixes this by repurposing another neuron to correct for the case when $j$ is present and $i$ is not. We omit the exact mechanism, but it takes advantage of the fact that the model has a ReLU at the very end, so it doesn't matter if output for a feature is very large and negative (the loss will be truncated at zero), but being large and positive is very bad.

Read the linked section of the Anthropic paper for details.

### Exercise - replicate the asymmetric superposition results

```c
Difficulty: ðŸ”´ðŸ”´âšªâšªâšª
Importance: ðŸ”µâšªâšªâšªâšª

You should spend up to 10-15 minutes on this exercise.
```

Can you find a set of hyperparameters (importance, sparsity values, number of features and neurons) where this behaviour is observed?

Note - we recommend sticking with 5000 optimization steps or fewer. Overtraining this mdoel can cause the magnitudes of $W_1$ to collapse, and $W_2$ to get very large, which makes the plot harder to visually interpret.

<details>
<summary>Solution (set of values I found which produced this pattern)</summary>

I used `n_features=6` and `d_hidden=10` as seen in Anthropic's diagram. Feature probabilities are all $0.25$. Importances are the same as in the example case above; $I_i = 0.8^i$. Around half the instances I trained with these parameters had at least one monosemantic neuron *and* at least one pair of neurons which showed this pattern.

</details>

```python
# YOUR CODE HERE - replicate the asymmetric superposition results
```

See the [Colab notebook](https://colab.research.google.com/drive/1mHKZpkhYAr0WWAQo2Y6pXL08yNfJHOVx?usp=sharing) for some sample code & outputs for these plots.

## Summary - what have we learned?

With toy models like this, it's important to make sure we take away generalizable lessons, rather than just details of the training setup.

The core things to take away form this paper are:

* What superposition is
* How it varies over feature importance and sparsity
* How it varies when we have correlated or anticorrelated features
* The difference between neuron and bottleneck superposition (or equivalently "computational and representational supervision")

""", unsafe_allow_html=True)