import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#extracting-and-using-fvs'>Extracting and using FVs</code></a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#exercise-implement-calculate-fn-vectors-and-intervene'><b>Exercise</b> - implement <code>calculate_fn_vectors_and_intervene</code></a></li>
        <li><a class='contents-el' href='#exercise-calculate-the-function-vector'><b>Exercise</b> - calculate the function vector</a></li>
    </ul></li>
    <li class='margtop'><a class='contents-el' href='#multi-token-generation'>Multi-token generation</code></a></li>
    <li><ul class="contents">
        <li><a class='contents-el' href='#using-nnsight-for-multi-token-generation'></a>Using <code>nnsight</code> for multi-token generation</li>
        <li><a class='contents-el' href='#caching'>caching</a></li>
        <li><a class='contents-el' href='#a-few-more-notes'>A few more notes</a></li>
        <li><a class='contents-el' href='#exercise-intervene-with-function-vector-in-multi-token-generation'><b>Exercise</b> - intervene with function vector, in multi-token generation</a></li>
        <li><a class='contents-el' href='#exercise-generalize-results-to-another-task-optional'><b>Exercise</b> - generalize results to another task (optional)</a></li>
    </ul></li>
</ul></li>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# Function Vectors

In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.

We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `"When you think of Netherlands, you usually think of"` by talking about Amsterdam.

(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).

> ### Learning Objectives
>
> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task
> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head
> * Learn how to use `nnsight` for multi-token generation

Here, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**

First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations that TransformerLens models do (i.e. storing vectors & attention weights separately by heads). In the case of GPT-J, the input to `out_proj` (the final linear map in the attention layer) is a tensor of value vectors which has already been concatenated along attention heads, and applying `out_proj` is equivalent to summing over the attention heads (if you can't see how this is possible, see the section "Attention Heads are Independent and Additive" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).

How can we deal with this? The easiest way is to just intervene on the input of `out_proj` instead (since this is causally the same as intervening on the output), and making sure we reshape this input tensor so that it has a head dimension (then we can intervene more easily on a per-head basis). In other words, you should intervene on the value which we've called `z` in the diagram below.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/out-out-proj.png" width="950">

When you actually need to calculate `a` (the output for a particular attention head), the easiest thing to do is just apply the appropriate slice of the linear map to `z` (we'll get to this in the next exercise, so don't worry about it for now).

## Extracting & using FVs

### Exercise - implement `calculate_fn_vectors_and_intervene`

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ

You should spend up to 30-60 minutes on this exercise.
```

This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:

* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.
    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.
* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).
    * You can use the method `create_corrupted_dataset` method of the `ICLDataset` class for this.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cie-intervention.png" width="1200">

Before you actually start writing the code, it might be helpful to answer the following questions:

<details>
<summary>What will your total batch size be (if your dataset has size <code>N</code>) ?</summary>

Your batch size will be `N * ((N_LAYERS * N_HEADS) + 2)`, because you'll need to run:

* A fowrad pass on the clean dataset (batch size `N`),
* A forward pass on the corrupted dataset (batch size `N`) with no intevention,
* A forward pass on the corrupted dataset (batch size `N`) once for each head, where you're intervening on the heads.

</details>

<details>
<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>

None. You're just doing causal interventions, and getting the logits. You don't need to save the $z$-tensors in order to causally intervene with them at later points within the context manager (see the solution for the `calculate_h_and_intervene` exercises).

</details>

A few other notes:

* We've added a `layers` argument, so you can iterate through different layers of the model (i.e. running the model with `LAYERS = [3, 4, 5]` will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you're getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so with 5 prompts per head this adds up fast!).
    * We've included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.
* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).
* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.
* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - it's like using code annotations within your actual code!

One last note - **if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don't rely on this function working.** However, you should definitely at least read & understand the solution.

```python
def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: Optional[List[int]] = None,
) -> Float[Tensor, "layers heads"]:
    '''
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)

        layers: Optional[List[int]]
            the layers which this function will calculate the score for (if None, we assume all layers)
    '''
    pass
```

<details>
<summary>Solution</summary>

```python
def calculate_fn_vectors_and_intervene(
    model: LanguageModel,
    dataset: ICLDataset,
    layers: Optional[List[int]] = None,
) -> Float[Tensor, "layers heads"]:
    '''
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)

        layers: Optional[List[int]]
            the layers which this function will calculate the score for (if None, we assume all layers)
    '''

    layers = range(model.config.n_layer) if (layers is None) else layers
    heads = range(model.config.n_head)
    n_heads = len(layers) * len(heads)

    # Get corrupted dataset
    corrupted_dataset = dataset.create_corrupted_dataset()
    N = len(dataset)

    with model.forward(remote=True) as runner:

        # Run a forward pass on clean prompts, where we store attention head outputs
        z_dict = {}
        with runner.invoke(dataset.prompts) as invoker:
            for layer in layers:
                # Get hidden states, reshape to get head dimension, store the mean tensor
                z = model.transformer.h[layer].attn.out_proj.input[0][:, -1]
                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)
                for head in heads:
                    z_dict[(layer, head)] = z_reshaped[head]

        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can
        # get the logits to compare with our intervention)
        with runner.invoke(corrupted_dataset.prompts) as invoker:
            pass

        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,
        # because we're doing different interventions each time)
        for layer in layers:
            for head in heads:
                with runner.invoke(corrupted_dataset.prompts) as invoker:
                    # Get hidden states, reshape to get head dimension, then set it to the a-vector
                    z = model.transformer.h[layer].attn.out_proj.input[0][:, -1]
                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]

    # Get output logits (which contains all `n_heads+2` sub-batches of size N, concatenated) and reshape into sub-batches
    output_logits = einops.rearrange(runner.output["logits"][:, -1], "(batch N) d_vocab -> batch N d_vocab", N=N)
    assert output_logits.shape[0] == 2 + n_heads

    # Get the corrupted logits & the logits with intervention (i.e. red in the diagram). Reshape latter to get head dim
    logits_corrupted = output_logits[1]
    logits_intervention = einops.rearrange(output_logits[2:], "(layer head) N d_vocab -> layer head N d_vocab", head=N_HEADS)

    # Get logprobs, for correct tokens
    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)["input_ids"]]
    logprobs_corrupted = logits_corrupted.log_softmax(dim=-1)[range(N), correct_completion_ids]
    logprobs_intervention = logits_intervention.log_softmax(dim=-1)[:, :, range(N), correct_completion_ids]

    # Return mean effect of intervention, over the batch dimension
    return (logprobs_intervention - logprobs_corrupted).mean(dim=-1)
```
</details>

As mentioned, the code below calls the function multiple times, clearing memory between each run, then combines the results. If you want to get more in-depth on memory management, check out [this PyTorch profiler tutorial](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html).

When you run this code & plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper's figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer).

```python
dataset = ICLDataset(ANTONYM_PAIRS, size=4, n_prepended=2)

def batch_process_layers(n_layers, batch_size):
    for i in range(0, n_layers, batch_size):
        yield range(n_layers)[i:i + batch_size]

results = t.empty((0, N_HEADS), device=device)

# If this fails to run, reduce the batch size so the fwd passes are split up more
for layers in batch_process_layers(N_LAYERS, batch_size=7):
    t.cuda.empty_cache()
    print(f"Computing layers in {layers} ...", end="")
    t0 = time.time()
    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])
    print(f"finished in {time.time()-t0:.2f} secods.")

    
imshow(
    results.T,
    title = "Average indirect effect of function-vector intervention on antonym task",
    width = 1000,
    height = 600,
    labels = {"x": "Layer", "y": "Head"},
    aspect = "equal",
)
```

<details>
<summary>Use this dropdown to see the figure you should get when doing this replication (very similar to the paper figure, with some small differences)</summary>

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/main-result.png" width="800">

</details>

### Exercise - calculate the function vector

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 20-35 minutes on this exercise.
```

Your next task is to actually calculate and return the function vector, so we can do a few experiments with it.

You should pick the 10 highest-scoring attention heads (according to the diagram above). You can hardcode these.

Mostly, this will involve taking your previous code and removing parts of it (since you only need to return the function vectors, not intervene with them). However, there is one difficulty here - in the previous exercises we causally intervened on the `out_proj` input (because that was easier), but here we need the actual attention head outputs. Can you see how to do this?

<details>
<summary>Answer</summary>

One way would be to slice `out_proj.input` to get the output for a particular head, then manually apply the output matrix.

However, because of the way `nnsight` works, it's actually easier to access functions like `out_proj` than it is to access the underlying weights. So instead, we can:

- Clone `out_proj.input` for a particular layer, and set to zero all the values for attention heads which aren't in our list,
- Apply `out_proj` to our new tensor.

In applying `out_proj`, we're summing over all heads in that layer which we didn't set to zero. Then, we sum this result over all layers, and get our function vector.

If this seems confusing, ask a TA for help or message in the Slack channel.

</details>

```python
def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: List[Tuple[int, int]],
)  -> Float[Tensor, "d_model"]:
    '''
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: List[Tuple[int, int]]
            list of attention heads we're calculating the function vector from
    '''
    pass


tests.test_calculate_fn_vector(calculate_fn_vector, model)
```

<details>
<summary>Solution</summary>

```python
def calculate_fn_vector(
    model: LanguageModel,
    dataset: ICLDataset,
    head_list: List[Tuple[int, int]],
)  -> Float[Tensor, "d_model"]:
    '''
    Returns a tensor of shape (layers, heads), containing the CIE for each head.

    Inputs:
        dataset: ICLDataset
            the dataset of clean prompts from which we'll extract the function vector (we'll also create a
            corrupted version of this dataset for interventions)
        head_list: List[Tuple[int, int]]
            list of attention heads we're calculating the function vector from
    '''
    # Turn head_list into a dict of {layer: heads which aren't in the list, in this layer}
    head_dict = {layer: list(range(N_HEADS)) for layer in range(N_LAYERS)}
    for layer, head in head_list:
        head_dict[layer].remove(head)

    fn_vector_list = []

    with model.forward(remote=True) as runner:

        with runner.invoke(dataset.prompts) as invoker:
            for layer, head_list in head_dict.items():

                # Get the output projection layer
                out_proj = model.transformer.h[layer].attn.out_proj

                # Get the mean output projection input (make sure it has a batch dim)
                hidden_states = out_proj.input[0][:, -1].mean(dim=0, keepdim=True).clone()

                # Zero-ablate all heads which aren't in our list, then get the output (which
                # will be the sum over the heads we actually do want!)
                for head in head_list:
                    head_start = D_HEAD * head
                    head_end =  D_HEAD * (head + 1)
                    hidden_states[:, head_start: head_end] = 0.0

                # Now that we've zeroed all unimportant heads, get the output & add it to the list
                out_proj_output = out_proj(hidden_states).squeeze()
                fn_vector_list.append(out_proj_output.save())

    # We sum all attention head outputs to get our function vector
    fn_vector = sum([v.value for v in fn_vector_list])

    assert fn_vector.shape == (D_MODEL,)
    return fn_vector


tests.test_calculate_fn_vector(calculate_fn_vector, model)
```
</details>

## Multi-token generation

We're now going to replicate some of the results in Table 3, in the paper:

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tab3.png" width="700">

This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.

Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.

The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces "next-token antonym behaviour" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).

### Using `nnsight` for multi-token generation

Previously, our context managers have looked like:

```python
with model.forward(remote=True) as runner:
    with runner.invoke(prompt) as invoker:

        # Do stuff to the model's internals
```

But for multi-token generation, we'll be using the `generate` method rather than `forward`. Our context managers will look like:

```python
with model.generate(max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id) as generator:
    with generator.invoke(prompt) as invoker:

        for n in range(max_new_tokens):
            # Do stuff to the model's internals, on the n-th forward pass
            invoker.next()
```

The line `invoker.next()` denotes that the following interventions should be applied to the subsequent generations.

Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).

### Caching

One important concept we've left out of discussions so far is **caching**. To speed up inference, transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) for more on transformer inference arithmetic.

When caching takes place, and we're doing causal interventions, we have to be careful that the caching won't override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention "add the function vector to *only* the final sequence position of the prompt for each token we generate" then we'd have to disable caching (since previous forward passes would contain cached values where we intervened on a sequence position which is no longer the final sequence position). However, here we're performing the intervention "add the function vector to the final token of the original prompt, and to *all subsequent sequence positions*", meaning enabling caching (the default behaviour) will give us the right causal intervention.

(Note - if this seems confusing, please ask a TA or send a message in Slack.)

### A few more notes

* The object `generator.output` is by default a tensor which contains the model's token ID completions (not the logits). If you want to return the logits, add the arguments `output_scores=True` and `return_dict_in_generate=True` to the generate method, so that you return not just the tensor of model's token ID completions, but a dictionary containing this and the logits too.
* The `pad_token_id` argument isn't strictly necessary (this is the default behaviour anyway), it just suppresses a warning message that would otherwise be printed.
* By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour.

### Exercise - intervene with function vector, in multi-token generation

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª

You should spend up to 15-30 minutes on this exercise.
```

You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template.

We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.

```python
def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template = 'The word "{x}" means',
    n_tokens: int = 5,
) -> Tuple[str, str]:
    '''
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    '''
    pass
```

<details>
<summary>Solution</summary>

```python
def intervene_with_fn_vector(
    model: LanguageModel,
    word: str,
    layer: int,
    fn_vector: Float[Tensor, "d_model"],
    prompt_template = 'The word "{x}" means',
    n_tokens: int = 5,
) -> Tuple[str, str]:
    '''
    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.

    Inputs:
        word: str
            The word which is substituted into the prompt template, via prompt_template.format(x=word)
        layer: int
            The layer we'll make the intervention (by adding the function vector)
        fn_vector: Float[Tensor, "d_model"]
            The vector we'll add to the final sequence position for each new token to be generated
        prompt_template:
            The template of the prompt we'll use to produce completions
        n_tokens: int
            The number of additional tokens we'll generate for our unsteered / steered completions

    Returns:
        completion: str
            The full completion (including original prompt) for the no-intervention case
        completion_intervention: str
            The full completion (including original prompt) for the intervention case
    '''
    prompt = prompt_template.format(x=word)

    with model.generate(max_new_tokens=n_tokens, pad_token_id=tokenizer.eos_token_id, remote=True) as generator:

        # No intervention
        with generator.invoke(prompt) as invoker:
            pass

        # Intervention
        with generator.invoke(prompt) as invoker:
            for _ in range(n_tokens):
                hidden_states = model.transformer.h[layer].output[0]
                hidden_states[:, -1] += fn_vector
                invoker.next()

    output = generator.output

    completion, completion_intervention = tokenizer.batch_decode(output)

    return completion, completion_intervention
```

</details>

To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym. If this works, congratulations - **you've just successfully induced an OOD behavioural change in a 6b-parameter model!**

```python
# Remove "light" from our pairs, so it can be a holdout
word = "light"
_ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]

# Define our dataset, and the attention heads we'll use
dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)
head_list = [(8, 0), (8, 1), (9, 14), (11, 0), (12, 10), (13, 12), (13, 13), (14, 9), (15, 5), (16, 14)]

# Extract the function vector
fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model,
    word = word,
    layer = 9,
    fn_vector = fn_vector,
    prompt_template = 'The word "{x}" means',
    n_tokens = 40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

### Exercise - generalize results to another task (optional)

```c
Difficulty: ðŸ”´ðŸ”´ðŸ”´ðŸ”´âšª
Importance: ðŸ”µðŸ”µðŸ”µâšªâšª

You should spend up to 20-45 minutes on this exercise.
```

In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).

We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.

<details>
<summary>Guidance for exercise</summary>

Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.

When you define the `ICLDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.

You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.

</details>

```python
YOUR CODE HERE - generalize results to another task
```

<details>
<summary>Solution (for the country-capitals task)</summary>

```python
# I found using GPT4 directly was easier than using the API for this function. I copied the output directly inline:
COUNTRY_CAPITAL_PAIRS = [
    ['Portugal', 'Lisbon'],
    ['Ireland', 'Dublin'],
    ['Chile', 'Santiago'],
    ['Japan', 'Tokyo'],
    ['Australia', 'Canberra'],
    ['India', 'New Delhi'],
    ['Canada', 'Ottawa'],
    ['Brazil', 'BrasÃ­lia'],
    ['Germany', 'Berlin'],
    ['France', 'Paris'],
    ['Italy', 'Rome'],
    ['Spain', 'Madrid'],
    ['Norway', 'Oslo'],
    ['Sweden', 'Stockholm'],
    ['Finland', 'Helsinki'],
    ['Netherlands', 'Amsterdam'],
    ['Russia', 'Moscow'],
    ['China', 'Beijing'],
    ['United Kingdom', 'London'],
    ['Peru', 'Lima'],
    ['Colombia', 'BogotÃ¡'],
    ['Venezuela', 'Caracas'],
    ['Egypt', 'Cairo'],
    ['Turkey', 'Ankara'],
    ['Iran', 'Tehran'],
    ['Iraq', 'Baghdad'],
    ['Afghanistan', 'Kabul'],
    ['Pakistan', 'Islamabad'],
    ['Bangladesh', 'Dhaka'],
    ['Nigeria', 'Abuja'],
    ['Kenya', 'Nairobi'],
    ['Uganda', 'Kampala'],
    ['Tanzania', 'Dodoma'],
    ['Ghana', 'Accra'],
    ['Senegal', 'Dakar'],
    ['Morocco', 'Rabat'],
    ['Algeria', 'Algiers'],
    ['Libya', 'Tripoli'],
    ['Sudan', 'Khartoum'],
    ['Tunisia', 'Tunis'],
    ['Zimbabwe', 'Harare'],
    ['Zambia', 'Lusaka'],
    ['Mozambique', 'Maputo'],
    ['Angola', 'Luanda'],
    ['Somalia', 'Mogadishu'],
    ['Mali', 'Bamako'],
    ['Niger', 'Niamey'],
    ['Malawi', 'Lilongwe'],
    ['Vietnam', 'Hanoi'],
    ['Thailand', 'Bangkok'],
    ['Singapore', 'Singapore'],
    ['Indonesia', 'Jakarta'],
    ['Philippines', 'Manila'],
    ['Myanmar', 'Naypyidaw'],
    ['Laos', 'Vientiane'],
    ['Mongolia', 'Ulaanbaatar'],
    ['Uzbekistan', 'Tashkent'],
    ['Turkmenistan', 'Ashgabat'],
    ['Kyrgyzstan', 'Bishkek'],
    ['Tajikistan', 'Dushanbe'],
    ['Georgia', 'Tbilisi'],
    ['Armenia', 'Yerevan'],
    ['Azerbaijan', 'Baku'],
    ['Ukraine', 'Kyiv'],
    ['Belarus', 'Minsk'],
    ['Poland', 'Warsaw'],
    ['Slovakia', 'Bratislava'],
    ['Hungary', 'Budapest'],
    ['Romania', 'Bucharest'],
    ['Bulgaria', 'Sofia'],
    ['Greece', 'Athens'],
    ['Cyprus', 'Nicosia'],
    ['Israel', 'Jerusalem'],
    ['Jordan', 'Amman'],
    ['Lebanon', 'Beirut'],
    ['Syria', 'Damascus'],
    ['Oman', 'Muscat'],
    ['Qatar', 'Doha'],
    ['Bahrain', 'Manama'],
    ['Nepal', 'Kathmandu'],
    ['Bhutan', 'Thimphu']
]

# Remove (Netherlands, Amsterdam) from the pairs, so it can be a holdout
country = "Netherlands"
_COUNTRY_CAPITAL_PAIRS = [pair for pair in COUNTRY_CAPITAL_PAIRS if pair[0] != country]

# Define our dataset, and the attention heads we'll use
dataset = ICLDataset(_COUNTRY_CAPITAL_PAIRS, size=20, n_prepended=5, bidirectional=False)
head_list = [(8, 0), (8, 1), (9, 14), (11, 0), (12, 10), (13, 12), (13, 13), (14, 9), (15, 5), (16, 14)]

# Extract the function vector
fn_vector = calculate_fn_vector(model, dataset, head_list)

# Intervene with the function vector
completion, completion_intervention = intervene_with_fn_vector(
    model = model,
    word = country,
    layer = 9,
    fn_vector = fn_vector,
    prompt_template = 'When you think of {x},',
    n_tokens = 40,
)

table = Table("No intervention", "intervention")
table.add_row(repr(completion), repr(completion_intervention))
rprint(table)
```

</details>
""", unsafe_allow_html=True)