import platform
import sys
from pathlib import Path

import streamlit as st
import streamlit_antd_components as sac
from streamlit_image_select import image_select

instructions_dir = Path(__file__).parent  # ARENA_3/chapter1_transformer_interp/instructions
chapter_dir = instructions_dir.parent  # ARENA_3/chapter1_transformer_interp
arena_root_dir = chapter_dir.parent  # ARENA_3
if str(arena_root_dir) not in sys.path:
    sys.path.append(str(arena_root_dir))

assert (arena_root_dir / "st_dependencies.py").exists(), "Path error: won't be able to handle local imports!"
from st_dependencies import HOMEPAGE_CONTENT, HOMEPAGE_TOC, generate_toc, styling

IS_LOCAL = platform.processor() != ""
DEBUG = False

styling(DEBUG)

ROOT_URL = "https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/"

# # Note, I don't think this code will work in the public page, only when run locally
# st.components.v1.html(
#     f"""
# <script>
# console.log("Is local: {IS_LOCAL}");
# console.log("Instructions dir: {instructions_dir}");
# console.log("Chapter dir: {chapter_dir}");
# console.log("Arena root dir: {arena_root_dir}");
# console.log("Number of files in ARENA root dir: {len(list(arena_root_dir.iterdir()))}");
# </script>
# """,
# )

INFOS = {
    "1.3": {
        "galaxies": (
            "[1.3.1] Toy Models of Superposition & SAEs",
            r"""
In this section you'll learn about the mathematical underpinnings of superposition, by replicating the key results from Anthropic's 2022 paper [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html). You'll also build the sparse autoencoder (SAE) architecture and train it to recover the features learned by these toy models, even replicating some more recent results such as those from [DeepMind's Gated SAE paper](https://deepmind.google/research/publications/88147/).
    """,
        ),
        "neuronpedia": (
            "[1.3.2] Interpretability with SAEs",
            r"""
In this section you'll be introduced to `SAELens`, which is essentially the analogue of TransformerLens for sparse autoencoders: providing a standardized architecture and syntax for functionality like training and various causal interventions. `SAELens` is also closely tied to `neuronpedia`, an open platform for interpretability research. In these exercises you'll take a guided tour of both, while learning what you can do with sparse autoencoders, and which paths have yet to be explored!
""",
        ),
    },
    "1.4": {
        "leaves": (
            "[1.4.1] Indirect Object Identification",
            r"""
This notebook / document is built around the [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper, in which the authors aim to understand the **indirect object identification circuit** in GPT-2 small. This circuit is resposible for the model's ability to complete sentences like `"John and Mary went to the shops, John gave a bag to"` with the correct token "`" Mary"`.

The flavour of the first ~2/3 of these exercises is experimental and loose, with a focus on demonstrating what exploratory analysis looks like in practice with the transformerlens library. They skimp on rigour, and instead try to speedrun the process of finding suggestive evidence for this circuit. The later sections take a slightly more rigorous approach, replicating some of the results from the IOI paper.
""",
        ),
        "fv_header": (
            "[1.4.2] Function Vectors & Model Steering",
            r"""
These exercises serve as an exploration of the following question: ***can we steer a model to produce different outputs / have a different behaviour, by intervening on the model's forward pass using vectors found by non gradient descent-based methods?***

The exercises also take you through use of the `nnsight` library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.
    """,
        ),
    },
    "1.5": {
        "gears2": (
            "[1.5.1] Balanced Bracket Classifier",
            r"""
When models are trained on synthetic, algorithmic tasks, they often learn to do some clean, interpretable computation inside. Choosing a suitable task and trying to reverse engineer a model can be a rich area of interesting circuits to interpret!

In these exercises, we'll interpret a 3-layer transformer which was trained to perform **bracket classification**, i.e. taking a string of parentheses like `"(())()"` and trying to output a prediction of "balanced" or "unbalanced". We will find an algorithmic solution for solving this problem, and reverse-engineer one of the circuits in our model that is responsible for implementing one part of this algorithm.
""",
        ),
        "wheel3-2": (
            "[1.5.2] Grokking and Modular Arithmetic",
            r"""
Our goal for these exercises is to reverse-engineer a one-layer transformer trained on modular addition! It turns out that the circuit responsible for this involves discrete Fourier transforms and trigonometric identities.

We will also go deeper than just analysing the model's final learned solution; we'll also look at the model's training over time, and find evidence of **grokking** - when the model rapidly goes from a high test loss to near zero. We'll investigate why this grokking happens, and ways it can be predicted.
""",
        ),
        "othello": (
            "[1.5.3] OthelloGPT",
            r"""
These exercises explore the OthelloGPT model, which is a GPT-2-like model trained on the Othello dataset. The model is trained to predict the next move in a game of Othello, given the current board state. The model is trained on a dataset of 1.5 million games, and achieves a 90% accuracy on the test set.

The paper [Emergent World Representations](https://arxiv.org/pdf/2210.13382) investigates whether the model has an interpretable board state. They discover it does, but this board state isn't represented linearly. Neel Nanda built on this paper's results, showing that a linear board state does exist. We will reproduce his results, and take deeper dives investigating particular circuits and neurons in the OthelloGPT model.
""",
        ),
        "alg-combined": (
            "Monthly Algorithmic Problems",
            r"""
At the end of this section, we have a series of 6 archived algorithmic problems, which ran during late 2023 / early 2024. These are designed to test some of the skills you will have accumulated during other exercises in this section. Note that these are better thought of as fun challenges / hackathon-type problems, as opposed to opportunities to learn about specific topics or tools, so we recommend not attempting them while you're working through the ARENA material during any kind of structured program (except as a hackathon). 
""",
        ),
    },
}


def show_sections(section: str):
    image_stems = list(INFOS[section].keys())
    titles = [INFOS[section][x][0] for x in image_stems]
    cols = st.columns(2)
    with cols[1]:
        img = image_select(
            label=f"Click to see a summary of each page in section {section}",
            images=[f"{ROOT_URL}{stem}.png" for stem in image_stems],
            captions=titles,
            use_container_width=False,
        )
    with cols[0]:
        for x in INFOS[section].keys():
            if x in img:
                title_bold = f"> **{INFOS[section][x][0]}**"
                contents_quoted = INFOS[section][x][1].strip().replace("\n", "\n>")
                st.markdown(f"{title_bold}\n> \n>{contents_quoted}", unsafe_allow_html=True)


FIRST_EXERCISE_DIR = "part1_transformer_from_scratch"
FIRST_EXERCISE_NB = "[1.1] Transformer from Scratch (exercises).ipynb"
FIRST_SOLNS_NB = "[1.1] Transformer from Scratch (solutions).ipynb"
REPO_STRUCTURE = """```
.
├── chapter0_fundamentals
├── chapter1_transformer_interp
│   ├── exercises
│   │   ├── part1_transformer_from_scratch
│   │   │   ├── [1.1] Transformer from Scratch (exercises).ipynb
│   │   │   ├── [1.1] Transformer from Scratch (solutions).ipynb
│   │   │   ├── solutions.py
│   │   │   ├── tests.py
│   │   │   └── answers.py*
│   │   ├── part2_intro_to_mech_interp
│   │   ⋮    ⋮
│   └── instructions
│       └── Home.py
├── chapter2_rl
└── requirements.txt
```"""

COLAB_NOTEBOOKS = r"""<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/optimus.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Transformer from Scratch<br>[**exercises**](https://colab.research.google.com/drive/1lDwKASSYGE4y_7DuGSqlo3DN41NHrXEw?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1bZkkJd8pAVnSN23svyszZ3f4WrnYKN_3?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/lens2.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Intro to Mech Interp<br>[**exercises**](https://colab.research.google.com/drive/1gZdHsBL8Ljq7nSWJtxxlsI4JWHmllxxP?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1TVHaqN7if-8aCmc06t8CAIaHUlhJ4ek7?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/leaves.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Indirect Object Identification<br>[**exercises**](https://colab.research.google.com/drive/1ZzLGDngppg5Y7CAKubww45RZrcXpVsrL?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1KgrEwvCKdX-8DQ1uSiIuxwIiwzJuQ3Gw?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/galaxies.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Superposition & SAEs<br>[**exercises**](https://colab.research.google.com/drive/1iCM3V0G3B7NSxKsZkvHtmv7k9rgBXYk4) | [**solutions**](https://colab.research.google.com/drive/1HABl0_vi0AokGVk8-wv7KX6Csd65ZSmr)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/neuronpedia.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Interpretability with SAEs<br>[**exercises**](https://colab.research.google.com/drive/1iCM3V0G3B7NSxKsZkvHtmv7k9rgBXYk4) | [**solutions**](https://colab.research.google.com/drive/1HABl0_vi0AokGVk8-wv7KX6Csd65ZSmr)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/fv_header.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Function Vectors & Model Steering<br>[**exercises**](https://colab.research.google.com/drive/1NYjR3tjOiDJ2v8nv3mhrph-_IM4p9goS?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1dQ-p8j_cCjHCQ82pc446vxxj5pNd2DvN?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/othello.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
OthelloGPT<br>[**exercises**](https://colab.research.google.com/drive/1-0HDNWowpG7gSiWJLrPWCHqEDEfXuLkq?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/13MY80u0jr_VuowyCOC846qzm8dHcNooo?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/gears2.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Balanced Bracket Classifier<br>[**exercises**](https://colab.research.google.com/drive/1q2NzRVjTAjz1XWb3N3DbICKd83AyqsrN?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1YcijqAptCrnIMuFxPvhGYwx80UnKfelQ?usp=sharing)

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/wheel3-2.png" width="160" style="margin-bottom:3px;margin-top:15px"><br>
Grokking and Modular Arithmetic<br>[**exercises**](https://colab.research.google.com/drive/1wg20amCB7n_myEgHYXMnltLiqH3f9aRN?usp=sharing) | [**solutions**](https://colab.research.google.com/drive/1tg4TyTVOWVRRjmTHj6Af7ewHAnFEe39Z?usp=sharing)
"""


def show_section_0():
    st.markdown(
        r"""
<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/magnifying-glass-2.png" width="600">

# Chapter 1: Transformer Interpretability

The material on this page covers transformers (what they are, how they are trained, how they are used to generate output) as well as mechanistic interpretability (what it is, what are some of the most important results in the field so far, why it might be important for alignment) and other topics related to interpretability (function vectors & model steering).

Some highlights from this chapter include:

* Building your own transformer from scratch, and using it to sample autoregressive output
* Using the [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) library developed by Neel Nanda to locate induction heads in a 2-layer model
* Finding a circuit for [indirect object identification](https://arxiv.org/abs/2211.00593) in GPT-2 small
* Intepreting model trained on toy tasks, e.g. classification of bracket strings, or modular arithmetic
* Replicating Anthropic's results on [superposition](https://transformer-circuits.pub/2022/toy_model/index.html), and training sparse autoencoders to recover features from superposition
* Using [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector) to induce behavioural changes in GPT2-XL

## Structure of this chapter

In the first chapter, all 5 exercise sets were compulsory & meant to be done in order. In this chapter however, only the first 2 exercise sets are compulsory (and should be done first). The rest of the material is split into 3 sections, each of which has several different exercise sets, each of which is connected by a similar theme.

### [1.1] Transformers from Scratch & [1.2] Intro to Mech Interp

The first two sections are compulsory, and will guide you through the basic ideas of transformers & mechanistic interpretability. **[1.1] Transformer from Scratch** will guide you through building a transformer from scratch (and show you how things like transformer training & sampling works), and **[1.2] Intro to Mech Interp** will guide you through the basics of mechanistic interpretability via the **TransformerLens library**. [1.2] has a particular focus on **induction heads**, one of the first major breakthroughs in the field of transformer interpretability, which are responsible for basic in-context learning. The ideas underpinning this section will reoccur in all subsequent parts of this chapter.

### [1.3] Superposition & SAEs

- **Summary**: This section contains material on **superposition** (the phenomena in which models can represent more features than it has dimensions in which to store those features). We explore how superposition occurs, why resolving superposition is important for interpretability, etc. 
- **Type of exercises**: These exercises span a full range, from the mathematical theory underpinning superposition & feature geometry to large-scale experiments steering with features on real world models. Beyond the first few compulsory sections, it's very much an a-la-carte set of exercises, where you can pick and choose what you're interested in.
- **Tools**: We'll learn to use `SAELens`, a library written by Joseph Bloom & Johnny Lin, which is essentially the TransformerLens of sparse autoencoders. We'll also use the `neuronpedia` service designed by these two.""",
        unsafe_allow_html=True,
    )
    show_sections("1.3")
    st.markdown(
        r"""
### [1.4] Circuits in LLMs

- **Summary**: This section will focus on analyzing large language models. We'll look for circuits, study causal interventions like ablation / patching / steering, etc. 
- **Type of exercises**: Because of the scale of the models we're working with, there will be a focus on how to narrow down a very large hypothesis space, and design experiments that hold all variables constant while only testing the things we're interested in. Filtering out the noise from messy, real-world models is often challenging. There will also be some focus on the engineering aspect of our work - how can we make our experiments run faster, on less memory? What is the most efficient way of gathering information? 
- **Tools**: We'll learn to use `nnsight`, a library developed by David Bau's research team, which facilitates experiments on very large language models. This is an invaluable resource for anyone who wants to perform interpretability experiments at a larger scale than the compute available to them.
""",
        unsafe_allow_html=True,
    )
    show_sections("1.4")
    st.markdown(
        r"""    
### [1.5] Toy Models

- **Summary**: This section will focus on analyzing small models, trained for a particular purpose. For example, we'll look at models trained to perform modular arithmetic, or basic LeetCode-type tasks which can be algorithmically specified. 
- **Type of exercises**: This section focuses on understanding relatively small models in depth. Because of the simplicity of these models, we will often be able to break down an entire model into understandable circuits and functions. You will have to be on the lookout for anomalies in this section, because the single-purpose nature of these models means that any piece of evidence going against your hypothesis might be a sign that the hypothesis is wrong. At a meta-level, you'll also think about when toy models are appropriate, and how to walk the right line in your research: too simple a model and it will fail to reflect reality; too complex and they will fail to isolate the phenomena you want to study.
- **Tools**: This section won't introduce any major new tools or libraries, we'll be focusing on TransformerLens.
""",
        unsafe_allow_html=True,
    )
    show_sections("1.5")
    st.markdown(
        r"""
Note that these sections aren't cleanly divided - for example we do study circuits and toy models in SAEs, plus we can study toy models like OthelloGPT which have complexity closer to language models like GPT-2 than to 1-layer toy models. However, you should still find this division useful when it comes to the general flavour of the exercises, and what you should expect from them.
""",
        unsafe_allow_html=True,
    )
    st.markdown(
        HOMEPAGE_CONTENT.replace("REPO_STRUCTURE", REPO_STRUCTURE)
        .replace("FIRST_EXERCISE_DIR", FIRST_EXERCISE_DIR)
        .replace("FIRST_EXERCISE_NB", FIRST_EXERCISE_NB)
        .replace("FIRST_SOLNS_NB", FIRST_SOLNS_NB)
        .replace("COLAB_NOTEBOOKS", COLAB_NOTEBOOKS),
        unsafe_allow_html=True,
    )


SECTION_1 = r"""# Reference Page

This page contains links to a bunch of things (blog posts, diagrams, tables) as well as guides and code references, all of which are useful to have at hand when doing this chapter.

*If you have any other suggestions for this page, please add them on Slack!*

## Links

### Logistics

* [Notion page](https://www.notion.so/ARENA-2-0-Virtual-Resources-7934b3cbcfbf4f249acac8842f887a99?pvs=4) for people studying virtually
* [ARENA Slack group invite link](https://join.slack.com/t/arena-uk/shared_invite/zt-2noug8mpy-TRYbCnc3pzj7ITNrZIjKww)
* [Open Source Mechanistic Interpretability Slack group invite link](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-1qosyh8g3-9bF3gamhLNJiqCL_QqLFrA)

### General

* [Google Drive folder](https://drive.google.com/drive/folders/1N5BbZVh5_pZ3sH1lv4krp-2_wJrB-Ahg) containing Colab versions of all these exercises
* Neel Nanda's [Mech Interp Dynalist notes](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J)
* Neel Nanda's [Concrete Steps to Get Started in Transformer Mechanistic Interpretability](https://www.neelnanda.io/mechanistic-interpretability/getting-started)
* Neel Nanda's [An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers)

### How Transformers Work

* Neel Nanda's Implementing a Transformer Walkthrough: [Part 1/2](https://www.youtube.com/watch?v=bOYE6E8JrtU), [Part 2/2](https://www.youtube.com/watch?v=dsjUDacBw8o)
* Callum McDougall's [An Analogy for Understanding Transformers](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)
* Callum McDougall's [full transformer excalidraw diagram](https://link.excalidraw.com/l/9KwMnW35Xt8/4kxUsfrPeuS)
* Jay Alammar's [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

### TransformerLens

* TransformerLens documentation page:
    * [Homepage](https://transformerlensorg.github.io/TransformerLens/index.html)
    * [Table of model properties](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)

### Diagrams

[Link to excalidraw](https://link.excalidraw.com/l/9KwMnW35Xt8/6PEWgOPSxXH) for the diagram below.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png" width="1200">

## Quick reference - TransformerLens

Here's a list of some useful functions and methods in TransformerLens.

### Weights

You can index weights via the full name, e.g. `model.blocks[0].attn.W_Q` returns something of shape `(n_heads, d_head, d_model)`. But you can also use `model.W_Q` which returns all the matrices stacked along the layer dimension, i.e. shape `(n_layers, n_heads, d_head, d_model)`. This works for all other weights:

```python
model.W_Q -> query weights
model.W_K -> key weights
model.W_V -> value weights
model.W_O -> output weights

model.W_in -> MLP input weights
model.W_out -> MLP output weights
```

and exactly the same for biases.

### Hooks

Template for a hook function is:

```python
def hook_fn(activation: Tensor, hook: HookPoint):
    # Modify / store activation
    # optionally return new (changed) value of activation
    ...
```

Useful functions for hooks are:

#### `model.add_hooks`

Takes arguments `hook_name` and `hook_fn`. Adds a hook to the model, which will be called and removed after each forward pass.

Note that `hook_name` can be a boolean function which takes names as arguments, in which case the hook will be added to all activations where the function evaluates to True.

Example: `model.add_hooks(lambda name: name.endswith("z"), hook_fn)` adds hooks to `z` at all layers.

#### `model.run_with_hooks`

Used as follows:

```python
out = model.run_with_hooks(
    inputs,
    fwd_hooks = [(hook_name, hook_fn), ...],
    return_type = "logits" # can also do "loss", "both", or None
)
```

#### `model.run_with_cache`

Used as follows:

```python
out, cache = model.run_with_cache(
    inputs,
    return_type = "logits" # can also do "loss", "both", or None
)
```

An optional argument is `names_filter`, which is a boolean function taking names as arguments (we'll only cache the activations where `names_filter(name)` is True).

### Making your own hooked model

Hooks are implemented as special instances of `nn.Identity` modules, which come some special functionality.

The following code snipped shows how to define a simple hooked model, which will have methods like `run_with_hooks` and `run_with_cache`.

```python
from transformer_lens.hook_points import HookedRootModule, HookPoint


class SquareThenAdd(nn.Module):
    def __init__(self, offset):
        super().__init__()
        self.offset = nn.Parameter(torch.tensor(offset))
        self.hook_square = HookPoint()

    def forward(self, x):
        # The hook_square doesn't change the value, but lets us access it
        square = self.hook_square(x * x)
        return self.offset + square


class TwoLayerModel(HookedRootModule):
    def __init__(self):
        super().__init__()
        self.layer1 = SquareThenAdd(3.0)
        self.layer2 = SquareThenAdd(-4.0)
        self.hook_in = HookPoint()
        self.hook_mid = HookPoint()
        self.hook_out = HookPoint()

        # We need to call the setup function of HookedRootModule to build an
        # internal dictionary of modules and hooks, and to give each hook a name
        super().setup()

    def forward(self, x):
        # We wrap the input and each layer's output in a hook - they leave the
        # value unchanged (unless there's a hook added to explicitly change it),
        # but allow us to access it.
        x_in = self.hook_in(x)
        x_mid = self.hook_mid(self.layer1(x_in))
        x_out = self.hook_out(self.layer2(x_mid))
        return x_out


model = TwoLayerModel()
```

### Cache

The `ActivationCache` class has a few useful methods for performing operations on its activations. These include:

* `cache.apply_ln_to_stack(resid_stack: Tensor)`
    * Apply layernorm scaling to a stack of residual stream values.
    * We used this to help us go from "final value in residual stream" to "projection of logits in logit difference directions", without getting the code too messy!
* `cache.accumulated_resid(layer)`
    * Returns the accumulated residual stream up to layer `layer` (or up to the final value of residual stream if layer is None), i.e. a stack of previous residual streams up to that layer's input.
    * Useful when studying the **logit lens**.
    * First dimension of output is `(0_pre, 0_mid, 1_pre, 1_mid, ..., final_post)`.
* `cache.decompose_resid(layer)`.
    * Decomposes the residual stream input to layer `layer` into a stack of the output of previous layers. The sum of these is the input to layer `layer`.
    * First dimension of output is `(embed, pos_embed, 0_attn_out, 0_mlp_out, ...)`.
* `cache.stack_head_results(layer)`
    * Returns a stack of all head results (i.e. residual stream contribution) up to layer `layer`
    * (i.e. like `decompose_resid` except it splits each attention layer by head rather than splitting each layer by attention/MLP)
    * First dimension of output is `layer * head` (we needed to rearrange to `(layer, head)` to plot it).

### Tokenization

Here are some useful functions for tokenization:

* `model.to_str_tokens` maps strings -> list of strings
* `model.to_tokens` maps strings -> tokens (as tensor)
* `model.to_single_tokens` maps a string for a single token -> that token id (as an int)
* `model.to_string` maps an int token id -> string
* `model.get_token_position(token, string)` returns the (first) position of the token in the string

**Gotcha** - remember the `prepend_bos` argument in all of these! When this is true, we prepend the BOS token to the start of the string, and the token positions are shifted by 1.

### Utils

`utils.test_prompt(prompt, answer, model)`

* Tests the model on a prompt, and prints useful output. 
* Useful for exploratory analysis.
* Example: `utils.test_prompt("One plus one equals", " two", gpt2_small)`

`utils.to_numpy`

* Converts a (possibly cuda, attached-to-computational-graph) tensor to a numpy array.
* Don't have to mess around with clone, detach, cpu and numpy methods.

`utils.get_act_name`

* Works the same way as indexing into the cache (actually, this is what gets called under the hood when we index into the cache).
* Returns full name of activation.
* Example: `utils.get_act_name(q, 0)` (second argument is layer index).
* Important - unlike the cache, you can't use negative layer indices.

### Circuitsvis

#### Attention

Circuitsvis is a great way to visualise attention patterns. You can use it as shown in the code below.

```python
import circuitsvis as cv

cv.attention.attention_patterns(
    tokens, # list of strings
    attention, # tensor of shape (n_heads, seq_len, seq_len),
    attention_head_names
)
```
                
<iframe src="https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-10/attention_patterns.html" width="1020" height="420"></iframe>

```python
cv.attention.attention_heads(
    attention, # tensor of shape (n_heads, seq_len, seq_len),
    tokens, # list of strings
    attention_head_names
)
```

<iframe src="https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-10/attention_heads.html" width="1020" height="800"></iframe>

A few notes:
* Attention heads and attention patterns have similar syntax, but present information in different ways. Which one you use depends on your use case, and personal preference.
* The version of circuitsvis I'm having people use is a fork of the main library (because the main library doesn't offer the `attention_head_names` argument for the `attention_patterns`) function. I've kept the same order of the first 2 (non-optional) arguments so as not to break compatibility, which for some arcane reason isn't the same for both functions! It's probably safer to use them as keyword arguments so they don't get mixed up.
* Make sure to check that tokens have the right length, and attention has the right shape, because there aren't error messages for this.
* These functions can be called in a cell, but they also return an html object which you can display using `IPython.display.display(html_object)`.
* Sometimes the visualisations can behave weirdly (in particular, the `attention_patterns` visualisation can shrink infinitely after being displayed). A hacky way around this is to save and open the plot in your browser:

```python
html_obj = cv.attention.attention_patterns(...)

with open("attn.html", "w") as f:
    f.write(str(html_obj))

# the following will only work if on local machine; if on VM then download & open in browser
import webbrowser; webbrowser.open("attn.html") 
```

#### Neuron Activations

A lesser-known fact is that circuitsvis can also help you visualise neuron activations. Below is some example code (we don't show all neurons, so the page doesn't slow down).

```python
cv.activations.text_neuron_activations(
    tokens, # list of strings
    activations, # list of tensors of shape (seq_pos, layers, neurons)
)
```

<iframe src="https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-10/neurons_1.html" width="1020" height="200"></iframe>

The next function shows which words each of the neurons activates most / least on (note that it requires some weird indexing to work correctly). Again, we've not shown all neurons.

```python
cv.topk_tokens.topk_tokens(
    tokens, # list of strings, each of length `seq_pos`
    activations, # corresponding list of tensors, each of shape (layers, seq_pos, neurons)
    max_k=4, # how many top/bottom activating tokens to show
)
```

<iframe src="https://callummcdougall.github.io/computational-thread-art/example_images/misc/media-10/neurons_2.html" width="1020" height="420"></iframe>

### FactoredMatrix

In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix $M = AB$, where M is `[large, large]`, but A is `[large, small]` and B is `[small, large]`. This is a common structure in transformers, and the `FactoredMatrix` class is a convenient way to work with these. It implements efficient algorithms for various operations on these, acting as a drop-in replacement for the actual matrix product. 

We define a factored matrix as follows:

```python
AB_factored = FactoredMatrix(A, B)
```

Some supported methods are:

```python
AB.eigenvalues # returns eigenvalues
AB.U, AB.S, AB.Vh # returns SVD
AB.norm() # returns Frobenius norm
AB.A, AB.B # returns left and right matrices used in product
```

### Pre-Trained Checkpoints

All of TransformerLens' interpretability-friendly models have available checkpoints, including the toy models, SoLU models, and stanford-gpt models.

The checkpoint structure and labels is somewhat messy and ad-hoc, so you're recommended to use the `checkpoint_index` syntax (where you can just count from 0 to the number of checkpoints) rather than `checkpoint_value` syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps). The helper function `get_checkpoint_labels` tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type of label was used.

```python
from transformer_lens.loading_from_pretrained import get_checkpoint_labels
from plotly_utils import line

for model_name in ["attn-only-2l", "solu-12l", "stanford-gpt2-small-a"]:
    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)
    line(
        checkpoint_labels,
        labels={"x": "Checkpoint Index", "y": f"Checkpoint Value ({checkpoint_label_type})"},
        title=f"Checkpoint Values for {model_name} (Log scale)",
        log_y=True,
        markers=True,
    )
for model_name in ["solu-1l-pile", "solu-6l-pile"]:
    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)
    line(
        checkpoint_labels,
        labels={"x": "Checkpoint Index", "y": f"Checkpoint Value ({checkpoint_label_type})"},
        title=f"Checkpoint Values for {model_name} (Linear scale)",
        log_y=False,
        markers=True,
    )
```

### Misc. 

#### Visualisation

Plotly is great - you can find plotly utils functions in these directories (along with many examples of them being used). The `RdBu` colorscheme is your friend!

#### Memory management

Call `torch.enable_grad(False)` to disable gradient tracking, if you don't need it (which you won't most of the time). This saves a lot of memory!

Call `torch.cuda.empty_cache()` to clear memory, if you find yourself needing to.

#### Notebooks vs Python files vs Colabs

It's important to konw what tools to use in different situations. All have advantages and disadvantages.

**Google Colab** provides minimal setup cost, cheap GPU support, and is a good place to share results with others and get feedback. But it also doesn't have some of the same useful tools as VSCode (e.g. Copilot, better editing and navigation features, other extensions, etc).

**VSCode-based ipynb notebooks** are good ways to display results, and are useful for exploratory analysis. Most people find them a more pleasant experience to code in than Colab. However, they do have a larger setup cost, and notebooks can also encourage bad practices (e.g. lack of structure, cluttered code, etc).

**VSCode-based Python files** can be given notebook-like functionality by cell-separation comments `# %%`. You can also write functions in a Python file and import them from other python files and notebooks, which is extremely valuable. But unlike notebooks, then can't display results inline.

Your workflow might use all three of these, e.g. working in VSCode using a combination of notebooks for exploratory analysis and Python files for writing functions that you'll import into your notebooks, then finally converting your notebooks to Colabs to publish your results.
"""


with st.sidebar:
    CHAPTER = sac.steps(
        [
            sac.StepsItem(title="Home", icon="house"),
            sac.StepsItem(title="Reference page", icon="book"),
        ],
        size="small",
        return_index=True,
    )

if int(CHAPTER) == 0:
    st.sidebar.markdown(HOMEPAGE_TOC, unsafe_allow_html=True)
    show_section_0()
elif int(CHAPTER) == 1:
    st.sidebar.markdown(generate_toc(SECTION_1), unsafe_allow_html=True)
    st.markdown(SECTION_1, unsafe_allow_html=True)
